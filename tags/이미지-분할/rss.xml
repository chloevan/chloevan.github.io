<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>이미지 분할 on Data Science | ChloEvan</title>
    <link>https://chloevan.github.io/tags/%EC%9D%B4%EB%AF%B8%EC%A7%80-%EB%B6%84%ED%95%A0/</link>
    <description>Recent content in 이미지 분할 on Data Science | ChloEvan</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Mon, 11 May 2020 12:20:30 +0900</lastBuildDate>
    
        <atom:link href="https://chloevan.github.io/tags/%EC%9D%B4%EB%AF%B8%EC%A7%80-%EB%B6%84%ED%95%A0/rss.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>Tensorflow 2.0 Tutorial ch9.5 - 이미지 분할</title>
      <link>https://chloevan.github.io/python/tensorflow2.0/ch9_5_image_segmentation/</link>
      <pubDate>Mon, 11 May 2020 12:20:30 +0900</pubDate>
      
      <guid>https://chloevan.github.io/python/tensorflow2.0/ch9_5_image_segmentation/</guid>
      <description>&lt;h2 id=&#34;공지&#34;&gt;공지&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;본 Tutorial은 교재 &lt;code&gt;시작하세요 텐서플로 2.0 프로그래밍&lt;/code&gt;의 강사에게 국비교육 강의를 듣는 사람들에게 자료 제공을 목적으로 제작하였습니다.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;강사의 주관적인 판단으로 압축해서 자료를 정리하였기 때문에, 자세하게 공부를 하고 싶으신 분은 반드시 교재를 구매하실 것을 권해드립니다.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;img src=&#34;https://chloevan.github.io/img/tensorflow2.0/book.jpg&#34; alt=&#34;&#34;&gt;&lt;!-- raw HTML omitted --&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;본 교재 외에 강사가 추가한 내용에 대한 Reference를 확인하셔서, 추가적으로 학습하시는 것을 권유드립니다.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;tutorial&#34;&gt;Tutorial&lt;/h2&gt;
&lt;p&gt;이전 강의가 궁금하신 분들은 아래에서 선택하여 추가 학습 하시기를 바랍니다.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://chloevan.github.io/python/tensorflow2.0/googlecolab/&#34;&gt;Google Colab Tensorflow 2.0 Installation&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://chloevan.github.io/python/tensorflow2.0/ch3_3_1_random_signoid/&#34;&gt;Tensorflow 2.0 Tutorial ch3.3.1 - 난수 생성 및 시그모이드 함수&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://chloevan.github.io/python/tensorflow2.0/ch3_3_2_random_signoid_bias/&#34;&gt;Tensorflow 2.0 Tutorial ch3.3.2 - 난수 생성 및 시그모이드 함수 편향성&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://chloevan.github.io/python/tensorflow2.0/ch3_3_3_network_and/&#34;&gt;Tensorflow 2.0 Tutorial ch3.3.3 - 첫번째 신경망 네트워크 - AND&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://chloevan.github.io/python/tensorflow2.0/ch3_3_4_network_or/&#34;&gt;Tensorflow 2.0 Tutorial ch3.3.4 - 두번째 신경망 네트워크 - OR&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://chloevan.github.io/python/tensorflow2.0/ch3_3_5_network_xor/&#34;&gt;Tensorflow 2.0 Tutorial ch3.3.5 - 세번째 신경망 네트워크 - XOR&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://chloevan.github.io/python/tensorflow2.0/ch4_1_linear_regression/&#34;&gt;Tensorflow 2.0 Tutorial ch4.1 - 선형회귀&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://chloevan.github.io/python/tensorflow2.0/ch4_2_multiple_linear_regression/&#34;&gt;Tensorflow 2.0 Tutorial ch4.2 - 다항회귀&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://chloevan.github.io/python/tensorflow2.0/ch4_3_regression_with_deeplearning/&#34;&gt;Tensorflow 2.0 Tutorial ch4.3 - 딥러닝 네트워크를 이용한 회귀&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://chloevan.github.io/python/tensorflow2.0/ch4_4_boston_housing_deeplearning/&#34;&gt;Tensorflow 2.0 Tutorial ch4.4 - 보스턴 주택 가격 데이터세트&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://chloevan.github.io/python/tensorflow2.0/ch5_1_binary_classification/&#34;&gt;Tensorflow 2.0 Tutorial ch5.1 - 분류&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://chloevan.github.io/python/tensorflow2.0/ch5_2_multi_classification/&#34;&gt;Tensorflow 2.0 Tutorial ch5.2 - 다항분류&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://chloevan.github.io/python/tensorflow2.0/ch5_3_fashion_mnist/&#34;&gt;Tensorflow 2.0 Tutorial ch5.3 - Fashion MNIST&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://chloevan.github.io/python/tensorflow2.0/ch6_1_2_cnn_theory/&#34;&gt;Tensorflow 2.0 Tutorial ch6.1-2 - CNN 이론&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://chloevan.github.io/python/tensorflow2.0/ch6_3_fashion_mnist_with_cnn/&#34;&gt;Tensorflow 2.0 Tutorial ch6.3 - Fashion MNIST with CNN 실습&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://chloevan.github.io/python/tensorflow2.0/ch6_4_improve_performance/&#34;&gt;Tensorflow 2.0 Tutorial ch6.4 - 모형의 성능 높이기&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://chloevan.github.io/python/tensorflow2.0/ch7_1_2_rnn_theory1/&#34;&gt;Tensorflow 2.0 Tutorial ch7.1 - RNN 이론 (1)&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://chloevan.github.io/python/tensorflow2.0/ch7_1_2_rnn_theory2/&#34;&gt;Tensorflow 2.0 Tutorial ch7.1 - RNN 이론 (2)&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://chloevan.github.io/python/tensorflow2.0/ch7_3_sentimentanalysis/&#34;&gt;Tensorflow 2.0 Tutorial ch7.3 - 긍정, 부정 감성 분석&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://chloevan.github.io/python/tensorflow2.0/ch7_4_naturallanguagegeneration1/&#34;&gt;Tensorflow 2.0 Tutorial ch7.4 - (1) 단어 단위 생성&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://chloevan.github.io/python/tensorflow2.0/ch7_4_naturallanguagegeneration2/&#34;&gt;Tensorflow 2.0 Tutorial ch7.4 - (2) 단어 단위 생성&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://chloevan.github.io/python/tensorflow2.0/ch8_1_tensorflow_hub/&#34;&gt;Tensorflow 2.0 Tutorial ch8.1 - 텐서플로 허브&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://chloevan.github.io/python/tensorflow2.0/ch8_2_transfer_learning/&#34;&gt;Tensorflow 2.0 Tutorial ch8.2 - 전이 학습과 &amp;amp; Kaggle 대회&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://chloevan.github.io/python/tensorflow2.0/ch8_3_1_texture_synthesis/&#34;&gt;Tensorflow 2.0 Tutorial ch8.3.1 - 컨볼루션 신경망을 사용한 텍스처 합성&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://chloevan.github.io/python/tensorflow2.0/ch8_3_2_neural_style_transfer/&#34;&gt;Tensorflow 2.0 Tutorial ch8.3.2 - 컨볼루션 신경망을 사용한 신경 스타일 전이&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://chloevan.github.io/python/tensorflow2.0/ch9_1_auto_encoder/&#34;&gt;Tensorflow 2.0 Tutorial ch9.1-2 - 오토인코더 &amp;amp; MNIST&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://chloevan.github.io/python/tensorflow2.0/ch9_3_k_means_clustering/&#34;&gt;Tensorflow 2.0 Tutorial ch9.3 - 클러스터링&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://chloevan.github.io/python/tensorflow2.0/ch9_4_super_resolution/&#34;&gt;Tensorflow 2.0 Tutorial ch9.4 - 초해상도&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;i-개요&#34;&gt;I. 개요&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;이미지에서 단순히 경계선을 추출하는 작업은 전통적인 알고리즘의 필터나 한 층의 컨볼루션 레이어로도 가능하지만, 의미 있는 부분과 그렇지 않은 부분으로 분할하기 위해서는 학습이 필요합니다.&lt;/li&gt;
&lt;li&gt;앞 절에서 정의한 &lt;code&gt;REDNet&lt;/code&gt;을 조금만 수정하면 이미지 분할(&lt;code&gt;Segmentation&lt;/code&gt;)에서 사용할 수 있습니다.&lt;/li&gt;
&lt;li&gt;이미지의 경계선과 내용, 그리고 외곽의 3가지로 분류하는 &lt;code&gt;Oxford-IIIT Pet&lt;/code&gt; 데이터세트로 이미지 분할 문제를 학습합니다.&lt;/li&gt;
&lt;li&gt;교재에 있는 코드에서 몇몇 에러가 발생하였습니다. 내용상 텐서플로 홈페이지와 유사하여 텐서플로 공식 홈페이지에 있는 소스코드를 참고하였습니다.&lt;/li&gt;
&lt;li&gt;먼저 필수 파일들을 &lt;code&gt;pip&lt;/code&gt; 도구를 활용하여 설치합니다.&lt;/li&gt;
&lt;/ul&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span style=&#34;color:#960050;background-color:#1e0010&#34;&gt;!&lt;/span&gt;pip install git&lt;span style=&#34;color:#f92672&#34;&gt;+&lt;/span&gt;https:&lt;span style=&#34;color:#f92672&#34;&gt;//&lt;/span&gt;github&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;com&lt;span style=&#34;color:#f92672&#34;&gt;/&lt;/span&gt;tensorflow&lt;span style=&#34;color:#f92672&#34;&gt;/&lt;/span&gt;examples&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;git
&lt;span style=&#34;color:#960050;background-color:#1e0010&#34;&gt;!&lt;/span&gt;pip install &lt;span style=&#34;color:#f92672&#34;&gt;-&lt;/span&gt;U tfds&lt;span style=&#34;color:#f92672&#34;&gt;-&lt;/span&gt;nightly
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;pre&gt;&lt;code&gt;Collecting git+https://github.com/tensorflow/examples.git
  Cloning https://github.com/tensorflow/examples.git to /tmp/pip-req-build-36g0gu68
  Running command git clone -q https://github.com/tensorflow/examples.git /tmp/pip-req-build-36g0gu68
Requirement already satisfied: absl-py in /usr/local/lib/python3.6/dist-packages (from tensorflow-examples===63ee35adcc3e3dd2d228bc3283e27f6a1e2158ab-) (0.9.0)
Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from tensorflow-examples===63ee35adcc3e3dd2d228bc3283e27f6a1e2158ab-) (1.12.0)
Building wheels for collected packages: tensorflow-examples
  Building wheel for tensorflow-examples (setup.py) ... [?25l[?25hdone
  Created wheel for tensorflow-examples: filename=tensorflow_examples-63ee35adcc3e3dd2d228bc3283e27f6a1e2158ab_-cp36-none-any.whl size=125226 sha256=f2f0d0a9e57edde6593979e55a26983574dba25b3f7008be261173181109f5b8
  Stored in directory: /tmp/pip-ephem-wheel-cache-f32yqfdy/wheels/83/64/b3/4cfa02dc6f9d16bf7257892c6a7ec602cd7e0ff6ec4d7d714d
Successfully built tensorflow-examples
Installing collected packages: tensorflow-examples
Successfully installed tensorflow-examples-63ee35adcc3e3dd2d228bc3283e27f6a1e2158ab-
Collecting tfds-nightly
[?25l  Downloading https://files.pythonhosted.org/packages/8c/42/df5f05f2124f9d1b6e16b88c518f04a7d282d77daf3113ba548baadc4ce5/tfds_nightly-3.1.0.dev202005100106-py3-none-any.whl (3.3MB)
[K     |████████████████████████████████| 3.3MB 11.6MB/s 
[?25hRequirement already satisfied, skipping upgrade: wrapt in /usr/local/lib/python3.6/dist-packages (from tfds-nightly) (1.12.1)
Requirement already satisfied, skipping upgrade: requests&amp;gt;=2.19.0 in /usr/local/lib/python3.6/dist-packages (from tfds-nightly) (2.23.0)
Requirement already satisfied, skipping upgrade: tqdm in /usr/local/lib/python3.6/dist-packages (from tfds-nightly) (4.41.1)
Requirement already satisfied, skipping upgrade: attrs&amp;gt;=18.1.0 in /usr/local/lib/python3.6/dist-packages (from tfds-nightly) (19.3.0)
Requirement already satisfied, skipping upgrade: future in /usr/local/lib/python3.6/dist-packages (from tfds-nightly) (0.16.0)
Requirement already satisfied, skipping upgrade: absl-py in /usr/local/lib/python3.6/dist-packages (from tfds-nightly) (0.9.0)
Requirement already satisfied, skipping upgrade: dill in /usr/local/lib/python3.6/dist-packages (from tfds-nightly) (0.3.1.1)
Requirement already satisfied, skipping upgrade: promise in /usr/local/lib/python3.6/dist-packages (from tfds-nightly) (2.3)
Collecting tensorflow-metadata&amp;lt;0.16,&amp;gt;=0.15
  Downloading https://files.pythonhosted.org/packages/3b/0c/afb81ea6998f6e26521671585d1cd9d3f7945a8b9834764e91757453dc25/tensorflow_metadata-0.15.2-py2.py3-none-any.whl
Requirement already satisfied, skipping upgrade: six in /usr/local/lib/python3.6/dist-packages (from tfds-nightly) (1.12.0)
Requirement already satisfied, skipping upgrade: protobuf&amp;gt;=3.6.1 in /usr/local/lib/python3.6/dist-packages (from tfds-nightly) (3.10.0)
Requirement already satisfied, skipping upgrade: numpy in /usr/local/lib/python3.6/dist-packages (from tfds-nightly) (1.18.4)
Requirement already satisfied, skipping upgrade: termcolor in /usr/local/lib/python3.6/dist-packages (from tfds-nightly) (1.1.0)
Requirement already satisfied, skipping upgrade: chardet&amp;lt;4,&amp;gt;=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests&amp;gt;=2.19.0-&amp;gt;tfds-nightly) (3.0.4)
Requirement already satisfied, skipping upgrade: certifi&amp;gt;=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests&amp;gt;=2.19.0-&amp;gt;tfds-nightly) (2020.4.5.1)
Requirement already satisfied, skipping upgrade: idna&amp;lt;3,&amp;gt;=2.5 in /usr/local/lib/python3.6/dist-packages (from requests&amp;gt;=2.19.0-&amp;gt;tfds-nightly) (2.9)
Requirement already satisfied, skipping upgrade: urllib3!=1.25.0,!=1.25.1,&amp;lt;1.26,&amp;gt;=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests&amp;gt;=2.19.0-&amp;gt;tfds-nightly) (1.24.3)
Requirement already satisfied, skipping upgrade: googleapis-common-protos in /usr/local/lib/python3.6/dist-packages (from tensorflow-metadata&amp;lt;0.16,&amp;gt;=0.15-&amp;gt;tfds-nightly) (1.51.0)
Requirement already satisfied, skipping upgrade: setuptools in /usr/local/lib/python3.6/dist-packages (from protobuf&amp;gt;=3.6.1-&amp;gt;tfds-nightly) (46.1.3)
Installing collected packages: tensorflow-metadata, tfds-nightly
  Found existing installation: tensorflow-metadata 0.21.2
    Uninstalling tensorflow-metadata-0.21.2:
      Successfully uninstalled tensorflow-metadata-0.21.2
Successfully installed tensorflow-metadata-0.15.2 tfds-nightly-3.1.0.dev202005100106
&lt;/code&gt;&lt;/pre&gt;
&lt;h2 id=&#34;ii-rednet1&#34;&gt;II. REDNet[^1]&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;REDNet&lt;/code&gt;은 &lt;code&gt;Residual Encoder-Decoder Network&lt;/code&gt;의 약자이며, &lt;code&gt;Residual&lt;/code&gt;은 &lt;code&gt;ResNet&lt;/code&gt;등에서 사용하는 건너뛴 연결(&lt;code&gt;skip-connection&lt;/code&gt;)입니다.&lt;/li&gt;
&lt;li&gt;다수의 레이어가 중첩되는 구조에서 앞쪽의 정보를 잃어버리기 않기 위해 뒤쪽에 정보를 그대로 전달해줄 때 건너뛴 연결이 사용됩니다.&lt;/li&gt;
&lt;/ul&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span style=&#34;color:#f92672&#34;&gt;import&lt;/span&gt; tensorflow &lt;span style=&#34;color:#f92672&#34;&gt;as&lt;/span&gt; tf
&lt;span style=&#34;color:#f92672&#34;&gt;from&lt;/span&gt; tensorflow_examples.models.pix2pix &lt;span style=&#34;color:#f92672&#34;&gt;import&lt;/span&gt; pix2pix

&lt;span style=&#34;color:#f92672&#34;&gt;import&lt;/span&gt; tensorflow_datasets &lt;span style=&#34;color:#f92672&#34;&gt;as&lt;/span&gt; tfds
tfds&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;disable_progress_bar()

&lt;span style=&#34;color:#f92672&#34;&gt;from&lt;/span&gt; IPython.display &lt;span style=&#34;color:#f92672&#34;&gt;import&lt;/span&gt; clear_output
&lt;span style=&#34;color:#f92672&#34;&gt;import&lt;/span&gt; matplotlib.pyplot &lt;span style=&#34;color:#f92672&#34;&gt;as&lt;/span&gt; plt
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;h2 id=&#34;iii-데이터-불러오기&#34;&gt;III. 데이터 불러오기&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;tf.keras.utils.get_file()&lt;/code&gt; 데이터를 불러옵니다.&lt;/li&gt;
&lt;li&gt;교재에서는 &lt;code&gt;oxford_iiit_pet:3.0.0&lt;/code&gt;으로 되어 있었는데, 버전을 &lt;code&gt;3.*.*&lt;/code&gt;으로 수정하여 다운로드를 하기를 바랍니다.&lt;/li&gt;
&lt;/ul&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;dataset, info &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; tfds&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;load(&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;oxford_iiit_pet:3.*.*&amp;#39;&lt;/span&gt;, with_info&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;True)
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;pre&gt;&lt;code&gt;[1mDownloading and preparing dataset oxford_iiit_pet/3.2.0 (download: 773.52 MiB, generated: 774.69 MiB, total: 1.51 GiB) to /root/tensorflow_datasets/oxford_iiit_pet/3.2.0...[0m
Shuffling and writing examples to /root/tensorflow_datasets/oxford_iiit_pet/3.2.0.incompleteONHCBY/oxford_iiit_pet-train.tfrecord
Shuffling and writing examples to /root/tensorflow_datasets/oxford_iiit_pet/3.2.0.incompleteONHCBY/oxford_iiit_pet-test.tfrecord
[1mDataset oxford_iiit_pet downloaded and prepared to /root/tensorflow_datasets/oxford_iiit_pet/3.2.0. Subsequent calls will reuse this data.[0m
&lt;/code&gt;&lt;/pre&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;info
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;pre&gt;&lt;code&gt;tfds.core.DatasetInfo(
    name=&#39;oxford_iiit_pet&#39;,
    version=3.2.0,
    description=&#39;The Oxford-IIIT pet dataset is a 37 category pet image dataset with roughly 200
images for each class. The images have large variations in scale, pose and
lighting. All images have an associated ground truth annotation of breed.&#39;,
    homepage=&#39;http://www.robots.ox.ac.uk/~vgg/data/pets/&#39;,
    features=FeaturesDict({
        &#39;file_name&#39;: Text(shape=(), dtype=tf.string),
        &#39;image&#39;: Image(shape=(None, None, 3), dtype=tf.uint8),
        &#39;label&#39;: ClassLabel(shape=(), dtype=tf.int64, num_classes=37),
        &#39;segmentation_mask&#39;: Image(shape=(None, None, 1), dtype=tf.uint8),
        &#39;species&#39;: ClassLabel(shape=(), dtype=tf.int64, num_classes=2),
    }),
    total_num_examples=7349,
    splits={
        &#39;test&#39;: 3669,
        &#39;train&#39;: 3680,
    },
    supervised_keys=(&#39;image&#39;, &#39;label&#39;),
    citation=&amp;quot;&amp;quot;&amp;quot;@InProceedings{parkhi12a,
      author       = &amp;quot;Parkhi, O. M. and Vedaldi, A. and Zisserman, A. and Jawahar, C.~V.&amp;quot;,
      title        = &amp;quot;Cats and Dogs&amp;quot;,
      booktitle    = &amp;quot;IEEE Conference on Computer Vision and Pattern Recognition&amp;quot;,
      year         = &amp;quot;2012&amp;quot;,
    }&amp;quot;&amp;quot;&amp;quot;,
    redistribution_info=,
)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;code&gt;Dataset&lt;/code&gt;의 주요 정보를 구성하고 있는 부분은 &lt;code&gt;features&lt;/code&gt;입니다. 여기에는 &lt;code&gt;image&lt;/code&gt;, &lt;code&gt;label&lt;/code&gt;, &lt;code&gt;segmentation_mask&lt;/code&gt;가 보입니다.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;label&lt;/code&gt;은 각 애완동물에 대한 클래스 정보를 담고 있습니다.&lt;/li&gt;
&lt;li&gt;&lt;code&gt;image&lt;/code&gt;는 3차원인 것으로 보아 &lt;code&gt;컬러 이미지&lt;/code&gt;일 것이라고 추측할 수 있습니다.&lt;/li&gt;
&lt;li&gt;&lt;code&gt;segmentation_mask&lt;/code&gt;는 마지막 차원이 &lt;code&gt;1&lt;/code&gt;로 구성된 것을 볼 때 흑백 이미지처럼 다룰 수 있다고 추측해 볼 수 있습니다.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;iv-rednet-모형-학습&#34;&gt;IV. &lt;code&gt;REDNET&lt;/code&gt; 모형 학습&lt;/h2&gt;
&lt;p&gt;모형 학습을 위해 데이터 수집 부터 모델 정의까지 진행합니다.&lt;/p&gt;
&lt;h3 id=&#34;1-데이터-정규화&#34;&gt;(1) 데이터 정규화&lt;/h3&gt;
&lt;p&gt;The following code performs a simple augmentation of flipping an image. In addition, image is normalized to [0,1]. Finally, as mentioned above the pixels in the segmentation mask are labeled either {1, 2, 3}. For the sake of convenience, let&amp;rsquo;s subtract 1 from the segmentation mask, resulting in labels that are : {0, 1, 2}.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;다음 코드는 이미지를 뒤집는 Simple Augmentation을 수행합니다.&lt;/li&gt;
&lt;li&gt;또한 이미지는 [0,1]로 정규화 합니다.&lt;/li&gt;
&lt;li&gt;마지막으로, 위에서 언급한 바와 같이 분할 마스크의 픽셀은 {1, 2, 3} 중 하나로 라벨이 표시되어 있다.&lt;/li&gt;
&lt;li&gt;편의상 분할 마스크에서 1을 빼서 {0, 1, 2}의 레이블을 생성해 봅시다.&lt;/li&gt;
&lt;/ul&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;def&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;normalize&lt;/span&gt;(input_image, input_mask):
  input_image &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; tf&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;cast(input_image, tf&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;float32) &lt;span style=&#34;color:#f92672&#34;&gt;/&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;255.0&lt;/span&gt;
  input_mask &lt;span style=&#34;color:#f92672&#34;&gt;-=&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;
  &lt;span style=&#34;color:#66d9ef&#34;&gt;return&lt;/span&gt; input_image, input_mask
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;ul&gt;
&lt;li&gt;여기에서 마스크에서 1을 빼는 연산이 있습니다. 이 부분은 데이터를 출력 후 재 설명하도록 합니다.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;2-훈련데이터-테스트데이터-불러오기&#34;&gt;(2) 훈련데이터, 테스트데이터 불러오기&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;훈련데이터와 테스트 불러오는 함수를 작성합니다.&lt;/li&gt;
&lt;/ul&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span style=&#34;color:#a6e22e&#34;&gt;@tf.function&lt;/span&gt;
&lt;span style=&#34;color:#66d9ef&#34;&gt;def&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;load_image_train&lt;/span&gt;(datapoint):
  input_image &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; tf&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;image&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;resize(datapoint[&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;image&amp;#39;&lt;/span&gt;], (&lt;span style=&#34;color:#ae81ff&#34;&gt;128&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;128&lt;/span&gt;))
  input_mask &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; tf&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;image&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;resize(datapoint[&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;segmentation_mask&amp;#39;&lt;/span&gt;], (&lt;span style=&#34;color:#ae81ff&#34;&gt;128&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;128&lt;/span&gt;))

  &lt;span style=&#34;color:#66d9ef&#34;&gt;if&lt;/span&gt; tf&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;random&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;uniform(()) &lt;span style=&#34;color:#f92672&#34;&gt;&amp;gt;&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;0.5&lt;/span&gt;:
    input_image &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; tf&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;image&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;flip_left_right(input_image)
    input_mask &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; tf&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;image&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;flip_left_right(input_mask)

  input_image, input_mask &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; normalize(input_image, input_mask)

  &lt;span style=&#34;color:#66d9ef&#34;&gt;return&lt;/span&gt; input_image, input_mask
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;def&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;load_image_test&lt;/span&gt;(datapoint):
  input_image &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; tf&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;image&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;resize(datapoint[&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;image&amp;#39;&lt;/span&gt;], (&lt;span style=&#34;color:#ae81ff&#34;&gt;128&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;128&lt;/span&gt;))
  input_mask &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; tf&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;image&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;resize(datapoint[&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;segmentation_mask&amp;#39;&lt;/span&gt;], (&lt;span style=&#34;color:#ae81ff&#34;&gt;128&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;128&lt;/span&gt;))

  input_image, input_mask &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; normalize(input_image, input_mask)

  &lt;span style=&#34;color:#66d9ef&#34;&gt;return&lt;/span&gt; input_image, input_mask
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;TRAIN_LENGTH &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; info&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;splits[&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;train&amp;#39;&lt;/span&gt;]&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;num_examples
BATCH_SIZE &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;64&lt;/span&gt;
BUFFER_SIZE &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;1000&lt;/span&gt;
STEPS_PER_EPOCH &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; TRAIN_LENGTH &lt;span style=&#34;color:#f92672&#34;&gt;//&lt;/span&gt; BATCH_SIZE
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;ul&gt;
&lt;li&gt;데이터를 불러오면서 &lt;code&gt;image&lt;/code&gt;의 &lt;code&gt;flip&lt;/code&gt;을 주어 변형시켰다. 또한, &lt;code&gt;image&lt;/code&gt;와 &lt;code&gt;mask&lt;/code&gt;를 통해 정규화를 진행하고, &lt;code&gt;128 X 128&lt;/code&gt;로 변환합니다. 크기가 다르면 &lt;code&gt;tf.keras&lt;/code&gt;에서 학습이 되지 않기 때문에 그렇습니다.&lt;/li&gt;
&lt;li&gt;이제 데이터를 불러오도록 합니다.&lt;/li&gt;
&lt;/ul&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;train &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; dataset[&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;train&amp;#39;&lt;/span&gt;]&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;map(load_image_train, num_parallel_calls&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;tf&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;data&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;experimental&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;AUTOTUNE)
test &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; dataset[&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;test&amp;#39;&lt;/span&gt;]&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;map(load_image_test)

train_dataset &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; train&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;cache()&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;shuffle(BUFFER_SIZE)&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;batch(BATCH_SIZE)&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;repeat()
train_dataset &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; train_dataset&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;prefetch(buffer_size&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;tf&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;data&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;experimental&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;AUTOTUNE)
test_dataset &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; test&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;batch(BATCH_SIZE)
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;ul&gt;
&lt;li&gt;정의된 &lt;code&gt;load_image_*&lt;/code&gt; 함수로 이미지와 마스크를 반환하고, 데이터를 계속 학습시킬 수 있도록 &lt;code&gt;repeat()&lt;/code&gt; 함수를 적용합니다.&lt;/li&gt;
&lt;li&gt;그리고, 배치 사이즈를 각각 적용하도록 합니다.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;3-시각화-함수-정의-및-시각화&#34;&gt;(3) 시각화 함수 정의 및 시각화&lt;/h3&gt;
&lt;p&gt;우선 함수를 정의하도록 합니다.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;def&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;display&lt;/span&gt;(display_list):
  plt&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;figure(figsize&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;(&lt;span style=&#34;color:#ae81ff&#34;&gt;15&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;15&lt;/span&gt;))

  title &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; [&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;Input Image&amp;#39;&lt;/span&gt;, &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;True Mask&amp;#39;&lt;/span&gt;, &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;Predicted Mask&amp;#39;&lt;/span&gt;]

  &lt;span style=&#34;color:#66d9ef&#34;&gt;for&lt;/span&gt; i &lt;span style=&#34;color:#f92672&#34;&gt;in&lt;/span&gt; range(len(display_list)):
    plt&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;subplot(&lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;, len(display_list), i&lt;span style=&#34;color:#f92672&#34;&gt;+&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;)
    plt&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;title(title[i])
    plt&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;imshow(tf&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;keras&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;preprocessing&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;image&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;array_to_img(display_list[i]))
    plt&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;axis(&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;off&amp;#39;&lt;/span&gt;)
  plt&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;colorbar()  
  plt&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;show()
  
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;정의된 함수를 사용하여 이미지와 마스크를 출력합니다.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;for&lt;/span&gt; image, mask &lt;span style=&#34;color:#f92672&#34;&gt;in&lt;/span&gt; train&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;take(&lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;):
  sample_image, sample_mask &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; image, mask
display([sample_image, sample_mask])

&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;&lt;img src=&#34;https://chloevan.github.io/img/tensorflow2.0/tutorial_09_05/output_23_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;왼쪽은 원본 이미지가 보이고, 오른쪽에는 마스크와 각 숫자의 값을 나타내는 &lt;code&gt;colorbar&lt;/code&gt;를 표시했습니다.&lt;/li&gt;
&lt;li&gt;마스크 데이터에는 중심부, 외곽선 배경을 뜻하는 &lt;code&gt;1, 3, 2&lt;/code&gt;의 숫자가 각 픽셀에 대해 저장되어 있습니다.&lt;/li&gt;
&lt;li&gt;이미지 분할 문제는 기본적으로 각 픽셀을 분류하는 문제이기 때문에 라벨이 0부터 시작할 수 있게 1,2,3을 0,1,2로 바꿔줍니다.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;4-모형의-정의&#34;&gt;(4) 모형의 정의&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;이미지 분할 네트워크의 학습은 원본 이미지를 입력했을 때 마스크를 출력하게 합니다. 이를 위해서는 ch9_4에서 사용했던 &lt;code&gt;REDNet&lt;/code&gt;에서 마지막 레이어를 수정합니다.&lt;/li&gt;
&lt;/ul&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;def&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;REDNet_segmentation&lt;/span&gt;(num_layers):
    conv_layers &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; []
    deconv_layers &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; []
    residual_layers &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; []

    inputs &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; tf&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;keras&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;layers&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;Input(shape&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;(None, None, &lt;span style=&#34;color:#ae81ff&#34;&gt;3&lt;/span&gt;))
    conv_layers&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;append(tf&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;keras&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;layers&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;Conv2D(&lt;span style=&#34;color:#ae81ff&#34;&gt;3&lt;/span&gt;, kernel_size&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;3&lt;/span&gt;, padding&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;same&amp;#39;&lt;/span&gt;, activation&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;relu&amp;#39;&lt;/span&gt;))

    &lt;span style=&#34;color:#66d9ef&#34;&gt;for&lt;/span&gt; i &lt;span style=&#34;color:#f92672&#34;&gt;in&lt;/span&gt; range(num_layers&lt;span style=&#34;color:#f92672&#34;&gt;-&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;):
        conv_layers&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;append(tf&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;keras&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;layers&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;Conv2D(&lt;span style=&#34;color:#ae81ff&#34;&gt;64&lt;/span&gt;, kernel_size&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;3&lt;/span&gt;, padding&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;same&amp;#39;&lt;/span&gt;, activation&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;relu&amp;#39;&lt;/span&gt;))
        deconv_layers&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;append(tf&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;keras&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;layers&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;Conv2DTranspose(&lt;span style=&#34;color:#ae81ff&#34;&gt;64&lt;/span&gt;, kernel_size&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;3&lt;/span&gt;, padding&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;same&amp;#39;&lt;/span&gt;, activation&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;relu&amp;#39;&lt;/span&gt;))

    deconv_layers&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;append(tf&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;keras&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;layers&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;Conv2DTranspose(&lt;span style=&#34;color:#ae81ff&#34;&gt;3&lt;/span&gt;, kernel_size&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;3&lt;/span&gt;, padding&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;same&amp;#39;&lt;/span&gt;, activation&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;softmax&amp;#39;&lt;/span&gt;))

    x &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; conv_layers[&lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;](inputs)

    &lt;span style=&#34;color:#66d9ef&#34;&gt;for&lt;/span&gt; i &lt;span style=&#34;color:#f92672&#34;&gt;in&lt;/span&gt; range(num_layers&lt;span style=&#34;color:#f92672&#34;&gt;-&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;):
        x &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; conv_layers[i&lt;span style=&#34;color:#f92672&#34;&gt;+&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;](x)
        &lt;span style=&#34;color:#66d9ef&#34;&gt;if&lt;/span&gt; i &lt;span style=&#34;color:#f92672&#34;&gt;%&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;2&lt;/span&gt; &lt;span style=&#34;color:#f92672&#34;&gt;==&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;:
            residual_layers&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;append(x)

    &lt;span style=&#34;color:#66d9ef&#34;&gt;for&lt;/span&gt; i &lt;span style=&#34;color:#f92672&#34;&gt;in&lt;/span&gt; range(num_layers&lt;span style=&#34;color:#f92672&#34;&gt;-&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;):
        &lt;span style=&#34;color:#66d9ef&#34;&gt;if&lt;/span&gt; i &lt;span style=&#34;color:#f92672&#34;&gt;%&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;2&lt;/span&gt; &lt;span style=&#34;color:#f92672&#34;&gt;==&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;:
            x &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; tf&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;keras&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;layers&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;Add()([x, residual_layers&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;pop()])
            x &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; tf&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;keras&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;layers&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;Activation(&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;relu&amp;#39;&lt;/span&gt;)(x)
        x &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; deconv_layers[i](x) 

    x &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; deconv_layers[&lt;span style=&#34;color:#f92672&#34;&gt;-&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;](x)
    
    model &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; tf&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;keras&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;Model(inputs&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;inputs, outputs&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;x)
    &lt;span style=&#34;color:#66d9ef&#34;&gt;return&lt;/span&gt; model
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;수정된 부분은 마지막 레이어의 활성화함수입니다. 원래 활성화 함수가 없었기 때문에, &lt;code&gt;linear&lt;/code&gt; 활성화함수로 입력값을 그대로 출력하던 것을 소프트맥스 활성화함수로 교체합니다.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;분류 문제로 바뀌었기 때문에 네트워크를 컴파일할 때의 &lt;code&gt;loss&lt;/code&gt;도 바꿔줍니다.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;model &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; REDNet_segmentation(&lt;span style=&#34;color:#ae81ff&#34;&gt;15&lt;/span&gt;)
model&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;compile(optimizer&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;tf&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;optimizers&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;Adam(&lt;span style=&#34;color:#ae81ff&#34;&gt;0.0001&lt;/span&gt;),
              loss&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;sparse_categorical_crossentropy&amp;#39;&lt;/span&gt;,
              metrics&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;[&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;accuracy&amp;#39;&lt;/span&gt;])
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;분류를 위해 &lt;code&gt;loss&lt;/code&gt;에 &lt;code&gt;sparse_categorical_crossentropy&lt;/code&gt;를 사용했고, 분류의 정확도를 측정하기 위해 &lt;code&gt;metrics&lt;/code&gt;에 &lt;code&gt;accuracy&lt;/code&gt;를 넣었습니다.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;모형의 구조를 출력해봅니다.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;tf&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;keras&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;utils&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;plot_model(model, show_shapes&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;True)
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;&lt;img src=&#34;https://chloevan.github.io/img/tensorflow2.0/tutorial_09_05/output_31_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;h3 id=&#34;5-정의된-모형의-예측&#34;&gt;(5) 정의된 모형의 예측&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;모형을 학습시키기 전에 간단하게 예측을 통해서 어떤 결과값이 나타나는지 확인해봅니다.&lt;/li&gt;
&lt;/ul&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;def&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;create_mask&lt;/span&gt;(pred_mask):
  pred_mask &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; tf&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;argmax(pred_mask, axis&lt;span style=&#34;color:#f92672&#34;&gt;=-&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;)
  pred_mask &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; pred_mask[&lt;span style=&#34;color:#f92672&#34;&gt;...&lt;/span&gt;, tf&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;newaxis]
  &lt;span style=&#34;color:#66d9ef&#34;&gt;return&lt;/span&gt; pred_mask[&lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;]
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;def&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;show_predictions&lt;/span&gt;(dataset&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;None, num&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;):
  &lt;span style=&#34;color:#66d9ef&#34;&gt;if&lt;/span&gt; dataset:
    &lt;span style=&#34;color:#66d9ef&#34;&gt;for&lt;/span&gt; image, mask &lt;span style=&#34;color:#f92672&#34;&gt;in&lt;/span&gt; dataset&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;take(num):
      pred_mask &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; model&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;predict(image)
      display([image[&lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;], mask[&lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;], create_mask(pred_mask)])
  &lt;span style=&#34;color:#66d9ef&#34;&gt;else&lt;/span&gt;:
    display([sample_image, sample_mask,
             create_mask(model&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;predict(sample_image[tf&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;newaxis, &lt;span style=&#34;color:#f92672&#34;&gt;...&lt;/span&gt;]))])
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;show_predictions()
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;&lt;img src=&#34;https://chloevan.github.io/img/tensorflow2.0/tutorial_09_05/output_35_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;h3 id=&#34;6-모형-학습&#34;&gt;(6) 모형 학습&lt;/h3&gt;
&lt;p&gt;이제 본격적으로 네트워크를 학습해봅니다.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;class&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;DisplayCallback&lt;/span&gt;(tf&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;keras&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;callbacks&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;Callback):
  &lt;span style=&#34;color:#66d9ef&#34;&gt;def&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;on_epoch_end&lt;/span&gt;(self, epoch, logs&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;None):
    clear_output(wait&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;True)
    show_predictions()
    &lt;span style=&#34;color:#66d9ef&#34;&gt;print&lt;/span&gt; (&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;\n&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;Sample Prediction after epoch {}&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;\n&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;&lt;/span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;format(epoch&lt;span style=&#34;color:#f92672&#34;&gt;+&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;))
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;EPOCHS &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;20&lt;/span&gt;
VAL_SUBSPLITS &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;5&lt;/span&gt;
VALIDATION_STEPS &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; info&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;splits[&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;test&amp;#39;&lt;/span&gt;]&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;num_examples&lt;span style=&#34;color:#f92672&#34;&gt;//&lt;/span&gt;BATCH_SIZE&lt;span style=&#34;color:#f92672&#34;&gt;//&lt;/span&gt;VAL_SUBSPLITS

model_history &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; model&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;fit(train_dataset, epochs&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;EPOCHS,
                          steps_per_epoch&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;STEPS_PER_EPOCH,
                          validation_steps&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;VALIDATION_STEPS,
                          validation_data&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;test_dataset,
                          callbacks&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;[DisplayCallback()])
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;&lt;img src=&#34;https://chloevan.github.io/img/tensorflow2.0/tutorial_09_05/output_38_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;Sample Prediction after epoch 20

57/57 [==============================] - 112s 2s/step - loss: 0.5203 - accuracy: 0.7806 - val_loss: 0.5203 - val_accuracy: 0.7839
&lt;/code&gt;&lt;/pre&gt;
&lt;ul&gt;
&lt;li&gt;이미지 분할은 초해상도 이미지를 얻는 문제보다는 쉽기 때문에 에포크를 20으로 설정했습니다.&lt;/li&gt;
&lt;li&gt;학습결과 훈련 데이터와 검증 데이터에서의 정확도는 각각 78%, 78%로 확인되었습니다.&lt;/li&gt;
&lt;li&gt;가운데 정답 마스크고, 가장 오른쪽이 예측결과인데, 몸통 부위에 대해 예측을 하지 못했습니다.&lt;/li&gt;
&lt;li&gt;학습을 더 시켜도 될 것 같습니다.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;7-학습-모형-결과-시각화&#34;&gt;(7) 학습 모형 결과 시각화&lt;/h3&gt;
&lt;p&gt;학습된 모형에 대해 시각화를 진행하도록 합니다.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;loss &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; model_history&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;history[&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;loss&amp;#39;&lt;/span&gt;]
val_loss &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; model_history&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;history[&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;val_loss&amp;#39;&lt;/span&gt;]

epochs &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; range(EPOCHS)

plt&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;figure()
plt&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;plot(epochs, loss, &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;r&amp;#39;&lt;/span&gt;, label&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;Training loss&amp;#39;&lt;/span&gt;)
plt&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;plot(epochs, val_loss, &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;bo&amp;#39;&lt;/span&gt;, label&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;Validation loss&amp;#39;&lt;/span&gt;)
plt&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;title(&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;Training and Validation Loss&amp;#39;&lt;/span&gt;)
plt&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;xlabel(&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;Epoch&amp;#39;&lt;/span&gt;)
plt&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;ylabel(&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;Loss Value&amp;#39;&lt;/span&gt;)
plt&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;ylim([&lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;])
plt&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;legend()
plt&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;show()
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;&lt;img src=&#34;https://chloevan.github.io/img/tensorflow2.0/tutorial_09_05/output_41_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;위 시각화를 볼 때, 에포크를 늘려도 무난할 것 같습니다.&lt;/li&gt;
&lt;li&gt;이미지 보강 등의 방법을 사용하면 더 좋은 결과를 얻을 수 있을 것입니다.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;8-테스트-이미지-분할-확인&#34;&gt;(8) 테스트 이미지 분할 확인&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;각 행의 가운데 세로줄이 정답 마스크이고, 가장 오른쪽 열이 재구성된 마스크입니다.&lt;/li&gt;
&lt;/ul&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;show_predictions(test_dataset, &lt;span style=&#34;color:#ae81ff&#34;&gt;3&lt;/span&gt;)
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;&lt;img src=&#34;https://chloevan.github.io/img/tensorflow2.0/tutorial_09_05/output_44_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://chloevan.github.io/img/tensorflow2.0/tutorial_09_05/output_44_1.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://chloevan.github.io/img/tensorflow2.0/tutorial_09_05/output_44_2.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;위 3가지에서 조금 주목할만 것이 있다면, 첫번째 사진입니다. 정답 마스크에 비해 예측 마스크의 노란색이 조금 더 옅어진 것을 확인할 수 있습니다. 이는 오히려 정답 이미지에 비해 오히려 세밀한 예측을 하고 있다고 보여집니다.&lt;/li&gt;
&lt;li&gt;다만, 전반적으로 이미지가 깨지는 부분이 많아서 에포크를 늘리거나 이미지 보강등의 기법으로 진행하면 더 좋은 결과를 얻어낼 수 있을 것 같습니다.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;v-결론-및-정리&#34;&gt;V. 결론 및 정리&lt;/h2&gt;
&lt;p&gt;이번 9장에서는 컨볼루션 레이어와 디컨볼루션 레이어를 대칭적으로 쌓아올려 만든 오토인코더에 대해 살펴봅니다. 기본적인 형태의 오토인코더는 자기 자신을 재생성하는 특징이 있고, 오토인코더의 입력에 저해상도 이미지를 넣으면 초해상도 이미지를 얻을 수 있고, 출력에 분할 이미지를 넣으면 이미지 분할 문제를 학습시킬 수도 있습니다.&lt;/p&gt;
&lt;p&gt;오토인코더의 중심에서 잠재변수를 추출하여, 정보를 압축하는 과정에서 데이터의 특징을 가장 잘 나타내도록 학습되기 때문에 이를 클러스터링에 사용해 고차원의 데이터를 저차원으로 시각화도 할 수 있습니다.&lt;/p&gt;
&lt;h2 id=&#34;vi-reference&#34;&gt;VI. Reference&lt;/h2&gt;
&lt;p&gt;김환희. (2020). 시작하세요! 텐서플로 2.0 프로그래밍: 기초 이론부터 실전 예제까지 한번에 끝내는 머신러닝, 딥러닝 핵심 가이드. 서울: 위키북스.&lt;/p&gt;
</description>
    </item>
    
  </channel>
</rss>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>시그노이드 on Data Science | ChloEvan</title>
    <link>https://chloevan.github.io/tags/%EC%8B%9C%EA%B7%B8%EB%85%B8%EC%9D%B4%EB%93%9C/</link>
    <description>Recent content in 시그노이드 on Data Science | ChloEvan</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Mon, 13 Apr 2020 21:40:30 +0900</lastBuildDate>
    
        <atom:link href="https://chloevan.github.io/tags/%EC%8B%9C%EA%B7%B8%EB%85%B8%EC%9D%B4%EB%93%9C/rss.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>Tensorflow 2.0 Tutorial ch3.3.5 - 세번째 신경망 네트워크 - XOR</title>
      <link>https://chloevan.github.io/python/tensorflow2.0/ch3_3_5_network_xor/</link>
      <pubDate>Mon, 13 Apr 2020 21:40:30 +0900</pubDate>
      
      <guid>https://chloevan.github.io/python/tensorflow2.0/ch3_3_5_network_xor/</guid>
      <description>&lt;h2 id=&#34;공지&#34;&gt;공지&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;본 Tutorial은 교재 &lt;code&gt;시작하세요 텐서플로 2.0 프로그래밍&lt;/code&gt;의 강사에게 국비교육 강의를 듣는 사람들에게 자료 제공을 목적으로 제작하였습니다.&lt;/li&gt;
&lt;li&gt;강사의 주관적인 판단으로 압축해서 자료를 정리하였기 때문에, 자세하게 공부를 하고 싶은 반드시 교재를 구매하실 것을 권해드립니다.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;img src=&#34;https://chloevan.github.io/img/tensorflow2.0/book.jpg&#34; alt=&#34;&#34;&gt;&lt;!-- raw HTML omitted --&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;본 교재 외에 강사가 추가한 내용에 대한 Reference를 확인하셔서, 추가적으로 학습하시는 것을 권유드립니다.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;tutorial&#34;&gt;Tutorial&lt;/h2&gt;
&lt;p&gt;이전 강의가 궁금하신 분들은 아래에서 선택하여 추가 학습 하시기를 바랍니다.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://chloevan.github.io/python/tensorflow2.0/googlecolab/&#34;&gt;Google Colab Tensorflow 2.0 Installation&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://chloevan.github.io/python/tensorflow2.0/ch3_3_1_random_signoid/&#34;&gt;Tensorflow 2.0 Tutorial ch3.3.1 - 난수 생성 및 시그모이드 함수&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://chloevan.github.io/python/tensorflow2.0/ch3_3_2_random_signoid_bias/&#34;&gt;Tensorflow 2.0 Tutorial ch3.3.2 - 난수 생성 및 시그모이드 함수 편향성&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://chloevan.github.io/python/tensorflow2.0/ch3_3_3_network_and/&#34;&gt;Tensorflow 2.0 Tutorial ch3.3.3 - 첫번째 신경망 네트워크: AND&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://chloevan.github.io/python/tensorflow2.0/ch3_3_4_network_or/&#34;&gt;Tensorflow 2.0 Tutorial ch3.3.4 - 두번째 신경망 네트워크: OR&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;i-xor-연산의-기본-개념&#34;&gt;I. XOR 연산의 기본 개념&lt;/h2&gt;
&lt;p&gt;&lt;code&gt;XOR&lt;/code&gt; 연산의 기본개념은 아래와 같습니다. 여기서 주의해야 할 점은, 홀수 개의 입력이 참일 때만 결과값이 참이라는 점입니다.&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;&lt;code&gt;입력1&lt;/code&gt;&lt;/th&gt;
&lt;th&gt;&lt;code&gt;입력2&lt;/code&gt;&lt;/th&gt;
&lt;th&gt;&lt;code&gt;AND 연산&lt;/code&gt;&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;참&lt;/td&gt;
&lt;td&gt;참&lt;/td&gt;
&lt;td&gt;거짓&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;참&lt;/td&gt;
&lt;td&gt;거짓&lt;/td&gt;
&lt;td&gt;참&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;거짓&lt;/td&gt;
&lt;td&gt;참&lt;/td&gt;
&lt;td&gt;참&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;거짓&lt;/td&gt;
&lt;td&gt;거짓&lt;/td&gt;
&lt;td&gt;거짓&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;파이썬에서는 참, 거짓을 나타내는 값은 &lt;code&gt;True&lt;/code&gt;, &lt;code&gt;False&lt;/code&gt;입니다. 그런데, 딥러닝의 주요 입력값은 정수(Integer)나 실수(float)입니다. 참과 거짓의 값을 출력하여 확인해봅니다.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;print&lt;/span&gt;(int(True))
&lt;span style=&#34;color:#66d9ef&#34;&gt;print&lt;/span&gt;(int(False))
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;pre&gt;&lt;code&gt;1
0
&lt;/code&gt;&lt;/pre&gt;
&lt;h2 id=&#34;ii-review--edit-for-xor&#34;&gt;II. Review &amp;amp; Edit For XOR&lt;/h2&gt;
&lt;p&gt;&lt;code&gt;XOR&lt;/code&gt; 연산의 네트워크를 구성하는 코드를 작성해봅니다. 이 때에도 동일하게 기존 코드에서 &lt;code&gt;y&lt;/code&gt;값만 수정해야 하니, 잘 참조하셔서 코드 작성하기를 바랍니다.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span style=&#34;color:#75715e&#34;&gt;# 본 예제&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;import&lt;/span&gt; tensorflow &lt;span style=&#34;color:#f92672&#34;&gt;as&lt;/span&gt; tf
&lt;span style=&#34;color:#f92672&#34;&gt;import&lt;/span&gt; numpy &lt;span style=&#34;color:#f92672&#34;&gt;as&lt;/span&gt; np

&lt;span style=&#34;color:#f92672&#34;&gt;import&lt;/span&gt; math
&lt;span style=&#34;color:#75715e&#34;&gt;# 시그모이드 함수 정의&lt;/span&gt;
&lt;span style=&#34;color:#66d9ef&#34;&gt;def&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;sigmoid&lt;/span&gt;(x): 
  &lt;span style=&#34;color:#66d9ef&#34;&gt;return&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt; &lt;span style=&#34;color:#f92672&#34;&gt;/&lt;/span&gt; (&lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt; &lt;span style=&#34;color:#f92672&#34;&gt;+&lt;/span&gt; math&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;exp(&lt;span style=&#34;color:#f92672&#34;&gt;-&lt;/span&gt;x))

x &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; np&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;array([[&lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;], [&lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;], [&lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;], [&lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;]])
y &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; np&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;array([[&lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;], [&lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;], [&lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;], [&lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;]])
w &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; tf&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;random&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;normal([&lt;span style=&#34;color:#ae81ff&#34;&gt;2&lt;/span&gt;], &lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;)
b &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; tf&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;random&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;normal([&lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;], &lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;)
b_x &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;

&lt;span style=&#34;color:#66d9ef&#34;&gt;for&lt;/span&gt; i &lt;span style=&#34;color:#f92672&#34;&gt;in&lt;/span&gt; range(&lt;span style=&#34;color:#ae81ff&#34;&gt;2000&lt;/span&gt;): 
  error_sum &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;

  &lt;span style=&#34;color:#66d9ef&#34;&gt;for&lt;/span&gt; j &lt;span style=&#34;color:#f92672&#34;&gt;in&lt;/span&gt; range(&lt;span style=&#34;color:#ae81ff&#34;&gt;4&lt;/span&gt;):
    output &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; sigmoid(np&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;sum(x[j] &lt;span style=&#34;color:#f92672&#34;&gt;*&lt;/span&gt; w) &lt;span style=&#34;color:#f92672&#34;&gt;+&lt;/span&gt; b_x &lt;span style=&#34;color:#f92672&#34;&gt;*&lt;/span&gt; b)
    error &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; y[j][&lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;] &lt;span style=&#34;color:#f92672&#34;&gt;-&lt;/span&gt; output
    w &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; w &lt;span style=&#34;color:#f92672&#34;&gt;+&lt;/span&gt; x[j] &lt;span style=&#34;color:#f92672&#34;&gt;*&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;0.1&lt;/span&gt; &lt;span style=&#34;color:#f92672&#34;&gt;*&lt;/span&gt; error
    b &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; b &lt;span style=&#34;color:#f92672&#34;&gt;+&lt;/span&gt; b_x &lt;span style=&#34;color:#f92672&#34;&gt;*&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;0.1&lt;/span&gt; &lt;span style=&#34;color:#f92672&#34;&gt;*&lt;/span&gt; error
    error_sum &lt;span style=&#34;color:#f92672&#34;&gt;+=&lt;/span&gt; error

  &lt;span style=&#34;color:#66d9ef&#34;&gt;if&lt;/span&gt; i &lt;span style=&#34;color:#f92672&#34;&gt;%&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;200&lt;/span&gt; &lt;span style=&#34;color:#f92672&#34;&gt;==&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;199&lt;/span&gt;:
    &lt;span style=&#34;color:#66d9ef&#34;&gt;print&lt;/span&gt;(i, error_sum)
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;pre&gt;&lt;code&gt;199 0.00035103711355610745
399 1.4262242213569643e-05
599 5.82624861134029e-07
799 1.8614212393686103e-09
999 1.8614210173240053e-09
1199 1.8614210173240053e-09
1399 1.8614210173240053e-09
1599 1.8614210173240053e-09
1799 1.8614210173240053e-09
1999 1.8614210173240053e-09
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;연산이 &lt;code&gt;999&lt;/code&gt; 이후 시점부터는 변하지 않는 것을 확인 할 수 있습니다.&lt;/p&gt;
&lt;h2 id=&#34;iii-xor-네트워크의-평가&#34;&gt;III. XOR 네트워크의 평가&lt;/h2&gt;
&lt;p&gt;일단, 네트워크 평가를 해봅니다.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;for&lt;/span&gt; i &lt;span style=&#34;color:#f92672&#34;&gt;in&lt;/span&gt; range(&lt;span style=&#34;color:#ae81ff&#34;&gt;4&lt;/span&gt;):
  &lt;span style=&#34;color:#66d9ef&#34;&gt;print&lt;/span&gt;(&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;X:&amp;#39;&lt;/span&gt;, x[i], &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;Y:&amp;#39;&lt;/span&gt;, y[i], &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;Output:&amp;#39;&lt;/span&gt;, sigmoid(np&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;sum(x[i]&lt;span style=&#34;color:#f92672&#34;&gt;*&lt;/span&gt;w)&lt;span style=&#34;color:#f92672&#34;&gt;+&lt;/span&gt;b))
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;pre&gt;&lt;code&gt;X: [1 1] Y: [0] Output: 0.5128176286712095
X: [1 0] Y: [1] Output: 0.5128176305326305
X: [0 1] Y: [1] Output: 0.4999999990686774
X: [0 0] Y: [0] Output: 0.5000000009313226
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;code&gt;Y&lt;/code&gt;와 &lt;code&gt;Output&lt;/code&gt; 사이에는 큰 차이가 있어 보이는데, &lt;code&gt;X&lt;/code&gt;가 변해도 &lt;code&gt;Output&lt;/code&gt;은 0.5 근처에서 머물고 있는데, 왜 그런결과가 나온걸까요?&lt;/p&gt;
&lt;h2 id=&#34;iv-xor-네트워크의-문제점&#34;&gt;IV. XOR 네트워크의 문제점&lt;/h2&gt;
&lt;p&gt;우선 &lt;code&gt;output = sigmoid(np.sum(x[j] * w) + b_x * b)&lt;/code&gt; 공식을 구성하는 &lt;code&gt;w&lt;/code&gt;와 &lt;code&gt;b&lt;/code&gt;를 출력해보면 다음과 같습니다.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;print&lt;/span&gt;(&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;w:&amp;#39;&lt;/span&gt;, w)
&lt;span style=&#34;color:#66d9ef&#34;&gt;print&lt;/span&gt;(&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;b:&amp;#39;&lt;/span&gt;, b)
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;pre&gt;&lt;code&gt;w: tf.Tensor([5.1281769e-02 3.7252903e-09], shape=(2,), dtype=float32)
b: tf.Tensor([-7.450581e-09], shape=(1,), dtype=float32)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;code&gt;w&lt;/code&gt;는 약 &lt;code&gt;0.0512&lt;/code&gt;, &lt;code&gt;-0.000000000745&lt;/code&gt;이고, &lt;code&gt;b&lt;/code&gt;는 &lt;code&gt;0.000000000372&lt;/code&gt; 입니다.&lt;/p&gt;
&lt;p&gt;조금더 구체적으로 XOR 네트워크의 중간값과 출력값이 어떻게 변하는지 확인하는 코드를 짜봅니다.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;for&lt;/span&gt; i &lt;span style=&#34;color:#f92672&#34;&gt;in&lt;/span&gt; range(&lt;span style=&#34;color:#ae81ff&#34;&gt;4&lt;/span&gt;):
  &lt;span style=&#34;color:#66d9ef&#34;&gt;print&lt;/span&gt;(&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;X:&amp;#39;&lt;/span&gt;, x[i], &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;Y:&amp;#39;&lt;/span&gt;, y[i], &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;Cal_mid:&amp;#39;&lt;/span&gt;, np&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;sum(x[i]&lt;span style=&#34;color:#f92672&#34;&gt;*&lt;/span&gt;w)&lt;span style=&#34;color:#f92672&#34;&gt;+&lt;/span&gt;b,&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;Output:&amp;#39;&lt;/span&gt;, sigmoid(np&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;sum(x[i]&lt;span style=&#34;color:#f92672&#34;&gt;*&lt;/span&gt;w)&lt;span style=&#34;color:#f92672&#34;&gt;+&lt;/span&gt;b))
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;pre&gt;&lt;code&gt;X: [1 1] Y: [0] Cal_mid: tf.Tensor([0.05128177], shape=(1,), dtype=float32) Output: 0.5128176323940516
X: [1 0] Y: [1] Cal_mid: tf.Tensor([0.05128176], shape=(1,), dtype=float32) Output: 0.5128176314633411
X: [0 1] Y: [1] Cal_mid: tf.Tensor([-3.7252903e-09], shape=(1,), dtype=float32) Output: 0.4999999990686774
X: [0 0] Y: [0] Cal_mid: tf.Tensor([-7.450581e-09], shape=(1,), dtype=float32) Output: 0.49999999813735485
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;첫번째 입력에 따라, &lt;code&gt;Cal_mid:&lt;/code&gt; 중간 계산값은 크게 달라지지만, &lt;code&gt;Output:&lt;/code&gt;은 큰 변동이 없는 것을 확인할 수 있습니다.&lt;/p&gt;
&lt;p&gt;&lt;code&gt;AND&lt;/code&gt; 네트워크와 비교해보면 그 차이는 더 명확해집니다.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span style=&#34;color:#75715e&#34;&gt;# 본 예제&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;import&lt;/span&gt; tensorflow &lt;span style=&#34;color:#f92672&#34;&gt;as&lt;/span&gt; tf
&lt;span style=&#34;color:#f92672&#34;&gt;import&lt;/span&gt; numpy &lt;span style=&#34;color:#f92672&#34;&gt;as&lt;/span&gt; np

&lt;span style=&#34;color:#f92672&#34;&gt;import&lt;/span&gt; math
&lt;span style=&#34;color:#75715e&#34;&gt;# 시그모이드 함수 정의&lt;/span&gt;
&lt;span style=&#34;color:#66d9ef&#34;&gt;def&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;sigmoid&lt;/span&gt;(x): 
  &lt;span style=&#34;color:#66d9ef&#34;&gt;return&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt; &lt;span style=&#34;color:#f92672&#34;&gt;/&lt;/span&gt; (&lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt; &lt;span style=&#34;color:#f92672&#34;&gt;+&lt;/span&gt; math&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;exp(&lt;span style=&#34;color:#f92672&#34;&gt;-&lt;/span&gt;x))

x &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; np&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;array([[&lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;], [&lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;], [&lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;], [&lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;]])
y &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; np&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;array([[&lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;], [&lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;], [&lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;], [&lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;]])
w &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; tf&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;random&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;normal([&lt;span style=&#34;color:#ae81ff&#34;&gt;2&lt;/span&gt;], &lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;)
b &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; tf&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;random&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;normal([&lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;], &lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;)
b_x &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;

&lt;span style=&#34;color:#66d9ef&#34;&gt;for&lt;/span&gt; i &lt;span style=&#34;color:#f92672&#34;&gt;in&lt;/span&gt; range(&lt;span style=&#34;color:#ae81ff&#34;&gt;2000&lt;/span&gt;): 
  error_sum &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;

  &lt;span style=&#34;color:#66d9ef&#34;&gt;for&lt;/span&gt; j &lt;span style=&#34;color:#f92672&#34;&gt;in&lt;/span&gt; range(&lt;span style=&#34;color:#ae81ff&#34;&gt;4&lt;/span&gt;): 
    output &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; sigmoid(np&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;sum(x[j] &lt;span style=&#34;color:#f92672&#34;&gt;*&lt;/span&gt; w) &lt;span style=&#34;color:#f92672&#34;&gt;+&lt;/span&gt; b_x &lt;span style=&#34;color:#f92672&#34;&gt;*&lt;/span&gt; b)
    error &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; y[j][&lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;] &lt;span style=&#34;color:#f92672&#34;&gt;-&lt;/span&gt; output
    w &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; w &lt;span style=&#34;color:#f92672&#34;&gt;+&lt;/span&gt; x[j] &lt;span style=&#34;color:#f92672&#34;&gt;*&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;0.1&lt;/span&gt; &lt;span style=&#34;color:#f92672&#34;&gt;*&lt;/span&gt; error
    b &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; b &lt;span style=&#34;color:#f92672&#34;&gt;+&lt;/span&gt; b_x &lt;span style=&#34;color:#f92672&#34;&gt;*&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;0.1&lt;/span&gt; &lt;span style=&#34;color:#f92672&#34;&gt;*&lt;/span&gt; error
    error_sum &lt;span style=&#34;color:#f92672&#34;&gt;+=&lt;/span&gt; error

  &lt;span style=&#34;color:#66d9ef&#34;&gt;if&lt;/span&gt; i &lt;span style=&#34;color:#f92672&#34;&gt;%&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;200&lt;/span&gt; &lt;span style=&#34;color:#f92672&#34;&gt;==&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;199&lt;/span&gt;:
    &lt;span style=&#34;color:#66d9ef&#34;&gt;print&lt;/span&gt;(i, error_sum)

&lt;span style=&#34;color:#66d9ef&#34;&gt;for&lt;/span&gt; i &lt;span style=&#34;color:#f92672&#34;&gt;in&lt;/span&gt; range(&lt;span style=&#34;color:#ae81ff&#34;&gt;4&lt;/span&gt;):
  &lt;span style=&#34;color:#66d9ef&#34;&gt;print&lt;/span&gt;(&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;X:&amp;#39;&lt;/span&gt;, x[i], &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;Y:&amp;#39;&lt;/span&gt;, y[i], &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;Cal_mid:&amp;#39;&lt;/span&gt;, np&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;sum(x[i]&lt;span style=&#34;color:#f92672&#34;&gt;*&lt;/span&gt;w)&lt;span style=&#34;color:#f92672&#34;&gt;+&lt;/span&gt;b,&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;Output:&amp;#39;&lt;/span&gt;, sigmoid(np&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;sum(x[i]&lt;span style=&#34;color:#f92672&#34;&gt;*&lt;/span&gt;w)&lt;span style=&#34;color:#f92672&#34;&gt;+&lt;/span&gt;b))
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;pre&gt;&lt;code&gt;199 -0.11459902898577794
399 -0.06719349972993488
599 -0.04743256822513228
799 -0.03654512498706137
999 -0.029666378761408263
1199 -0.024938480158672418
1399 -0.02149223338941213
1599 -0.01887404965969957
1799 -0.016816783475954222
1999 -0.0151601229142651
X: [1 1] Y: [1] Cal_mid: tf.Tensor([3.3052025], shape=(1,), dtype=float32) Output: 0.9646068559309787
X: [1 0] Y: [0] Cal_mid: tf.Tensor([-3.6603136], shape=(1,), dtype=float32) Output: 0.025079293237166324
X: [0 1] Y: [0] Cal_mid: tf.Tensor([-3.657158], shape=(1,), dtype=float32) Output: 0.02515656706949759
X: [0 0] Y: [0] Cal_mid: tf.Tensor([-10.622674], shape=(1,), dtype=float32) Output: 2.4356827795380005e-05
&lt;/code&gt;&lt;/pre&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;print&lt;/span&gt;(&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;w:&amp;#39;&lt;/span&gt;, w)
&lt;span style=&#34;color:#66d9ef&#34;&gt;print&lt;/span&gt;(&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;b:&amp;#39;&lt;/span&gt;, b)
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;pre&gt;&lt;code&gt;w: tf.Tensor([6.9623604 6.965516 ], shape=(2,), dtype=float32)
b: tf.Tensor([-10.622674], shape=(1,), dtype=float32)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;code&gt;AND&lt;/code&gt; 네트워크의 가중치가 하려는 작업은 XOR 네트워크에 비해 분명합니다. 편향값이 &lt;code&gt;-10.622&lt;/code&gt; 큰 음수인데, 두 가중치 &lt;code&gt;w1=6.9623604&lt;/code&gt;, &lt;code&gt;w2=6.965516&lt;/code&gt;를 모두 합쳐야 음수 편향을 이겨낼 수 있습니다.&lt;/p&gt;
&lt;p&gt;반면에 &lt;code&gt;XOR&lt;/code&gt; 네트워크는 어떤 일을 하려는지 명확하지 않습니다. 가중치 &lt;code&gt;w1=5.1281769e-02&lt;/code&gt;이 &lt;code&gt;w2=3.7252903e-09&lt;/code&gt;에 비해 조금 더 큰 값을 가지고 있기는 하지만, 중간값이 이미 0에 가까워지고 따라서 시그모이드 함수를 취한 값은 &lt;code&gt;0.5&lt;/code&gt;에 가까워질 뿐입니다.&lt;/p&gt;
&lt;p&gt;이부분이 &lt;code&gt;XOR&lt;/code&gt; 문제입니다. 하나의 퍼셉트론으로는 간단한 XOR 연산자도 만들어낼 수 없다는 것을 &lt;code&gt;퍼셉트론(Perceptron)&lt;/code&gt;에서 마빈 민스키(Marvin Minsky)와 시모어 페퍼트(Seymour Papert)가 증명해냈습니다.&lt;/p&gt;
&lt;p&gt;이러한 해결책으로 등장한 것이 여러 개의 퍼셉트론을 사용합니다. 딥러닝 자체가 사실 이러한 여러개의 퍼셉트론을 만들어 가는 과정이고, 단순 반복적인 코드를 함수화해서 구현하는 것이 딥러닝 프레임워크의 일반적인 전개 과정입니다.&lt;/p&gt;
&lt;h2 id=&#34;v-keras-model-모델-활용&#34;&gt;V. Keras Model 모델 활용&lt;/h2&gt;
&lt;p&gt;이제 &lt;code&gt;tf.keras&lt;/code&gt;를 사용해서 네트워크를 만드는 과정을 담습니다.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span style=&#34;color:#f92672&#34;&gt;import&lt;/span&gt; numpy &lt;span style=&#34;color:#f92672&#34;&gt;as&lt;/span&gt; np
x &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; np&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;array([[&lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;,&lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;], [&lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;,&lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;], [&lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;,&lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;], [&lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;,&lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;]])
y &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; np&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;array([[&lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;], [&lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;], [&lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;], [&lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;]])

model &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; tf&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;keras&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;Sequential([
    tf&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;keras&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;layers&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;Dense(units&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;2&lt;/span&gt;, activation&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;sigmoid&amp;#39;&lt;/span&gt;, input_shape&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;(&lt;span style=&#34;color:#ae81ff&#34;&gt;2&lt;/span&gt;,)), 
    tf&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;keras&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;layers&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;Dense(units&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;, activation&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;sigmoid&amp;#39;&lt;/span&gt;)     
])

model&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;compile(optimizer&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;tf&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;keras&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;optimizers&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;SGD(lr&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;0.1&lt;/span&gt;), loss&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;mse&amp;#39;&lt;/span&gt;)

model&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;summary()
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;pre&gt;&lt;code&gt;Model: &amp;quot;sequential_3&amp;quot;
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
dense_6 (Dense)              (None, 2)                 6         
_________________________________________________________________
dense_7 (Dense)              (None, 1)                 3         
=================================================================
Total params: 9
Trainable params: 9
Non-trainable params: 0
_________________________________________________________________
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;위 코드에 대한 설명은 다음과 같습니다.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;model: 딥러닝 계산을 위한 여러 함수와 변수의 묶음입니다.&lt;/li&gt;
&lt;li&gt;tf.keras.Sequential: 순차적(sequential)으로 뉴런과 뉴런이 합쳐진 단위인 레이어를 일직선으로 배치한 것입니다. 외국어 표기법으로 시퀀셜 네트워크, 시퀀셜 모델로 부릅니다.&lt;/li&gt;
&lt;li&gt;tf.keras.layers.Dense: 모델에서 사용하는 레이어를 정의하는 명령입니다. &lt;code&gt;Dense&lt;/code&gt;는 가장 기본적인 레이어로써, 레이어의 입력과 출력 사이에 있는 모든 뉴런이 서로 연결되는 레이어입니다.&lt;/li&gt;
&lt;li&gt;units: 레이어를 구성하는 뉴런의 수를 정의합니다.&lt;/li&gt;
&lt;li&gt;input_shape: 입력의 차원수를 정의합니다. x의 array가 각 데이터가 &lt;code&gt;[1, 1]&lt;/code&gt;, &lt;code&gt;[1, 0]&lt;/code&gt; 1차원 array이기 때문에 원소의 개수인 2를 명시해서 (2, )라고 정의했습니다.&lt;/li&gt;
&lt;li&gt;보통 Dense 레이어의 파라미터 수는 &lt;code&gt;(입력측 뉴런의 수 + 1) X (출력측 뉴런의 수)&lt;/code&gt;의 식으로 구할 수 있습니다. 여기서 입력측, 출력측이란 &lt;code&gt;Dense&lt;/code&gt; 레이어에 들어오는 입력을 입력측, &lt;code&gt;Dense&lt;/code&gt;레이어의 뉴런을 출력측이라고 합니다. 이 식에 따르면 첫 번째 레이어의 파라미터 수는 &lt;code&gt;(2+1) X 2 = 6&lt;/code&gt;이고, 두 번째 레이어의 파라미터 수는 &lt;code&gt;(2+1) X 1 = 3&lt;/code&gt;으로 [&lt;code&gt;OUT&lt;/code&gt;]에서 출력되는 결과와 동일합니다.&lt;/li&gt;
&lt;li&gt;optimizer: 최적화 함수라고 하며, 딥러닝의 학습식을 정의하는 부분입니다. 미리 정의된 최적화 함수를 불러올 수 있습니다. &lt;code&gt;SGD&lt;/code&gt;는 확률적 경사 하강법(Stochastic Gradient Descent)의 약자입니다. &lt;code&gt;확률적&lt;/code&gt;이라는 말의 뜻은 전체를 한번에 계산하지 않고 확률적으로 일부 샘플을 구해서 조금씩 나눠서 계산한다는 뜻입니다.&lt;/li&gt;
&lt;li&gt;loss(손실): 앞에서 살펴본 error와 비슷한 개념입니다. 평균 제곱 오차(Mean Squared Error)의 약자로 기대출력에서 실제출력을 뺀 뒤에 제곱한 값을 평균하는 것입니다. 수식으로 나타내면 다음과 같습니다.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;$$Mean Squared Error = \frac{1}{n}\sum_{k=1}^{n}\left (y_{k} - output_{k}\right )^2$$&lt;/p&gt;
&lt;p&gt;앞의 예제들에서 사용했던 에러 식 $error = y - output$과 비슷한 기능을 합니다.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;model.summary()는 현재 네트워크의 구조를 알아보기 위해 쉽게 출력하는 기능입니다. 이 때 에러가 난다면 앞의 소스코드를 재확인 해야 합니다.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;이제 실제로 모형을 학습시킵니다.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;history &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; model&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;fit(x, y, epochs&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;2000&lt;/span&gt;, batch_size&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;)
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;pre&gt;&lt;code&gt;Epoch 1/2000
4/4 [==============================] - 0s 2ms/step - loss: 0.2670
Epoch 2/2000
4/4 [==============================] - 0s 2ms/step - loss: 0.2653
Epoch 3/2000
4/4 [==============================] - 0s 1ms/step - loss: 0.2638
Epoch 4/2000
4/4 [==============================] - 0s 1ms/step - loss: 0.2625
Epoch 5/2000
4/4 [==============================] - 0s 1ms/step - loss: 0.2615
Epoch 6/2000
.
.
.
Epoch 1997/2000
4/4 [==============================] - 0s 2ms/step - loss: 0.2516
Epoch 1998/2000
4/4 [==============================] - 0s 2ms/step - loss: 0.2516
Epoch 1999/2000
4/4 [==============================] - 0s 2ms/step - loss: 0.2516
Epoch 2000/2000
4/4 [==============================] - 0s 2ms/step - loss: 0.2516
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;code&gt;for&lt;/code&gt;문을 실행한 것처럼 에포크(epochs)에 지정된 횟수만큼 학습시킵니다. &lt;code&gt;batch_size&lt;/code&gt;는 한번에 학습시키는 데이터의 수인데, 여기서는 1로 지정해서 입력을 넣었을 때 정확한 값을 출력하는지 알아보려고 합니다. 첫 부분의 x, y는 각각 입력과 기대출력을 나타냅니다.&lt;/p&gt;
&lt;p&gt;학습이 끝나면 네트워크를 평가해볼 수 있습니다.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;model&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;predict(x)
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;pre&gt;&lt;code&gt;array([[0.05740971 ],
       [0.9505177 ],
       [0.95034307],
       [0.03410044]], dtype=float32)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;첫번째와 네번째 값이 0에 가깝고, 두번째와 세번째 값은 1에 가깝게 나온 것을 확인할 수 있습니다. &lt;code&gt;XOR&lt;/code&gt;과 비교했을 때보다 네트워크를 좀 더 잘 계산하고 있는 것을 확인할 수 있습니다.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;for&lt;/span&gt; weight &lt;span style=&#34;color:#f92672&#34;&gt;in&lt;/span&gt; model&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;weights:
  &lt;span style=&#34;color:#66d9ef&#34;&gt;print&lt;/span&gt;(weight)
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;pre&gt;&lt;code&gt;&amp;lt;tf.Variable &#39;dense_2/kernel:0&#39; shape=(2, 2) dtype=float32, numpy=
array([[-0.90128756,  0.5459947 ],
       [ 1.5134072 , -1.2097402 ]], dtype=float32)&amp;gt;
&amp;lt;tf.Variable &#39;dense_2/bias:0&#39; shape=(2,) dtype=float32, numpy=array([ 0.6614232, -0.3731091], dtype=float32)&amp;gt;
&amp;lt;tf.Variable &#39;dense_3/kernel:0&#39; shape=(2, 1) dtype=float32, numpy=
array([[-0.7053246 ],
       [ 0.23433231]], dtype=float32)&amp;gt;
&amp;lt;tf.Variable &#39;dense_3/bias:0&#39; shape=(1,) dtype=float32, numpy=array([0.3860617], dtype=float32)&amp;gt;
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;가중치 정보는 &lt;code&gt;model.weights&lt;/code&gt;에 저장되어 있습니다. 입력과 레이어 또는 레이어 사이의 뉴런을 연결할 때 사용되는 가중치는 &lt;code&gt;kernel&lt;/code&gt;이고, 편향과 연결된 가중치는 &lt;code&gt;bias&lt;/code&gt;로 표시됩니다.&lt;/p&gt;
&lt;p&gt;보통 네트워크의 가중치 숫자가 많기 때문에 구분을 위해 편의상 가중치에 첨자를 붙여서 표시합니다. 레이어의 순서대로 위첨자를 붙이고, 아래첨자는 각 뉴런의 순서에 맞게 차례로 붙입니다.&lt;/p&gt;
&lt;p&gt;뉴런의 개수가 3개, 레이어 개수가 2개로 늘자 이 가중치들이 무슨 일을 하는지 한눈에 잘 들어오지 않습니다. 뉴런과 레이어가 많아지면 이 문제는 더욱 커집니다.&lt;/p&gt;
&lt;p&gt;가중치 시각화보다 네트워크의 학습 상황을 더 잘 파악할 수 있는 방법이 필요한데, &lt;code&gt;matplotlib.pyplot&lt;/code&gt;을 활용하여 시각화를 진행합니다.&lt;/p&gt;
&lt;h2 id=&#34;vi-시각화-기초&#34;&gt;VI. 시각화 기초&lt;/h2&gt;
&lt;h2 id=&#34;1-간단한-꺾은선-그래프-그리기&#34;&gt;(1) 간단한 꺾은선 그래프 그리기&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;x = range(20)&lt;/code&gt;으로 &lt;code&gt;[0, 1, 2, ..., 19]&lt;/code&gt;의 20개의 정수로 구성된 리스트를 넣었습니다.&lt;/li&gt;
&lt;li&gt;&lt;code&gt;y&lt;/code&gt; 난수 생성을 활용해 랜덤 데이터를 변수에 저장했습니다.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;code&gt;plt.plot(x, y)&lt;/code&gt;는 &lt;code&gt;x&lt;/code&gt;축, &lt;code&gt;y&lt;/code&gt;축에 각각 x, y를 넣어서 그래프를 그린 후, &lt;code&gt;plt.show()&lt;/code&gt;함수를 호출합니다.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span style=&#34;color:#f92672&#34;&gt;import&lt;/span&gt; matplotlib.pyplot &lt;span style=&#34;color:#f92672&#34;&gt;as&lt;/span&gt; plt
x &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; range(&lt;span style=&#34;color:#ae81ff&#34;&gt;20&lt;/span&gt;)
y &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; tf&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;random&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;normal([&lt;span style=&#34;color:#ae81ff&#34;&gt;20&lt;/span&gt;], &lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;)
plt&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;plot(x, y)
plt&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;show()
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;&lt;img src=&#34;https://chloevan.github.io/img/tensorflow2.0/tutorial_03_05/tutorial_01.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;h2 id=&#34;2-정규분포-히스토그램-그리기&#34;&gt;(2) 정규분포 히스토그램 그리기&lt;/h2&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span style=&#34;color:#f92672&#34;&gt;import&lt;/span&gt; matplotlib.pyplot &lt;span style=&#34;color:#f92672&#34;&gt;as&lt;/span&gt; plt
random_normal &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; tf&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;random&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;normal([&lt;span style=&#34;color:#ae81ff&#34;&gt;100000&lt;/span&gt;], &lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;)
plt&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;hist(random_normal, bins &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;100&lt;/span&gt;)
plt&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;show()
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;&lt;img src=&#34;https://chloevan.github.io/img/tensorflow2.0/tutorial_03_05/tutorial_02.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;h2 id=&#34;v-2-레이어-xor-네트워크의-정보-시각화&#34;&gt;V. 2-레이어 XOR 네트워크의 정보 시각화&lt;/h2&gt;
&lt;p&gt;딥러닝을 학습시킬 때 가장 많이 보게 되는 그래프는 바로 학습이 잘 되고 있는지 확인하기 위한 측정치(&lt;code&gt;metric&lt;/code&gt;)변화량을 나타내는 선 그래프입니다. 여기서는 선 그래프를 이용해서 손실이 어떻게 변했는지를 알아보겠습니다.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span style=&#34;color:#f92672&#34;&gt;import&lt;/span&gt; matplotlib.pyplot &lt;span style=&#34;color:#f92672&#34;&gt;as&lt;/span&gt; plt
plt&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;plot(history&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;history[&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;loss&amp;#39;&lt;/span&gt;])
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;pre&gt;&lt;code&gt;[&amp;lt;matplotlib.lines.Line2D at 0x7f21635d7208&amp;gt;]
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://chloevan.github.io/img/tensorflow2.0/tutorial_03_05/tutorial_03.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;p&gt;그래프를 통해 처음에는 손실이 서서히 감소하다가 어느 시점부터 급격히 감소하고, 나중에는 거의 감소하지 않는 뒤집힌 S자 곡선을 그리는 모습을 확인할 수 있습니다. 이렇게 손실을 시각화하면 네트워크의 학습 현황을 한눈에 파악할 수 있습니다.&lt;/p&gt;
&lt;h2 id=&#34;vii-연습-파일&#34;&gt;VII. 연습 파일&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://colab.research.google.com/github/chloevan/deeplearningAI/blob/master/tensorflow2.0/ch3_3_5_Network_XOR.ipynb&#34;&gt;구글 Colab에서 직접 연습해보자&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;viii-reference&#34;&gt;VIII. Reference&lt;/h2&gt;
&lt;p&gt;김환희. (2020). 시작하세요! 텐서플로 2.0 프로그래밍: 기초 이론부터 실전 예제까지 한번에 끝내는 머신러닝, 딥러닝 핵심 가이드. 서울: 위키북스.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Tensorflow 2.0 Tutorial ch3.3.4 - 두번째 신경망 네트워크: OR</title>
      <link>https://chloevan.github.io/python/tensorflow2.0/ch3_3_4_network_or/</link>
      <pubDate>Sun, 12 Apr 2020 23:40:30 +0900</pubDate>
      
      <guid>https://chloevan.github.io/python/tensorflow2.0/ch3_3_4_network_or/</guid>
      <description>&lt;h2 id=&#34;공지&#34;&gt;공지&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;본 Tutorial은 교재 &lt;code&gt;시작하세요 텐서플로 2.0 프로그래밍&lt;/code&gt;의 강사에게 국비교육 강의를 듣는 사람들에게 자료 제공을 목적으로 제작하였습니다.&lt;/li&gt;
&lt;li&gt;강사의 주관적인 판단으로 압축해서 자료를 정리하였기 때문에, 자세하게 공부를 하고 싶은 반드시 교재를 구매하실 것을 권해드립니다.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;img src=&#34;https://chloevan.github.io/img/tensorflow2.0/book.jpg&#34; alt=&#34;&#34;&gt;&lt;!-- raw HTML omitted --&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;본 교재 외에 강사가 추가한 내용에 대한 Reference를 확인하셔서, 추가적으로 학습하시는 것을 권유드립니다.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;tutorial&#34;&gt;Tutorial&lt;/h2&gt;
&lt;p&gt;이전 강의가 궁금하신 분들은 아래에서 선택하여 추가 학습 하시기를 바랍니다.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://chloevan.github.io/python/tensorflow2.0/googlecolab/&#34;&gt;Google Colab Tensorflow 2.0 Installation&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://chloevan.github.io/python/tensorflow2.0/ch3_3_1_random_signoid/&#34;&gt;Tensorflow 2.0 Tutorial ch3.3.1 - 난수 생성 및 시그모이드 함수&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://chloevan.github.io/python/tensorflow2.0/ch3_3_2_random_signoid_bias/&#34;&gt;Tensorflow 2.0 Tutorial ch3.3.2 - 난수 생성 및 시그모이드 함수 편향성&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://chloevan.github.io/python/tensorflow2.0/ch3_3_3_network_and/&#34;&gt;Tensorflow 2.0 Tutorial ch3.3.3 - 첫번째 신경망 네트워크: AND&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;i-or-연산의-기본-개념&#34;&gt;I. OR 연산의 기본 개념&lt;/h2&gt;
&lt;p&gt;&lt;code&gt;OR&lt;/code&gt; 연산의 기본개념은 아래와 같습니다. &lt;code&gt;AND&lt;/code&gt;와 달리 하나만 참이어도, 결과값은 모두 참이 됩니다.&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;&lt;code&gt;입력1&lt;/code&gt;&lt;/th&gt;
&lt;th&gt;&lt;code&gt;입력2&lt;/code&gt;&lt;/th&gt;
&lt;th&gt;&lt;code&gt;AND 연산&lt;/code&gt;&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;참&lt;/td&gt;
&lt;td&gt;참&lt;/td&gt;
&lt;td&gt;참&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;참&lt;/td&gt;
&lt;td&gt;거짓&lt;/td&gt;
&lt;td&gt;참&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;거짓&lt;/td&gt;
&lt;td&gt;참&lt;/td&gt;
&lt;td&gt;참&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;거짓&lt;/td&gt;
&lt;td&gt;거짓&lt;/td&gt;
&lt;td&gt;거짓&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;파이썬에서는 참, 거짓을 나타내는 값은 &lt;code&gt;True&lt;/code&gt;, &lt;code&gt;False&lt;/code&gt;입니다. 그런데, 딥러닝의 주요 입력값은 정수(Integer)나 실수(float)입니다. 참과 거짓의 값을 출력하여 확인해봅니다.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;print&lt;/span&gt;(int(True))
&lt;span style=&#34;color:#66d9ef&#34;&gt;print&lt;/span&gt;(int(False))
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;pre&gt;&lt;code&gt;1
0
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;참은 1로 출력하고 거짓은 0으로 출력한 것을 확인하였습니다. 다시 &lt;code&gt;OR&lt;/code&gt; 연산 기본개념에 적용하면 아래와 같습니다.&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;&lt;code&gt;입력1&lt;/code&gt;&lt;/th&gt;
&lt;th&gt;&lt;code&gt;입력2&lt;/code&gt;&lt;/th&gt;
&lt;th&gt;&lt;code&gt;AND 연산&lt;/code&gt;&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;1&lt;/td&gt;
&lt;td&gt;1&lt;/td&gt;
&lt;td&gt;1&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;1&lt;/td&gt;
&lt;td&gt;0&lt;/td&gt;
&lt;td&gt;1&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;0&lt;/td&gt;
&lt;td&gt;1&lt;/td&gt;
&lt;td&gt;1&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;0&lt;/td&gt;
&lt;td&gt;0&lt;/td&gt;
&lt;td&gt;0&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;h2 id=&#34;ii-review--edit-for-or&#34;&gt;II. Review &amp;amp; Edit For OR&lt;/h2&gt;
&lt;p&gt;&lt;code&gt;OR&lt;/code&gt; 연산을 계산하는 네트워크를 생성하는 코드도 예제 3.16의 &lt;code&gt;AND&lt;/code&gt; 네트워크와 매우 비슷합니다. 달라지는 것은 &lt;code&gt;y&lt;/code&gt; 부분의 기대출력뿐입니다.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span style=&#34;color:#75715e&#34;&gt;# 본 예제&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;import&lt;/span&gt; tensorflow &lt;span style=&#34;color:#f92672&#34;&gt;as&lt;/span&gt; tf
&lt;span style=&#34;color:#f92672&#34;&gt;import&lt;/span&gt; numpy &lt;span style=&#34;color:#f92672&#34;&gt;as&lt;/span&gt; np

&lt;span style=&#34;color:#f92672&#34;&gt;import&lt;/span&gt; math
&lt;span style=&#34;color:#75715e&#34;&gt;# 시그모이드 함수 정의&lt;/span&gt;
&lt;span style=&#34;color:#66d9ef&#34;&gt;def&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;sigmoid&lt;/span&gt;(x): 
  &lt;span style=&#34;color:#66d9ef&#34;&gt;return&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt; &lt;span style=&#34;color:#f92672&#34;&gt;/&lt;/span&gt; (&lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt; &lt;span style=&#34;color:#f92672&#34;&gt;+&lt;/span&gt; math&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;exp(&lt;span style=&#34;color:#f92672&#34;&gt;-&lt;/span&gt;x))

x &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; np&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;array([[&lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;], [&lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;], [&lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;], [&lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;]])
y &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; np&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;array([[&lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;], [&lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;], [&lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;], [&lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;]])
w &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; tf&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;random&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;normal([&lt;span style=&#34;color:#ae81ff&#34;&gt;2&lt;/span&gt;], &lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;)
b &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; tf&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;random&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;normal([&lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;], &lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;)
b_x &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;

&lt;span style=&#34;color:#66d9ef&#34;&gt;for&lt;/span&gt; i &lt;span style=&#34;color:#f92672&#34;&gt;in&lt;/span&gt; range(&lt;span style=&#34;color:#ae81ff&#34;&gt;2000&lt;/span&gt;): 
  error_sum &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;

  &lt;span style=&#34;color:#66d9ef&#34;&gt;for&lt;/span&gt; j &lt;span style=&#34;color:#f92672&#34;&gt;in&lt;/span&gt; range(&lt;span style=&#34;color:#ae81ff&#34;&gt;4&lt;/span&gt;): 
    output &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; sigmoid(np&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;sum(x[j] &lt;span style=&#34;color:#f92672&#34;&gt;*&lt;/span&gt; w) &lt;span style=&#34;color:#f92672&#34;&gt;+&lt;/span&gt; b_x &lt;span style=&#34;color:#f92672&#34;&gt;*&lt;/span&gt; b)
    error &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; y[j][&lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;] &lt;span style=&#34;color:#f92672&#34;&gt;-&lt;/span&gt; output
    w &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; w &lt;span style=&#34;color:#f92672&#34;&gt;+&lt;/span&gt; x[j] &lt;span style=&#34;color:#f92672&#34;&gt;*&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;0.1&lt;/span&gt; &lt;span style=&#34;color:#f92672&#34;&gt;*&lt;/span&gt; error
    b &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; b &lt;span style=&#34;color:#f92672&#34;&gt;+&lt;/span&gt; b_x &lt;span style=&#34;color:#f92672&#34;&gt;*&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;0.1&lt;/span&gt; &lt;span style=&#34;color:#f92672&#34;&gt;*&lt;/span&gt; error
    error_sum &lt;span style=&#34;color:#f92672&#34;&gt;+=&lt;/span&gt; error

  &lt;span style=&#34;color:#66d9ef&#34;&gt;if&lt;/span&gt; i &lt;span style=&#34;color:#f92672&#34;&gt;%&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;200&lt;/span&gt; &lt;span style=&#34;color:#f92672&#34;&gt;==&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;199&lt;/span&gt;:
    &lt;span style=&#34;color:#66d9ef&#34;&gt;print&lt;/span&gt;(i, error_sum)
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;pre&gt;&lt;code&gt;199 -0.04753116837317456
399 -0.025197255723869036
599 -0.017073852769690062
799 -0.012871429837134879
999 -0.010311318147416329
1199 -0.008592360635920386
1399 -0.0073599238611461795
1599 -0.006433635333937565
1799 -0.005713413823827767
1999 -0.0051357747755025746
&lt;/code&gt;&lt;/pre&gt;
&lt;h2 id=&#34;iii-or-네트워크의-평가&#34;&gt;III. OR 네트워크의 평가&lt;/h2&gt;
&lt;p&gt;&lt;code&gt;OR&lt;/code&gt; 네트워크를 평가하는 코드와 결과는 다음과 같습니다.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;for&lt;/span&gt; i &lt;span style=&#34;color:#f92672&#34;&gt;in&lt;/span&gt; range(&lt;span style=&#34;color:#ae81ff&#34;&gt;4&lt;/span&gt;): 
  &lt;span style=&#34;color:#66d9ef&#34;&gt;print&lt;/span&gt;(&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;X:&amp;#39;&lt;/span&gt;, x[i], &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;Y:&amp;#39;&lt;/span&gt;, y[i], &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;Output:&amp;#39;&lt;/span&gt;, sigmoid(np&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;sum(x[i]&lt;span style=&#34;color:#f92672&#34;&gt;*&lt;/span&gt;w)&lt;span style=&#34;color:#f92672&#34;&gt;+&lt;/span&gt;b))
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;pre&gt;&lt;code&gt;X: [1 1] Y: [1] Output: 0.9999972280734293
X: [1 0] Y: [1] Output: 0.9898012421887588
X: [0 1] Y: [1] Output: 0.9898094027343277
X: [0 0] Y: [0] Output: 0.025464390776753325
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;학습 수&lt;code&gt;(for i in range(2000):의 2000)&lt;/code&gt;를 바꿔가며 실습하는 것을 권장합니다. 학습 수가 커지면 커질수록 실제 출력이 기대출력 값에 가까워지는 것을 확인할 수 있습니다.&lt;/p&gt;
&lt;h2 id=&#34;iv-연습-파일&#34;&gt;IV. 연습 파일&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://colab.research.google.com/github/chloevan/deeplearningAI/blob/master/tensorflow2.0/ch3_3_4_Network_OR.ipynb&#34;&gt;구글 Colab에서 직접 연습해보자&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;v-reference&#34;&gt;V. Reference&lt;/h2&gt;
&lt;p&gt;김환희. (2020). 시작하세요! 텐서플로 2.0 프로그래밍: 기초 이론부터 실전 예제까지 한번에 끝내는 머신러닝, 딥러닝 핵심 가이드. 서울: 위키북스.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Tensorflow 2.0 Tutorial ch3.3.3 - 첫번째 신경망 네트워크, AND</title>
      <link>https://chloevan.github.io/python/tensorflow2.0/ch3_3_3_network_and/</link>
      <pubDate>Sat, 11 Apr 2020 11:20:30 +0900</pubDate>
      
      <guid>https://chloevan.github.io/python/tensorflow2.0/ch3_3_3_network_and/</guid>
      <description>&lt;h2 id=&#34;공지&#34;&gt;공지&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;본 Tutorial은 교재 &lt;code&gt;시작하세요 텐서플로 2.0 프로그래밍&lt;/code&gt;의 강사에게 국비교육 강의를 듣는 사람들에게 자료 제공을 목적으로 제작하였습니다.&lt;/li&gt;
&lt;li&gt;강사의 주관적인 판단으로 압축해서 자료를 정리하였기 때문에, 자세하게 공부를 하고 싶은 반드시 교재를 구매하실 것을 권해드립니다.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;img src=&#34;https://chloevan.github.io/img/tensorflow2.0/book.jpg&#34; alt=&#34;&#34;&gt;&lt;!-- raw HTML omitted --&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;본 교재 외에 강사가 추가한 내용에 대한 Reference를 확인하셔서, 추가적으로 학습하시는 것을 권유드립니다.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;tutorial&#34;&gt;Tutorial&lt;/h2&gt;
&lt;p&gt;이전 강의가 궁금하신 분들은 아래에서 선택하여 추가 학습 하시기를 바랍니다.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://chloevan.github.io/python/tensorflow2.0/googlecolab/&#34;&gt;Google Colab Tensorflow 2.0 Installation&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://chloevan.github.io/python/tensorflow2.0/ch3_3_1_random_signoid/&#34;&gt;Tensorflow 2.0 Tutorial ch3.3.1 - 난수 생성 및 시그모이드 함수&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://chloevan.github.io/python/tensorflow2.0/ch3_3_2_random_signoid_bias/&#34;&gt;Tensorflow 2.0 Tutorial ch3.3.2 - 난수 생성 및 시그모이드 함수 편향성&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;i-and-연산의-기본-개념&#34;&gt;I. AND 연산의 기본 개념&lt;/h2&gt;
&lt;p&gt;AND 연산의 기본개념은 아래와 같습니다. 다른 프로그래밍과 다르지 않습니다.&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;&lt;code&gt;입력1&lt;/code&gt;&lt;/th&gt;
&lt;th&gt;&lt;code&gt;입력2&lt;/code&gt;&lt;/th&gt;
&lt;th&gt;&lt;code&gt;AND 연산&lt;/code&gt;&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;참&lt;/td&gt;
&lt;td&gt;참&lt;/td&gt;
&lt;td&gt;참&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;참&lt;/td&gt;
&lt;td&gt;거짓&lt;/td&gt;
&lt;td&gt;거짓&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;거짓&lt;/td&gt;
&lt;td&gt;참&lt;/td&gt;
&lt;td&gt;거짓&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;거짓&lt;/td&gt;
&lt;td&gt;거짓&lt;/td&gt;
&lt;td&gt;거짓&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;파이썬에서는 참, 거짓을 나타내는 값은 &lt;code&gt;True&lt;/code&gt;, &lt;code&gt;False&lt;/code&gt;입니다. 그런데, 딥러닝의 주요 입력값은 정수(Integer)나 실수(float)입니다. 참과 거짓의 값을 출력하여 확인해봅니다.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;print&lt;/span&gt;(int(True))
&lt;span style=&#34;color:#66d9ef&#34;&gt;print&lt;/span&gt;(int(False))
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;pre&gt;&lt;code&gt;1
0
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;참은 1로 출력하고 거짓은 0으로 출력한 것을 확인하였습니다. 다시 AND 연산 기본개념에 적용하면 아래와 같습니다.&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;&lt;code&gt;입력1&lt;/code&gt;&lt;/th&gt;
&lt;th&gt;&lt;code&gt;입력2&lt;/code&gt;&lt;/th&gt;
&lt;th&gt;&lt;code&gt;AND 연산&lt;/code&gt;&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;1&lt;/td&gt;
&lt;td&gt;1&lt;/td&gt;
&lt;td&gt;1&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;1&lt;/td&gt;
&lt;td&gt;0&lt;/td&gt;
&lt;td&gt;0&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;0&lt;/td&gt;
&lt;td&gt;1&lt;/td&gt;
&lt;td&gt;0&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;0&lt;/td&gt;
&lt;td&gt;0&lt;/td&gt;
&lt;td&gt;0&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;이제 숫자로 된 네개의 입력과 출력(AND 연산)의 쌍이 생겼습니다. 이제 AND 연산을 할 수 있는 신경망 네트워크를 만들어봅니다.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span style=&#34;color:#75715e&#34;&gt;# 본 예제&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;import&lt;/span&gt; tensorflow &lt;span style=&#34;color:#f92672&#34;&gt;as&lt;/span&gt; tf
&lt;span style=&#34;color:#f92672&#34;&gt;import&lt;/span&gt; numpy &lt;span style=&#34;color:#f92672&#34;&gt;as&lt;/span&gt; np

&lt;span style=&#34;color:#f92672&#34;&gt;import&lt;/span&gt; math
&lt;span style=&#34;color:#75715e&#34;&gt;# 시그모이드 함수 정의&lt;/span&gt;
&lt;span style=&#34;color:#66d9ef&#34;&gt;def&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;sigmoid&lt;/span&gt;(x): 
  &lt;span style=&#34;color:#66d9ef&#34;&gt;return&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt; &lt;span style=&#34;color:#f92672&#34;&gt;/&lt;/span&gt; (&lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt; &lt;span style=&#34;color:#f92672&#34;&gt;+&lt;/span&gt; math&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;exp(&lt;span style=&#34;color:#f92672&#34;&gt;-&lt;/span&gt;x))

x &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; np&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;array([[&lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;], [&lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;], [&lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;], [&lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;]])
y &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; np&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;array([[&lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;], [&lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;], [&lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;], [&lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;]])
w &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; tf&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;random&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;normal([&lt;span style=&#34;color:#ae81ff&#34;&gt;2&lt;/span&gt;], &lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;)
b &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; tf&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;random&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;normal([&lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;], &lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;)
b_x &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;

&lt;span style=&#34;color:#66d9ef&#34;&gt;for&lt;/span&gt; i &lt;span style=&#34;color:#f92672&#34;&gt;in&lt;/span&gt; range(&lt;span style=&#34;color:#ae81ff&#34;&gt;2000&lt;/span&gt;): 
  error_sum &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;

  &lt;span style=&#34;color:#66d9ef&#34;&gt;for&lt;/span&gt; j &lt;span style=&#34;color:#f92672&#34;&gt;in&lt;/span&gt; range(&lt;span style=&#34;color:#ae81ff&#34;&gt;4&lt;/span&gt;): 
    output &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; sigmoid(np&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;sum(x[j] &lt;span style=&#34;color:#f92672&#34;&gt;*&lt;/span&gt; w) &lt;span style=&#34;color:#f92672&#34;&gt;+&lt;/span&gt; b_x &lt;span style=&#34;color:#f92672&#34;&gt;*&lt;/span&gt; b)
    error &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; y[j][&lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;] &lt;span style=&#34;color:#f92672&#34;&gt;-&lt;/span&gt; output
    w &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; w &lt;span style=&#34;color:#f92672&#34;&gt;+&lt;/span&gt; x[j] &lt;span style=&#34;color:#f92672&#34;&gt;*&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;0.1&lt;/span&gt; &lt;span style=&#34;color:#f92672&#34;&gt;*&lt;/span&gt; error
    b &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; b &lt;span style=&#34;color:#f92672&#34;&gt;+&lt;/span&gt; b_x &lt;span style=&#34;color:#f92672&#34;&gt;*&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;0.1&lt;/span&gt; &lt;span style=&#34;color:#f92672&#34;&gt;*&lt;/span&gt; error
    error_sum &lt;span style=&#34;color:#f92672&#34;&gt;+=&lt;/span&gt; error

  &lt;span style=&#34;color:#66d9ef&#34;&gt;if&lt;/span&gt; i &lt;span style=&#34;color:#f92672&#34;&gt;%&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;200&lt;/span&gt; &lt;span style=&#34;color:#f92672&#34;&gt;==&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;199&lt;/span&gt;:
    &lt;span style=&#34;color:#66d9ef&#34;&gt;print&lt;/span&gt;(i, error_sum)
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;pre&gt;&lt;code&gt;199 -0.10664604227192419
399 -0.0643946715061894
599 -0.04600188172236917
799 -0.03567996639075575
999 -0.029089461092594807
1199 -0.024526551513673306
1399 -0.0211850336803021
1599 -0.018634950907257527
1799 -0.016626949381179347
1999 -0.01500491979044496
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;code&gt;NumPy&lt;/code&gt;에 관한 내용은 여기에서는 생략합니다.&lt;/p&gt;
&lt;p&gt;위에 가중치(&lt;code&gt;w&lt;/code&gt;)에 적용된 수식은 다음과 같습니다. $$w = w + x[j] \times 0.1 \times error$$&lt;/p&gt;
&lt;p&gt;여기서는 $x[j]\times0.1$ 부분의 계산을 빨리 하기 위해 &lt;code&gt;NumPy Array&lt;/code&gt;를 활용했습니다.&lt;/p&gt;
&lt;h2 id=&#34;ii-numpy-vs-list&#34;&gt;II. NumPy VS List&lt;/h2&gt;
&lt;p&gt;여기에서 NumPy와 List를 혼동할 때가 많습니다. 메모리 연산 등에 비교하는 문서가 필요하면 &lt;a href=&#34;https://www.geeksforgeeks.org/python-lists-vs-numpy-arrays/&#34;&gt;Python Lists VS Numpy Arrays&lt;/a&gt;에서 확인 후 직접 코드 실행을 해서 익히기를 바랍니다.&lt;/p&gt;
&lt;p&gt;딥러닝에서 확인하고 싶은 것은 왜 List 대신에 NumPy가 사용되는 것인지에 대한 내용입니다.&lt;/p&gt;
&lt;p&gt;수식을 보면, 오차(&lt;code&gt;error&lt;/code&gt;)가 먼저 계산되고 난 뒤, 가중치를 업데이트 하게 됩니다. 이 때 만약 리스트로 값이 이루어진다면, 오차가 &lt;code&gt;0&lt;/code&gt;이거나 또는 &lt;code&gt;음수 &lt;/code&gt;일 때는 Empty(빈)리스트가 반환될 것입니다.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;print&lt;/span&gt;([&lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;2&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;3&lt;/span&gt;] &lt;span style=&#34;color:#f92672&#34;&gt;*&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;2&lt;/span&gt;)
&lt;span style=&#34;color:#66d9ef&#34;&gt;print&lt;/span&gt;([&lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;2&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;3&lt;/span&gt;] &lt;span style=&#34;color:#f92672&#34;&gt;*&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;)
&lt;span style=&#34;color:#66d9ef&#34;&gt;print&lt;/span&gt;([&lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;2&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;3&lt;/span&gt;] &lt;span style=&#34;color:#f92672&#34;&gt;*&lt;/span&gt; &lt;span style=&#34;color:#f92672&#34;&gt;-&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;)
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;pre&gt;&lt;code&gt;[1, 2, 3, 1, 2, 3]
[]
[]
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;또한 0.01과 같은 실수를 곱하면 다음과 같은 에러가 발생합니다.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;print&lt;/span&gt;([&lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;,&lt;span style=&#34;color:#ae81ff&#34;&gt;2&lt;/span&gt;,&lt;span style=&#34;color:#ae81ff&#34;&gt;3&lt;/span&gt;] &lt;span style=&#34;color:#f92672&#34;&gt;*&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;0.01&lt;/span&gt;)
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;pre&gt;&lt;code&gt;---------------------------------------------------------------------------

TypeError                                 Traceback (most recent call last)

&amp;lt;ipython-input-4-31e359ec4ae9&amp;gt; in &amp;lt;module&amp;gt;()
----&amp;gt; 1 print([1,2,3] * 0.01)


TypeError: can&#39;t multiply sequence by non-int of type &#39;float&#39;
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;반대로, NumPy array에 실수를 곱하면 어떨까요?&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span style=&#34;color:#f92672&#34;&gt;import&lt;/span&gt; numpy &lt;span style=&#34;color:#f92672&#34;&gt;as&lt;/span&gt; np
&lt;span style=&#34;color:#66d9ef&#34;&gt;print&lt;/span&gt;(np&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;array([&lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;,&lt;span style=&#34;color:#ae81ff&#34;&gt;2&lt;/span&gt;,&lt;span style=&#34;color:#ae81ff&#34;&gt;3&lt;/span&gt;]) &lt;span style=&#34;color:#f92672&#34;&gt;*&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;2&lt;/span&gt;)
&lt;span style=&#34;color:#66d9ef&#34;&gt;print&lt;/span&gt;(np&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;array([&lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;,&lt;span style=&#34;color:#ae81ff&#34;&gt;2&lt;/span&gt;,&lt;span style=&#34;color:#ae81ff&#34;&gt;3&lt;/span&gt;]) &lt;span style=&#34;color:#f92672&#34;&gt;*&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;)
&lt;span style=&#34;color:#66d9ef&#34;&gt;print&lt;/span&gt;(np&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;array([&lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;,&lt;span style=&#34;color:#ae81ff&#34;&gt;2&lt;/span&gt;,&lt;span style=&#34;color:#ae81ff&#34;&gt;3&lt;/span&gt;]) &lt;span style=&#34;color:#f92672&#34;&gt;*&lt;/span&gt; &lt;span style=&#34;color:#f92672&#34;&gt;-&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;)
&lt;span style=&#34;color:#66d9ef&#34;&gt;print&lt;/span&gt;(np&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;array([&lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;,&lt;span style=&#34;color:#ae81ff&#34;&gt;2&lt;/span&gt;,&lt;span style=&#34;color:#ae81ff&#34;&gt;3&lt;/span&gt;]) &lt;span style=&#34;color:#f92672&#34;&gt;*&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;0.01&lt;/span&gt;)
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;pre&gt;&lt;code&gt;[2 4 6]
[0 0 0]
[-1 -2 -3]
[0.01 0.02 0.03]
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;위에서 확인하는 것처럼 모두 출력되는 것을 확인할 수 있습니다. AND 연산의 수가 4배로 많아졌기 때문에 네트워크가 경사하강법으로 수렴하는데에는 더 많은 연산이 필요합니다. &lt;code&gt;error_sum&lt;/code&gt;을 통해서 점점 줄어드는 것을 확인할 수 있습니다.&lt;/p&gt;
&lt;h3 id=&#34;iii-and-네트워크의-평가&#34;&gt;III. AND 네트워크의 평가&lt;/h3&gt;
&lt;p&gt;이렇게 학습시킨 네트워크가 정상적으로 작동하는지 평가해본다. 네트워크에 x의 각 값을 넣었을 때, 실제 출력이 기대출력인 y값에 얼마나 가까운지 다음 코드를 통해 확인한다.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;for&lt;/span&gt; i &lt;span style=&#34;color:#f92672&#34;&gt;in&lt;/span&gt; range(&lt;span style=&#34;color:#ae81ff&#34;&gt;4&lt;/span&gt;): 
  &lt;span style=&#34;color:#66d9ef&#34;&gt;print&lt;/span&gt;(&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;X:&amp;#39;&lt;/span&gt;, x[i], &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;Y:&amp;#39;&lt;/span&gt;, y[i], &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;Output:&amp;#39;&lt;/span&gt;, sigmoid(np&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;sum(x[i]&lt;span style=&#34;color:#f92672&#34;&gt;*&lt;/span&gt;w)&lt;span style=&#34;color:#f92672&#34;&gt;+&lt;/span&gt;b))
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;pre&gt;&lt;code&gt;X: [1 1] Y: [1] Output: 0.9649695824038163
X: [1 0] Y: [0] Output: 0.024824442411186316
X: [0 1] Y: [0] Output: 0.024900221652718595
X: [0 0] Y: [0] Output: 2.359785345402601e-05
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;마지막 &lt;code&gt;Output&lt;/code&gt;이 &lt;code&gt;2.359785345402601e-05&lt;/code&gt;나오는 것은 일종의 과학적 표기법으로 실수를 가수와 지수로 표현하는 방법이다.&lt;/p&gt;
&lt;p&gt;다음을 통해서 확인해본다.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;print&lt;/span&gt;(&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;{:.16f}&amp;#34;&lt;/span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;format(float(&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;2.359785345402601e-05&amp;#34;&lt;/span&gt;)))
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;pre&gt;&lt;code&gt;0.0000235978534540
&lt;/code&gt;&lt;/pre&gt;
&lt;h2 id=&#34;iv-연습-파일&#34;&gt;IV. 연습 파일&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://colab.research.google.com/github/chloevan/deeplearningAI/blob/master/tensorflow2.0/ch3_3_3_Network_AND.ipynb&#34;&gt;구글 Colab에서 직접 연습해보자&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;v-reference&#34;&gt;V. Reference&lt;/h2&gt;
&lt;p&gt;김환희. (2020). 시작하세요! 텐서플로 2.0 프로그래밍: 기초 이론부터 실전 예제까지 한번에 끝내는 머신러닝, 딥러닝 핵심 가이드. 서울: 위키북스.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Tensorflow 2.0 Tutorial ch3.3.2 - 난수 생성 및 시그모이드 함수 편향성</title>
      <link>https://chloevan.github.io/python/tensorflow2.0/ch3_3_2_random_signoid_bias/</link>
      <pubDate>Fri, 10 Apr 2020 10:20:30 +0900</pubDate>
      
      <guid>https://chloevan.github.io/python/tensorflow2.0/ch3_3_2_random_signoid_bias/</guid>
      <description>&lt;h2 id=&#34;공지&#34;&gt;공지&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;본 Tutorial은 교재 &lt;code&gt;시작하세요 텐서플로 2.0 프로그래밍&lt;/code&gt;의 강사에게 국비교육 강의를 듣는 사람들에게 자료 제공을 목적으로 제작하였습니다.&lt;/li&gt;
&lt;li&gt;강사의 주관적인 판단으로 압축해서 자료를 정리하였기 때문에, 자세하게 공부를 하고 싶은 반드시 교재를 구매하실 것을 권해드립니다.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;img src=&#34;https://chloevan.github.io/img/tensorflow2.0/book.jpg&#34; alt=&#34;&#34;&gt;&lt;!-- raw HTML omitted --&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;본 교재 외에 강사가 추가한 내용에 대한 Reference를 확인하셔서, 추가적으로 학습하시는 것을 권유드립니다.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;tutorial&#34;&gt;Tutorial&lt;/h2&gt;
&lt;p&gt;이전 강의가 궁금하신 분들은 아래에서 선택하여 추가 학습 하시기를 바랍니다.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://chloevan.github.io/python/tensorflow2.0/googlecolab/&#34;&gt;Google Colab Tensorflow 2.0 Installation&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://chloevan.github.io/python/tensorflow2.0/ch3_3_1_random_signoid/&#34;&gt;Tensorflow 2.0 Tutorial ch3.3.1 - 난수 생성 및 시그모이드 함수&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://chloevan.github.io/python/tensorflow2.0/ch3_2_2_random_signoid_bias/&#34;&gt;Tensorflow 2.0 Tutorial ch3.2.2 - 난수 생성 및 시그모이드 함수 편향성&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;i-편향성-bias&#34;&gt;I. 편향성 (Bias)&lt;/h2&gt;
&lt;p&gt;지난 시간에 경사하강법 원리를 통해 오차가 적어지는 것을 확인할 수 있었다.&lt;/p&gt;
&lt;p&gt;$$w = w + x \times a \times error$$&lt;/p&gt;
&lt;p&gt;여기에서 입력으로 0을 넣게 되면 출력으로 1을 얻는 뉴런은 어떻게 만들 수 있을까? 지난시간에 배운 내용으로 확인해보자.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span style=&#34;color:#f92672&#34;&gt;import&lt;/span&gt; tensorflow &lt;span style=&#34;color:#f92672&#34;&gt;as&lt;/span&gt; tf
&lt;span style=&#34;color:#66d9ef&#34;&gt;print&lt;/span&gt;(tf&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;__version__)
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;pre&gt;&lt;code&gt;2.2.0-rc2
&lt;/code&gt;&lt;/pre&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span style=&#34;color:#f92672&#34;&gt;import&lt;/span&gt; math
&lt;span style=&#34;color:#66d9ef&#34;&gt;def&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;sigmoid&lt;/span&gt;(x): 
  &lt;span style=&#34;color:#66d9ef&#34;&gt;return&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt; &lt;span style=&#34;color:#f92672&#34;&gt;/&lt;/span&gt; (&lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt; &lt;span style=&#34;color:#f92672&#34;&gt;+&lt;/span&gt; math&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;exp(&lt;span style=&#34;color:#f92672&#34;&gt;-&lt;/span&gt;x))
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;x &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;
y &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;
w &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; tf&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;random&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;normal([&lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;], &lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;)

&lt;span style=&#34;color:#66d9ef&#34;&gt;for&lt;/span&gt; i &lt;span style=&#34;color:#f92672&#34;&gt;in&lt;/span&gt; range(&lt;span style=&#34;color:#ae81ff&#34;&gt;1000&lt;/span&gt;): 
  output &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; sigmoid(x &lt;span style=&#34;color:#f92672&#34;&gt;*&lt;/span&gt; w)
  error &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; y &lt;span style=&#34;color:#f92672&#34;&gt;-&lt;/span&gt; output
  w &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; w &lt;span style=&#34;color:#f92672&#34;&gt;+&lt;/span&gt; x &lt;span style=&#34;color:#f92672&#34;&gt;*&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;0.1&lt;/span&gt; &lt;span style=&#34;color:#f92672&#34;&gt;*&lt;/span&gt; error

  &lt;span style=&#34;color:#66d9ef&#34;&gt;if&lt;/span&gt; i &lt;span style=&#34;color:#f92672&#34;&gt;%&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;100&lt;/span&gt; &lt;span style=&#34;color:#f92672&#34;&gt;==&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;99&lt;/span&gt;:
    &lt;span style=&#34;color:#66d9ef&#34;&gt;print&lt;/span&gt;(i, error, output)
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;pre&gt;&lt;code&gt;99 0.5 0.5
199 0.5 0.5
299 0.5 0.5
399 0.5 0.5
499 0.5 0.5
599 0.5 0.5
699 0.5 0.5
799 0.5 0.5
899 0.5 0.5
999 0.5 0.5
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;경사 하강법의 원리를 적용했지만, &lt;code&gt;error&lt;/code&gt;가 변하지 않았고, 출력도 변하지 않았다. 수식을 기업하면, &lt;code&gt;x = 0&lt;/code&gt; 이므로, &lt;code&gt;w&lt;/code&gt;에 더해지는 값은 없다. &lt;code&gt;1,000&lt;/code&gt;번의 실행 동안 &lt;code&gt;w&lt;/code&gt;값은 변하지 않았다.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;여기에서, 책은 간단히 짚고 넘어갔지만, 왜 딥러닝에서 수학적인 원리가 중요한지 알 수 있다. 일반적으로 개발자 분들은 이러한 부분들을 크게 생각 안하는 경우가 많지만, 기능을 구현하는 것 보다 더 중요한 건 코드를 짤 때, 수학적인 원리도 같이 고민해야 하는 경우가 많다. 머신러닝도 그렇지만, 딥러닝도 다양한 인자들이 존재하고, 이를 이해하려면 공식문서를 잘 참고해야 하며, 더 좋은 성과 및 퍼포먼스를 내려면 결국엔 관련 Thesis, 즉 논문을 읽어낼줄 알아야 한다. 그래야 성과가 나온다. 딥러닝은 1년 단위로 논문이 나오는 걸 잊지 말자!&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;그럼 어떻게 해결해야 할까? 이러한 경우를 방지하기 위해 &lt;code&gt;bias&lt;/code&gt; 라는 개념이 존재한다. 간단히 설명하면 x가 0이 들어오면 대안으로 1이 들어온 것처럼 코드를 작성하는 것이다.&lt;/p&gt;
&lt;p&gt;수식에서는 관용적으로 &lt;code&gt;bias&lt;/code&gt;의 앞 글자인 &lt;code&gt;b&lt;/code&gt;를 쓴다.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;x &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;
y &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;
w &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; tf&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;random&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;normal([&lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;], &lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;)
b &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; tf&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;random&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;normal([&lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;], &lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;)

&lt;span style=&#34;color:#66d9ef&#34;&gt;for&lt;/span&gt; i &lt;span style=&#34;color:#f92672&#34;&gt;in&lt;/span&gt; range(&lt;span style=&#34;color:#ae81ff&#34;&gt;1000&lt;/span&gt;): 
  output &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; sigmoid(x &lt;span style=&#34;color:#f92672&#34;&gt;*&lt;/span&gt; w &lt;span style=&#34;color:#f92672&#34;&gt;+&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt; &lt;span style=&#34;color:#f92672&#34;&gt;*&lt;/span&gt; b)
  error &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; y &lt;span style=&#34;color:#f92672&#34;&gt;-&lt;/span&gt; output
  w &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; w &lt;span style=&#34;color:#f92672&#34;&gt;+&lt;/span&gt; x &lt;span style=&#34;color:#f92672&#34;&gt;*&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;0.1&lt;/span&gt; &lt;span style=&#34;color:#f92672&#34;&gt;*&lt;/span&gt; error
  b &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; b &lt;span style=&#34;color:#f92672&#34;&gt;+&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt; &lt;span style=&#34;color:#f92672&#34;&gt;*&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;0.1&lt;/span&gt; &lt;span style=&#34;color:#f92672&#34;&gt;*&lt;/span&gt; error

  &lt;span style=&#34;color:#66d9ef&#34;&gt;if&lt;/span&gt; i &lt;span style=&#34;color:#f92672&#34;&gt;%&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;100&lt;/span&gt; &lt;span style=&#34;color:#f92672&#34;&gt;==&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;99&lt;/span&gt;:
    &lt;span style=&#34;color:#66d9ef&#34;&gt;print&lt;/span&gt;(i, error, output)
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;pre&gt;&lt;code&gt;99 0.09964988125873331 0.9003501187412667
199 0.051655392657922405 0.9483446073420776
299 0.03453200474689977 0.9654679952531002
399 0.025856559869467777 0.9741434401305322
499 0.020637549126996557 0.9793624508730034
599 0.017159579370033873 0.9828404206299661
699 0.014678743824020679 0.9853212561759793
799 0.01282129876032978 0.9871787012396702
899 0.011379070366788868 0.9886209296332111
999 0.010227240359459322 0.9897727596405407
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;편향을 나타내는 &lt;code&gt;b&lt;/code&gt;가 추가되었고, &lt;code&gt;w&lt;/code&gt;처럼 초기화를 진행한다. 그리고, &lt;code&gt;sigmoid()&lt;/code&gt; 안에 각 입력에 가중치와 편향을 곱해서 더해준 뒤 시그모이드 함수를 취한다. 기대출력과 실제출력의 차이인 &lt;code&gt;error&lt;/code&gt;로 &lt;code&gt;w&lt;/code&gt;와 &lt;code&gt;b&lt;/code&gt;를 각각 업데이트해서 뉴런을 학습시킨다.&lt;/p&gt;
&lt;p&gt;프로그램을 실행한 결과, &lt;code&gt;error&lt;/code&gt;는 0에 가까워지고, &lt;code&gt;output&lt;/code&gt;은 기대출력인 1에 가까워진다.&lt;/p&gt;
&lt;p&gt;이번 시간까지는 간단하게 워밍업을 진행하였고, 다음 시간부터는 순차적으로 신경망 네트워크의 기본 원리에 대해 학습하도록 한다.&lt;/p&gt;
&lt;h2 id=&#34;ii-연습-파일&#34;&gt;II. 연습 파일&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://colab.research.google.com/github/chloevan/deeplearningAI/blob/master/tensorflow2.0/ch3_2_2_random_signoid_bias.ipynb&#34;&gt;구글 Colab에서 직접 연습해보자&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;iii-reference&#34;&gt;III. Reference&lt;/h2&gt;
&lt;p&gt;김환희. (2020). 시작하세요! 텐서플로 2.0 프로그래밍: 기초 이론부터 실전 예제까지 한번에 끝내는 머신러닝, 딥러닝 핵심 가이드. 서울: 위키북스.&lt;/p&gt;
</description>
    </item>
    
  </channel>
</rss>
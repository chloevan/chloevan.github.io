<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Natural Language Generation on Data Science | ChloEvan</title>
    <link>https://chloevan.github.io/tags/natural-language-generation/</link>
    <description>Recent content in Natural Language Generation on Data Science | ChloEvan</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Mon, 27 Apr 2020 14:20:30 +0900</lastBuildDate>
    
        <atom:link href="https://chloevan.github.io/tags/natural-language-generation/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>Tensorflow 2.0 Tutorial ch7.4 - (2) 단어 단위 생성</title>
      <link>https://chloevan.github.io/python/tensorflow2.0/ch7_4_naturallanguagegeneration2/</link>
      <pubDate>Mon, 27 Apr 2020 14:20:30 +0900</pubDate>
      
      <guid>https://chloevan.github.io/python/tensorflow2.0/ch7_4_naturallanguagegeneration2/</guid>
      <description>&lt;h2 id=&#34;공지&#34;&gt;공지&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;본 Tutorial은 교재 &lt;code&gt;시작하세요 텐서플로 2.0 프로그래밍&lt;/code&gt;의 강사에게 국비교육 강의를 듣는 사람들에게 자료 제공을 목적으로 제작하였습니다.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;강사의 주관적인 판단으로 압축해서 자료를 정리하였기 때문에, 자세하게 공부를 하고 싶으신 분은 반드시 교재를 구매하실 것을 권해드립니다.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;img src=&#34;https://chloevan.github.io/img/tensorflow2.0/book.jpg&#34; alt=&#34;&#34;&gt;&lt;!-- raw HTML omitted --&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;본 교재 외에 강사가 추가한 내용에 대한 Reference를 확인하셔서, 추가적으로 학습하시는 것을 권유드립니다.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;tutorial&#34;&gt;Tutorial&lt;/h2&gt;
&lt;p&gt;이전 강의가 궁금하신 분들은 아래에서 선택하여 추가 학습 하시기를 바랍니다.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://chloevan.github.io/python/tensorflow2.0/googlecolab/&#34;&gt;Google Colab Tensorflow 2.0 Installation&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://chloevan.github.io/python/tensorflow2.0/ch3_3_1_random_signoid/&#34;&gt;Tensorflow 2.0 Tutorial ch3.3.1 - 난수 생성 및 시그모이드 함수&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://chloevan.github.io/python/tensorflow2.0/ch3_3_2_random_signoid_bias/&#34;&gt;Tensorflow 2.0 Tutorial ch3.3.2 - 난수 생성 및 시그모이드 함수 편향성&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://chloevan.github.io/python/tensorflow2.0/ch3_3_3_network_and/&#34;&gt;Tensorflow 2.0 Tutorial ch3.3.3 - 첫번째 신경망 네트워크 - AND&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://chloevan.github.io/python/tensorflow2.0/ch3_3_4_network_or/&#34;&gt;Tensorflow 2.0 Tutorial ch3.3.4 - 두번째 신경망 네트워크 - OR&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://chloevan.github.io/python/tensorflow2.0/ch3_3_5_network_xor/&#34;&gt;Tensorflow 2.0 Tutorial ch3.3.5 - 세번째 신경망 네트워크 - XOR&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://chloevan.github.io/python/tensorflow2.0/ch4_1_linear_regression/&#34;&gt;Tensorflow 2.0 Tutorial ch4.1 - 선형회귀&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://chloevan.github.io/python/tensorflow2.0/ch4_2_multiple_linear_regression/&#34;&gt;Tensorflow 2.0 Tutorial ch4.2 - 다항회귀&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://chloevan.github.io/python/tensorflow2.0/ch4_3_regression_with_deeplearning/&#34;&gt;Tensorflow 2.0 Tutorial ch4.3 - 딥러닝 네트워크를 이용한 회귀&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://chloevan.github.io/python/tensorflow2.0/ch4_4_boston_housing_deeplearning/&#34;&gt;Tensorflow 2.0 Tutorial ch4.4 - 보스턴 주택 가격 데이터세트&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://chloevan.github.io/python/tensorflow2.0/ch5_1_binary_classification/&#34;&gt;Tensorflow 2.0 Tutorial ch5.1 - 분류&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://chloevan.github.io/python/tensorflow2.0/ch5_2_multi_classification/&#34;&gt;Tensorflow 2.0 Tutorial ch5.2 - 다항분류&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://chloevan.github.io/python/tensorflow2.0/ch5_3_fashion_mnist/&#34;&gt;Tensorflow 2.0 Tutorial ch5.3 - Fashion MNIST&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://chloevan.github.io/python/tensorflow2.0/ch6_1_2_cnn_theory/&#34;&gt;Tensorflow 2.0 Tutorial ch6.1-2 - CNN 이론&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://chloevan.github.io/python/tensorflow2.0/ch6_3_fashion_mnist_with_cnn/&#34;&gt;Tensorflow 2.0 Tutorial ch6.3 - Fashion MNIST with CNN 실습&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://chloevan.github.io/python/tensorflow2.0/ch6_4_improve_performance/&#34;&gt;Tensorflow 2.0 Tutorial ch6.4 - 모형의 성능 높이기&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://chloevan.github.io/python/tensorflow2.0/ch7_1_2_rnn_theory1/&#34;&gt;Tensorflow 2.0 Tutorial ch7.1 - RNN 이론 (1)&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://chloevan.github.io/python/tensorflow2.0/ch7_1_2_rnn_theory2/&#34;&gt;Tensorflow 2.0 Tutorial ch7.1 - RNN 이론 (2)&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://chloevan.github.io/python/tensorflow2.0/ch7_3_sentimentanalysis/&#34;&gt;Tensorflow 2.0 Tutorial ch7.3 - 긍정, 부정 감성 분석&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://chloevan.github.io/python/tensorflow2.0/ch7_4_naturallanguagegeneration1/&#34;&gt;Tensorflow 2.0 Tutorial ch7.4 - (1) 단어 단위 생성&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;i-개요&#34;&gt;I. 개요&lt;/h2&gt;
&lt;p&gt;테슬라 &lt;code&gt;AI Director&lt;/code&gt;인 안드레아 카르파티(&lt;code&gt;Andrej Karpathy&lt;/code&gt;)는 &lt;code&gt;The Unreasonable Effectiveness of Recurrent Neural Networks&lt;/code&gt;라는 글을 개인 블로그에 작성했는데, 짧게 요약하면 문자 단위의 순환 신경망이 셰익스피어의 희곡, 소스코드, &lt;code&gt;Latex&lt;/code&gt;등을 재생산하는데 순환 신경망이 효과적이라는 것을 보여줍니다.&lt;/p&gt;
&lt;p&gt;이를 바탕으로, 한글 원본 텍스트를 자소 단위와 단어 단위로 나눠서 순환 신경망으로 생성해보도록 합니다.&lt;/p&gt;
&lt;h2 id=&#34;ii-자소-단위-생성&#34;&gt;II. 자소 단위 생성&lt;/h2&gt;
&lt;p&gt;자소 단위 생성을 하기 위해서는 한글을 자소 단위로 분리하고 다시 합칠 수 있는 라이브러리가 필요합니다. 이러한 작업을 할 수 있도록 신해빈 님이 만든 &lt;a href=&#34;https://github.com/HaebinShin/jamotools&#34;&gt;jamotools&lt;/a&gt;가 있습니다.&lt;/p&gt;
&lt;p&gt;구글 코랩에서 배시 셀 명령어를 이용해 라이브러리를 설치합니다.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span style=&#34;color:#960050;background-color:#1e0010&#34;&gt;!&lt;/span&gt;pip install jamotools
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;pre&gt;&lt;code&gt;Collecting jamotools
  Downloading https://files.pythonhosted.org/packages/3d/d6/ec13c68f7ea6a8085966390d256d183bf8488f8b9770028359acb86df643/jamotools-0.1.10-py2.py3-none-any.whl
Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from jamotools) (1.18.3)
Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from jamotools) (1.12.0)
Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from jamotools) (0.16.0)
Installing collected packages: jamotools
Successfully installed jamotools-0.1.10
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;jamotools의 기능을 테스트하기 위해 조선왕조실록 텍스트를 다시 사용합니다.&lt;/p&gt;
&lt;h3 id=&#34;1-데이터-로드&#34;&gt;(1) 데이터 로드&lt;/h3&gt;
&lt;p&gt;데이터 파일은 약 62MB 정도입니다.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span style=&#34;color:#75715e&#34;&gt;# 텐서플로 2 버전 선택&lt;/span&gt;
&lt;span style=&#34;color:#66d9ef&#34;&gt;try&lt;/span&gt;:
    &lt;span style=&#34;color:#75715e&#34;&gt;# %tensorflow_version only exists in Colab.&lt;/span&gt;
    &lt;span style=&#34;color:#f92672&#34;&gt;%&lt;/span&gt;tensorflow_version &lt;span style=&#34;color:#ae81ff&#34;&gt;2.&lt;/span&gt;x
&lt;span style=&#34;color:#66d9ef&#34;&gt;except&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;Exception&lt;/span&gt;:
    &lt;span style=&#34;color:#66d9ef&#34;&gt;pass&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;import&lt;/span&gt; tensorflow &lt;span style=&#34;color:#f92672&#34;&gt;as&lt;/span&gt; tf
&lt;span style=&#34;color:#f92672&#34;&gt;import&lt;/span&gt; numpy &lt;span style=&#34;color:#f92672&#34;&gt;as&lt;/span&gt; np
&lt;span style=&#34;color:#f92672&#34;&gt;import&lt;/span&gt; pandas &lt;span style=&#34;color:#f92672&#34;&gt;as&lt;/span&gt; pd
&lt;span style=&#34;color:#f92672&#34;&gt;import&lt;/span&gt; jamotools
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;path_to_train_file &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; tf&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;keras&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;utils&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;get_file(&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;input.txt&amp;#39;&lt;/span&gt;, &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;https://raw.githubusercontent.com/greentec/greentec.github.io/master/public/other/data/chosundynasty/corpus.txt&amp;#39;&lt;/span&gt;)
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;pre&gt;&lt;code&gt;Downloading data from https://raw.githubusercontent.com/greentec/greentec.github.io/master/public/other/data/chosundynasty/corpus.txt
62013440/62012502 [==============================] - 1s 0us/step
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;데이터를 메모리에 불러옵니다. 인코딩 형식으로 &lt;code&gt;utf-8&lt;/code&gt;을 지정한 뒤 텍스트가 총 몇 자인지 확인해봅니다.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;train_text &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; open(path_to_train_file, &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;rb&amp;#39;&lt;/span&gt;)&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;read()&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;decode(encoding&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;utf-8&amp;#39;&lt;/span&gt;)
s &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; train_text[:&lt;span style=&#34;color:#ae81ff&#34;&gt;100&lt;/span&gt;]
&lt;span style=&#34;color:#66d9ef&#34;&gt;print&lt;/span&gt;(s)

&lt;span style=&#34;color:#75715e&#34;&gt;# 한글 텍스트를 자모 단위로 분리합니다. 한자 등에는 영향이 없습니다. &lt;/span&gt;
s_split &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; jamotools&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;split_syllables(s)
&lt;span style=&#34;color:#66d9ef&#34;&gt;print&lt;/span&gt;(s_split)
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;pre&gt;&lt;code&gt;﻿태조 이성계 선대의 가계. 목조 이안사가 전주에서 삼척·의주를 거쳐 알동에 정착하다 
태조 강헌 지인 계운 성문 신무 대왕(太祖康獻至仁啓運聖文神武大王)의 성은 이씨(李氏)요, 휘
﻿ㅌㅐㅈㅗ ㅇㅣㅅㅓㅇㄱㅖ ㅅㅓㄴㄷㅐㅇㅢ ㄱㅏㄱㅖ. ㅁㅗㄱㅈㅗ ㅇㅣㅇㅏㄴㅅㅏㄱㅏ ㅈㅓㄴㅈㅜㅇㅔㅅㅓ ㅅㅏㅁㅊㅓㄱ·ㅇㅢㅈㅜㄹㅡㄹ ㄱㅓㅊㅕ ㅇㅏㄹㄷㅗㅇㅇㅔ ㅈㅓㅇㅊㅏㄱㅎㅏㄷㅏ 
ㅌㅐㅈㅗ ㄱㅏㅇㅎㅓㄴ ㅈㅣㅇㅣㄴ ㄱㅖㅇㅜㄴ ㅅㅓㅇㅁㅜㄴ ㅅㅣㄴㅁㅜ ㄷㅐㅇㅘㅇ(太祖康獻至仁啓運聖文神武大王)ㅇㅢ ㅅㅓㅇㅇㅡㄴ ㅇㅣㅆㅣ(李氏)ㅇㅛ, ㅎㅟ
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;code&gt;split_syllables()&lt;/code&gt;함수를 이용해서 100글자의 한글이 자모 단위로 정상적으로 분리되는 것을 확인할 수 있습니다. &lt;code&gt;jamotools&lt;/code&gt;는 영문이나 한자 등에는 영향을 주지 않습니다. 분리한 자모를 다시 합칠 수도 있습니다.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;s2 &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; jamotools&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;join_jamos(s_split)
&lt;span style=&#34;color:#66d9ef&#34;&gt;print&lt;/span&gt;(s2)
&lt;span style=&#34;color:#66d9ef&#34;&gt;print&lt;/span&gt;(s &lt;span style=&#34;color:#f92672&#34;&gt;==&lt;/span&gt; s2)
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;pre&gt;&lt;code&gt;﻿태조 이성계 선대의 가계. 목조 이안사가 전주에서 삼척·의주를 거쳐 알동에 정착하다 
태조 강헌 지인 계운 성문 신무 대왕(太祖康獻至仁啓運聖文神武大王)의 성은 이씨(李氏)요, 휘
True
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;자모를 분리했다가 다시 합친 &lt;code&gt;s2&lt;/code&gt;는 &lt;code&gt;s&lt;/code&gt;와 같다는 것을 두 번째 출력이 &lt;code&gt;True&lt;/code&gt;인 것에서 확인할 수 있습니다.&lt;/p&gt;
&lt;h3 id=&#34;2-자모-토큰화&#34;&gt;(2) 자모 토큰화&lt;/h3&gt;
&lt;p&gt;그럼 이제 자모를 토큰화합니다. 여기서는 따로 텍스트 전처리를 하지 않습니다. 괄호, 한자 등이 토큰에 모두 포함될 것입니다.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;텍스트를 자모 단위로 나눕니다. 데이터가 크기 때문에 약간 시간이 걸립니다.&lt;/li&gt;
&lt;/ul&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;train_text_X &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; jamotools&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;split_syllables(train_text)
vocab &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; sorted(set(train_text_X))
vocab&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;append(&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;UNK&amp;#39;&lt;/span&gt;)

&lt;span style=&#34;color:#66d9ef&#34;&gt;print&lt;/span&gt;(&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;{} unique characters&amp;#39;&lt;/span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;format(len(vocab)))
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;pre&gt;&lt;code&gt;6198 unique characters
&lt;/code&gt;&lt;/pre&gt;
&lt;ul&gt;
&lt;li&gt;이제, vocal list를 숫자로 매핑하고, 반대도 실행합니다.&lt;/li&gt;
&lt;/ul&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;char2idx &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; {u:i &lt;span style=&#34;color:#66d9ef&#34;&gt;for&lt;/span&gt; i, u &lt;span style=&#34;color:#f92672&#34;&gt;in&lt;/span&gt; enumerate(vocab)}
idx2char &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; np&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;array(vocab)

test_as_int &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; np&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;array([char2idx[c] &lt;span style=&#34;color:#66d9ef&#34;&gt;for&lt;/span&gt; c &lt;span style=&#34;color:#f92672&#34;&gt;in&lt;/span&gt; train_text_X])
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;ul&gt;
&lt;li&gt;이제 word2idx의 일부를 알아보기 쉽게 출력합니다.&lt;/li&gt;
&lt;/ul&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;print&lt;/span&gt;(&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;{&amp;#39;&lt;/span&gt;)
&lt;span style=&#34;color:#66d9ef&#34;&gt;for&lt;/span&gt; char, _ &lt;span style=&#34;color:#f92672&#34;&gt;in&lt;/span&gt; zip(char2idx, range(&lt;span style=&#34;color:#ae81ff&#34;&gt;10&lt;/span&gt;)):
  &lt;span style=&#34;color:#66d9ef&#34;&gt;print&lt;/span&gt;(&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39; {:4s}: {:3d}, &amp;#39;&lt;/span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;format(repr(char), char2idx[char]))
&lt;span style=&#34;color:#66d9ef&#34;&gt;print&lt;/span&gt;(&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;   ...&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;\n&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;}&amp;#39;&lt;/span&gt;)

&lt;span style=&#34;color:#66d9ef&#34;&gt;print&lt;/span&gt;(&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;index of UNK: {}&amp;#39;&lt;/span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;format(char2idx[&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;UNK&amp;#39;&lt;/span&gt;]))
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;pre&gt;&lt;code&gt;{
 &#39;\n&#39;:   0, 
 &#39; &#39; :   1, 
 &#39;!&#39; :   2, 
 &#39;&amp;quot;&#39; :   3, 
 &amp;quot;&#39;&amp;quot; :   4, 
 &#39;(&#39; :   5, 
 &#39;)&#39; :   6, 
 &#39;+&#39; :   7, 
 &#39;,&#39; :   8, 
 &#39;-&#39; :   9, 
   ...
}
index of UNK: 6197
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;자모를 토큰화하고 혹시 사전에 정의되지 않은 기호를 만날수도 있으므로 &lt;code&gt;UNK&lt;/code&gt;도 사전에 추가합니다. 이렇게 중복되 않은 자모는 총 &lt;code&gt;6197&lt;/code&gt;개가 나옵니다. 단어에 비해 매우 적은 숫자임을 알 수 있습니다.&lt;/p&gt;
&lt;p&gt;이제 토큰 데이터를 출력합니다.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;print&lt;/span&gt;(train_text_X[:&lt;span style=&#34;color:#ae81ff&#34;&gt;20&lt;/span&gt;])
&lt;span style=&#34;color:#66d9ef&#34;&gt;print&lt;/span&gt;(test_as_int[:&lt;span style=&#34;color:#ae81ff&#34;&gt;20&lt;/span&gt;])
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;pre&gt;&lt;code&gt;﻿ㅌㅐㅈㅗ ㅇㅣㅅㅓㅇㄱㅖ ㅅㅓㄴㄷㅐㅇ
[6158   83   87   79   94    1   78  106   76   90   78   56   93    1
   76   90   59   62   87   78]
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;code&gt;ㅌ&lt;/code&gt;은 6158, &lt;code&gt;ㅐ&lt;/code&gt;는 83 등으로 토큰화가 된 것을 확인할 수 있습니다.&lt;/p&gt;
&lt;h3 id=&#34;3-데이터-생성&#34;&gt;(3) 데이터 생성&lt;/h3&gt;
&lt;p&gt;단어 생성 단위에 있던 학습 데이터세트를 생성합니다. 이 부분의 코드는 큰 변경사항이 없습니다.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;sentence_dataet&lt;/code&gt;에서 &lt;code&gt;char_dataset&lt;/code&gt;으로 변동하시면 됩니다.&lt;/li&gt;
&lt;li&gt;&lt;code&gt;idx2word&lt;/code&gt;에서 &lt;code&gt;idx2char&lt;/code&gt;으로 변동하시면 됩니다.&lt;/li&gt;
&lt;/ul&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;seq_length &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;80&lt;/span&gt;
examples_per_epoch &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; len(test_as_int) &lt;span style=&#34;color:#f92672&#34;&gt;//&lt;/span&gt; seq_length
char_dataset &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; tf&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;data&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;Dataset&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;from_tensor_slices(test_as_int)

char_dataset &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; char_dataset&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;batch(seq_length&lt;span style=&#34;color:#f92672&#34;&gt;+&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;, drop_remainder&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;True)
&lt;span style=&#34;color:#66d9ef&#34;&gt;for&lt;/span&gt; item &lt;span style=&#34;color:#f92672&#34;&gt;in&lt;/span&gt; char_dataset&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;take(&lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;):
    &lt;span style=&#34;color:#66d9ef&#34;&gt;print&lt;/span&gt;(idx2char[item&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;numpy()])
    &lt;span style=&#34;color:#66d9ef&#34;&gt;print&lt;/span&gt;(item&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;numpy())

&lt;span style=&#34;color:#66d9ef&#34;&gt;def&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;split_input_target&lt;/span&gt;(chunk):
    &lt;span style=&#34;color:#66d9ef&#34;&gt;return&lt;/span&gt; [chunk[:&lt;span style=&#34;color:#f92672&#34;&gt;-&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;], chunk[&lt;span style=&#34;color:#f92672&#34;&gt;-&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;]]

train_dataset &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; char_dataset&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;map(split_input_target)
&lt;span style=&#34;color:#66d9ef&#34;&gt;for&lt;/span&gt; x,y &lt;span style=&#34;color:#f92672&#34;&gt;in&lt;/span&gt; train_dataset&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;take(&lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;):
    &lt;span style=&#34;color:#66d9ef&#34;&gt;print&lt;/span&gt;(idx2char[x&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;numpy()])
    &lt;span style=&#34;color:#66d9ef&#34;&gt;print&lt;/span&gt;(x&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;numpy())
    &lt;span style=&#34;color:#66d9ef&#34;&gt;print&lt;/span&gt;(idx2char[y&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;numpy()])
    &lt;span style=&#34;color:#66d9ef&#34;&gt;print&lt;/span&gt;(y&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;numpy())

BATCH_SIZE &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;256&lt;/span&gt;
steps_per_epoch &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; examples_per_epoch &lt;span style=&#34;color:#f92672&#34;&gt;//&lt;/span&gt; BATCH_SIZE
BUFFER_SIZE &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;10000&lt;/span&gt;

train_dataset &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; train_dataset&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;shuffle(BUFFER_SIZE)&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;batch(BATCH_SIZE, drop_remainder&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;True)
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;pre&gt;&lt;code&gt;[&#39;\ufeff&#39; &#39;ㅌ&#39; &#39;ㅐ&#39; &#39;ㅈ&#39; &#39;ㅗ&#39; &#39; &#39; &#39;ㅇ&#39; &#39;ㅣ&#39; &#39;ㅅ&#39; &#39;ㅓ&#39; &#39;ㅇ&#39; &#39;ㄱ&#39; &#39;ㅖ&#39; &#39; &#39; &#39;ㅅ&#39; &#39;ㅓ&#39; &#39;ㄴ&#39;
 &#39;ㄷ&#39; &#39;ㅐ&#39; &#39;ㅇ&#39; &#39;ㅢ&#39; &#39; &#39; &#39;ㄱ&#39; &#39;ㅏ&#39; &#39;ㄱ&#39; &#39;ㅖ&#39; &#39;.&#39; &#39; &#39; &#39;ㅁ&#39; &#39;ㅗ&#39; &#39;ㄱ&#39; &#39;ㅈ&#39; &#39;ㅗ&#39; &#39; &#39; &#39;ㅇ&#39;
 &#39;ㅣ&#39; &#39;ㅇ&#39; &#39;ㅏ&#39; &#39;ㄴ&#39; &#39;ㅅ&#39; &#39;ㅏ&#39; &#39;ㄱ&#39; &#39;ㅏ&#39; &#39; &#39; &#39;ㅈ&#39; &#39;ㅓ&#39; &#39;ㄴ&#39; &#39;ㅈ&#39; &#39;ㅜ&#39; &#39;ㅇ&#39; &#39;ㅔ&#39; &#39;ㅅ&#39; &#39;ㅓ&#39;
 &#39; &#39; &#39;ㅅ&#39; &#39;ㅏ&#39; &#39;ㅁ&#39; &#39;ㅊ&#39; &#39;ㅓ&#39; &#39;ㄱ&#39; &#39;·&#39; &#39;ㅇ&#39; &#39;ㅢ&#39; &#39;ㅈ&#39; &#39;ㅜ&#39; &#39;ㄹ&#39; &#39;ㅡ&#39; &#39;ㄹ&#39; &#39; &#39; &#39;ㄱ&#39; &#39;ㅓ&#39;
 &#39;ㅊ&#39; &#39;ㅕ&#39; &#39; &#39; &#39;ㅇ&#39; &#39;ㅏ&#39; &#39;ㄹ&#39; &#39;ㄷ&#39; &#39;ㅗ&#39; &#39;ㅇ&#39; &#39;ㅇ&#39;]
[6158   83   87   79   94    1   78  106   76   90   78   56   93    1
   76   90   59   62   87   78  105    1   56   86   56   93   10    1
   72   94   56   79   94    1   78  106   78   86   59   76   86   56
   86    1   79   90   59   79   99   78   91   76   90    1   76   86
   72   81   90   56   36   78  105   79   99   64  104   64    1   56
   90   81   92    1   78   86   64   62   94   78   78]
[&#39;\ufeff&#39; &#39;ㅌ&#39; &#39;ㅐ&#39; &#39;ㅈ&#39; &#39;ㅗ&#39; &#39; &#39; &#39;ㅇ&#39; &#39;ㅣ&#39; &#39;ㅅ&#39; &#39;ㅓ&#39; &#39;ㅇ&#39; &#39;ㄱ&#39; &#39;ㅖ&#39; &#39; &#39; &#39;ㅅ&#39; &#39;ㅓ&#39; &#39;ㄴ&#39;
 &#39;ㄷ&#39; &#39;ㅐ&#39; &#39;ㅇ&#39; &#39;ㅢ&#39; &#39; &#39; &#39;ㄱ&#39; &#39;ㅏ&#39; &#39;ㄱ&#39; &#39;ㅖ&#39; &#39;.&#39; &#39; &#39; &#39;ㅁ&#39; &#39;ㅗ&#39; &#39;ㄱ&#39; &#39;ㅈ&#39; &#39;ㅗ&#39; &#39; &#39; &#39;ㅇ&#39;
 &#39;ㅣ&#39; &#39;ㅇ&#39; &#39;ㅏ&#39; &#39;ㄴ&#39; &#39;ㅅ&#39; &#39;ㅏ&#39; &#39;ㄱ&#39; &#39;ㅏ&#39; &#39; &#39; &#39;ㅈ&#39; &#39;ㅓ&#39; &#39;ㄴ&#39; &#39;ㅈ&#39; &#39;ㅜ&#39; &#39;ㅇ&#39; &#39;ㅔ&#39; &#39;ㅅ&#39; &#39;ㅓ&#39;
 &#39; &#39; &#39;ㅅ&#39; &#39;ㅏ&#39; &#39;ㅁ&#39; &#39;ㅊ&#39; &#39;ㅓ&#39; &#39;ㄱ&#39; &#39;·&#39; &#39;ㅇ&#39; &#39;ㅢ&#39; &#39;ㅈ&#39; &#39;ㅜ&#39; &#39;ㄹ&#39; &#39;ㅡ&#39; &#39;ㄹ&#39; &#39; &#39; &#39;ㄱ&#39; &#39;ㅓ&#39;
 &#39;ㅊ&#39; &#39;ㅕ&#39; &#39; &#39; &#39;ㅇ&#39; &#39;ㅏ&#39; &#39;ㄹ&#39; &#39;ㄷ&#39; &#39;ㅗ&#39; &#39;ㅇ&#39;]
[6158   83   87   79   94    1   78  106   76   90   78   56   93    1
   76   90   59   62   87   78  105    1   56   86   56   93   10    1
   72   94   56   79   94    1   78  106   78   86   59   76   86   56
   86    1   79   90   59   79   99   78   91   76   90    1   76   86
   72   81   90   56   36   78  105   79   99   64  104   64    1   56
   90   81   92    1   78   86   64   62   94   78]
ㅇ
78
&lt;/code&gt;&lt;/pre&gt;
&lt;ul&gt;
&lt;li&gt;seq_length의 의미는 자소 단위에서는 80개의 자소를 입력받았을 때 1개의 자소를 출력하도록 합니다.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;4-자소-단위-생성-모델-정의-및-학습&#34;&gt;(4) 자소 단위 생성 모델 정의 및 학습&lt;/h3&gt;
&lt;p&gt;자소 단위 생성 모델에서는 겹쳐진 순환 신경망을 사용하지 않고 &lt;code&gt;LSTM&lt;/code&gt;레이어를 하나만 사용했습니다. 대신 하나의 &lt;code&gt;LSTM&lt;/code&gt;레이어에서는 사용하는 뉴런의 수를 4배로 늘렸습니다. 또 단어의 수보다 자소의 수가 훨씬 적기 때문에 마지막 &lt;code&gt;Dense&lt;/code&gt;레이어의 뉴런 수가 적어집니다.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;total_chars &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; len(vocab)
model &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; tf&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;keras&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;Sequential([
  tf&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;keras&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;layers&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;Embedding(total_chars, &lt;span style=&#34;color:#ae81ff&#34;&gt;100&lt;/span&gt;, input_length&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;seq_length), 
  tf&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;keras&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;layers&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;LSTM(units&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;400&lt;/span&gt;), 
  tf&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;keras&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;layers&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;Dense(total_chars, activation&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;softmax&amp;#39;&lt;/span&gt;)
])

model&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;compile(optimizer&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;adam&amp;#39;&lt;/span&gt;, loss&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;sparse_categorical_crossentropy&amp;#39;&lt;/span&gt;, metrics&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;[&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;accuracy&amp;#39;&lt;/span&gt;])
model&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;summary()
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;pre&gt;&lt;code&gt;Model: &amp;quot;sequential&amp;quot;
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
embedding (Embedding)        (None, 80, 100)           619800    
_________________________________________________________________
lstm (LSTM)                  (None, 400)               801600    
_________________________________________________________________
dense (Dense)                (None, 6198)              2485398   
=================================================================
Total params: 3,906,798
Trainable params: 3,906,798
Non-trainable params: 0
_________________________________________________________________
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;모형을 학습시킵니다. 이 때 주의해야 하는 것은 &lt;code&gt;testmodel&lt;/code&gt; 함수에서, &lt;code&gt;jamotools&lt;/code&gt; 입력 부분을 추가해야 합니다. 이 소스코드의 위치를 살펴보시기를 바랍니다.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span style=&#34;color:#f92672&#34;&gt;from&lt;/span&gt; tensorflow.keras.preprocessing.sequence &lt;span style=&#34;color:#f92672&#34;&gt;import&lt;/span&gt; pad_sequences

&lt;span style=&#34;color:#66d9ef&#34;&gt;def&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;testmodel&lt;/span&gt;(epoch, logs):
    &lt;span style=&#34;color:#66d9ef&#34;&gt;if&lt;/span&gt; epoch &lt;span style=&#34;color:#f92672&#34;&gt;%&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;5&lt;/span&gt; &lt;span style=&#34;color:#f92672&#34;&gt;!=&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt; &lt;span style=&#34;color:#f92672&#34;&gt;and&lt;/span&gt; epoch &lt;span style=&#34;color:#f92672&#34;&gt;!=&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;49&lt;/span&gt;:
        &lt;span style=&#34;color:#66d9ef&#34;&gt;return&lt;/span&gt;
    test_sentence &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; train_text[&lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;]
    test_sentence &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; jamotools&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;split_syllables(test_sentence)

    next_chars &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;300&lt;/span&gt;
    &lt;span style=&#34;color:#66d9ef&#34;&gt;for&lt;/span&gt; _ &lt;span style=&#34;color:#f92672&#34;&gt;in&lt;/span&gt; range(next_chars):
        test_text_X &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; test_sentence&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;split(&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39; &amp;#39;&lt;/span&gt;)[&lt;span style=&#34;color:#f92672&#34;&gt;-&lt;/span&gt;seq_length:]
        test_text_X &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; np&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;array([char2idx[c] &lt;span style=&#34;color:#66d9ef&#34;&gt;if&lt;/span&gt; c &lt;span style=&#34;color:#f92672&#34;&gt;in&lt;/span&gt; char2idx &lt;span style=&#34;color:#66d9ef&#34;&gt;else&lt;/span&gt; char2idx[&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;UNK&amp;#39;&lt;/span&gt;] &lt;span style=&#34;color:#66d9ef&#34;&gt;for&lt;/span&gt; c &lt;span style=&#34;color:#f92672&#34;&gt;in&lt;/span&gt; test_text_X])
        test_text_X &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; pad_sequences([test_text_X], maxlen&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;seq_length, padding&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;pre&amp;#39;&lt;/span&gt;, value&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;char2idx[&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;UNK&amp;#39;&lt;/span&gt;])

        output_idx &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; model&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;predict_classes(test_text_X)
        test_sentence &lt;span style=&#34;color:#f92672&#34;&gt;+=&lt;/span&gt; &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39; &amp;#39;&lt;/span&gt; &lt;span style=&#34;color:#f92672&#34;&gt;+&lt;/span&gt; idx2char[output_idx[&lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;]]
    
    &lt;span style=&#34;color:#66d9ef&#34;&gt;print&lt;/span&gt;()
    &lt;span style=&#34;color:#66d9ef&#34;&gt;print&lt;/span&gt;(test_sentence)
    &lt;span style=&#34;color:#66d9ef&#34;&gt;print&lt;/span&gt;()

testmodelcb &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; tf&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;keras&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;callbacks&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;LambdaCallback(on_epoch_end&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;testmodel)
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;history &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; model&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;fit(train_dataset&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;repeat(), epochs&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;100&lt;/span&gt;, steps_per_epoch&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;steps_per_epoch, callbacks&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;[testmodelcb], verbose&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;2&lt;/span&gt;)
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;Epoch 1/100&lt;/p&gt;
&lt;p&gt;﻿태조 이성계 선대의 가계. 목조 이안사가 전주에서 삼척·의주를 거쳐 알동에 정착하다 이를 것을 것을 것을 것을 것을 것을 것을 것을 것을 것을 것을 것을 것을 것을 것을 것을 것을 것을 것을 것을 것을 것을 것을 것을 것을 것을 것을 것을 것을 것을 것을 것을 것을 것을 것을 것을 것을 것을 것을 것을 것을 것을&lt;/p&gt;
&lt;p&gt;2364/2364 - 270s - loss: 2.5904 - accuracy: 0.3065
Epoch 2/100
2364/2364 - 266s - loss: 1.9905 - accuracy: 0.4264
Epoch 3/100
2364/2364 - 266s - loss: 1.8423 - accuracy: 0.4608
Epoch 4/100
2364/2364 - 266s - loss: 1.7423 - accuracy: 0.4823
Epoch 5/100
2364/2364 - 264s - loss: 1.6784 - accuracy: 0.4948
Epoch 6/100&lt;/p&gt;
&lt;p&gt;﻿태조 이성계 선대의 가계. 목조 이안사가 전주에서 삼척·의주를 거쳐 알동에 정착하다 일이 있었다. 임금이 있었다. 임금이 있었다. 임금이 있었다. 임금이 있었다. 임금이 있었다. 임금이 있었다. 임금이 있었다. 임금이 있었다. 임금이 있었다. 임금이 있었다. 임금이 있었다. 임금이 있었다. 임금이 있었다. 임금이 있었다. 임금이 있었다.&lt;/p&gt;
&lt;p&gt;2364/2364 - 265s - loss: 1.6241 - accuracy: 0.5070
Epoch 7/100
2364/2364 - 264s - loss: 1.5739 - accuracy: 0.5184
Epoch 8/100
2364/2364 - 264s - loss: 1.5297 - accuracy: 0.5286
Epoch 9/100
2364/2364 - 266s - loss: 1.4916 - accuracy: 0.5372
Epoch 10/100
2364/2364 - 268s - loss: 1.4595 - accuracy: 0.5451
Epoch 11/100&lt;/p&gt;
&lt;p&gt;﻿태조 이성계 선대의 가계. 목조 이안사가 전주에서 삼척·의주를 거쳐 알동에 정착하다 하였다. 임금이 말하기를,
&amp;ldquo;임금이 말하기를,
&amp;ldquo;임금이 말하기를,
&amp;ldquo;임금이 말하기를,
&amp;ldquo;임금이 말하기를,
&amp;ldquo;임금이 말하기를,
&amp;ldquo;임금이 말하기를,
&amp;ldquo;임금이 말하기를,
&amp;ldquo;임금이 말하기를,
&amp;ldquo;임금이 말하기를,
&amp;ldquo;임금이 말하기를,
&amp;ldquo;임금이 말하기를,
&amp;ldquo;임금이 말하기를,
&amp;ldquo;임그&lt;/p&gt;
&lt;p&gt;2364/2364 - 270s - loss: 1.4323 - accuracy: 0.5516
Epoch 12/100
2364/2364 - 269s - loss: 1.4059 - accuracy: 0.5585
Epoch 13/100
2364/2364 - 269s - loss: 1.3848 - accuracy: 0.5631
Epoch 14/100
2364/2364 - 269s - loss: 1.3646 - accuracy: 0.5685
Epoch 15/100
2364/2364 - 270s - loss: 1.3473 - accuracy: 0.5730
Epoch 16/100&lt;/p&gt;
&lt;p&gt;﻿태조 이성계 선대의 가계. 목조 이안사가 전주에서 삼척·의주를 거쳐 알동에 정착하다 하였다. 임금이 말하기를,
&amp;ldquo;이제 이를 받들고 전하께서 아뢰기를,
&amp;ldquo;이제 이를 받들고 전하께서 아뢰기를,
&amp;ldquo;이제 이를 받들고 전하께서 아뢰기를,
&amp;ldquo;이제 이를 받들고 전하께서 아뢰기를,
&amp;ldquo;이제 이를 받들고 전하께서 아뢰기를,
&amp;ldquo;이제 이를 받들고 전하께서 아뢰기를,
&amp;ldquo;이제 이를 받들&lt;/p&gt;
&lt;p&gt;2364/2364 - 272s - loss: 1.3297 - accuracy: 0.5777
Epoch 17/100
2364/2364 - 271s - loss: 1.3138 - accuracy: 0.5825
Epoch 18/100
2364/2364 - 270s - loss: 1.3013 - accuracy: 0.5857
Epoch 19/100
2364/2364 - 270s - loss: 1.2859 - accuracy: 0.5904
Epoch 20/100
2364/2364 - 272s - loss: 1.2727 - accuracy: 0.5941
Epoch 21/100&lt;/p&gt;
&lt;p&gt;﻿태조 이성계 선대의 가계. 목조 이안사가 전주에서 삼척·의주를 거쳐 알동에 정착하다 하는 것은 그 고을에 있으면 그 사람을 감하게 하였다. 임금이 말하기를,
&amp;ldquo;각각 15석을 가지고 감사(監司)에서 아뢰기를,
&amp;ldquo;각각 15석을 가지고 감사(監司)에서 아뢰기를,
&amp;ldquo;각각 15석을 가지고 감사(監司)에서 아뢰기를,
&amp;ldquo;각각 15석을 가지고 감사(監司)에서 아뢰기를,
&amp;ldquo;각각 15석을 가지&lt;/p&gt;
&lt;p&gt;2364/2364 - 275s - loss: 1.2607 - accuracy: 0.5970
Epoch 22/100
2364/2364 - 273s - loss: 1.2478 - accuracy: 0.6010
Epoch 23/100
2364/2364 - 273s - loss: 1.2370 - accuracy: 0.6044
Epoch 24/100
2364/2364 - 273s - loss: 1.2262 - accuracy: 0.6075
Epoch 25/100
2364/2364 - 273s - loss: 1.2161 - accuracy: 0.6101
Epoch 26/100&lt;/p&gt;
&lt;p&gt;﻿태조 이성계 선대의 가계. 목조 이안사가 전주에서 삼척·의주를 거쳐 알동에 정착하다 하였다. 임금이 말하기를,
&amp;ldquo;이제 임금이 말하기를,
&amp;ldquo;이제 임금이 말하기를,
&amp;ldquo;이제 임금이 말하기를,
&amp;ldquo;이제 임금이 말하기를,
&amp;ldquo;이제 임금이 말하기를,
&amp;ldquo;이제 임금이 말하기를,
&amp;ldquo;이제 임금이 말하기를,
&amp;ldquo;이제 임금이 말하기를,
&amp;ldquo;이제 임금이 말하기를,
&amp;ldquo;이제 임금이 말하기를,&lt;/p&gt;
&lt;p&gt;2364/2364 - 275s - loss: 1.2050 - accuracy: 0.6134
Epoch 27/100
2364/2364 - 273s - loss: 1.1956 - accuracy: 0.6167
Epoch 28/100
2364/2364 - 273s - loss: 1.1863 - accuracy: 0.6193
Epoch 29/100
2364/2364 - 274s - loss: 1.1771 - accuracy: 0.6223
Epoch 30/100
2364/2364 - 274s - loss: 1.1682 - accuracy: 0.6246
Epoch 31/100&lt;/p&gt;
&lt;p&gt;﻿태조 이성계 선대의 가계. 목조 이안사가 전주에서 삼척·의주를 거쳐 알동에 정착하다 하니, 임금이 말하기를,
&amp;ldquo;이 때에 이르기를,
&amp;ldquo;상석(成石)을 진향하게 하였다. 임금이 말하기를,
&amp;ldquo;이 때에 이르기를,
&amp;ldquo;그러나 이를 주게 하다
예조에서 아뢰기를,
&amp;ldquo;이 때에 이르기를,
&amp;ldquo;그러나 이를 주게 하다
예조에서 아뢰기를,
&amp;ldquo;이 때에 이르기를,
&amp;ldquo;그러나 이를 주게 하다
예조에서 아뢰기를,
&amp;ldquo;이 ㄸ&lt;/p&gt;
&lt;p&gt;2364/2364 - 276s - loss: 1.1608 - accuracy: 0.6267
Epoch 32/100
2364/2364 - 273s - loss: 1.1522 - accuracy: 0.6293
Epoch 33/100
2364/2364 - 273s - loss: 1.1448 - accuracy: 0.6321
Epoch 34/100
2364/2364 - 273s - loss: 1.1370 - accuracy: 0.6344
Epoch 35/100
2364/2364 - 273s - loss: 1.1303 - accuracy: 0.6361
Epoch 36/100&lt;/p&gt;
&lt;p&gt;﻿태조 이성계 선대의 가계. 목조 이안사가 전주에서 삼척·의주를 거쳐 알동에 정착하다 할 것입니다. 이것은 이를 받았다. 임금이 이를 받았다. 임금이 이를 받았다. 임금이 이를 받았다. 임금이 이를 받았다. 임금이 이를 받았다. 임금이 이를 받았다. 임금이 이를 받았다. 임금이 이를 받았다. 임금이 이를 받았다. 임금이 이를 받았다. 임금이&lt;/p&gt;
&lt;p&gt;2364/2364 - 275s - loss: 1.1220 - accuracy: 0.6390
Epoch 37/100
2364/2364 - 272s - loss: 1.1161 - accuracy: 0.6407
Epoch 38/100
2364/2364 - 273s - loss: 1.1087 - accuracy: 0.6429
Epoch 39/100
2364/2364 - 273s - loss: 1.1020 - accuracy: 0.6448
Epoch 40/100
2364/2364 - 272s - loss: 1.0957 - accuracy: 0.6467
Epoch 41/100&lt;/p&gt;
&lt;p&gt;﻿태조 이성계 선대의 가계. 목조 이안사가 전주에서 삼척·의주를 거쳐 알동에 정착하다 한다. 전하께서는 알지 못한 자가 있으면 나와서 아뢰다
예조에서 아뢰기를,
&amp;ldquo;이 앞서 있는 것이 어떻게 아뢰다
예조에서 아뢰기를,
&amp;ldquo;이 앞서 정지는 알지 못한 자가 있으면 날이 없으니, 이제 있는 것이 어떻겠습니까. 이제 상소하기를,
&amp;ldquo;이 앞서 중에 있는 것은 알지 못한 작&lt;/p&gt;
&lt;p&gt;2364/2364 - 268s - loss: 1.0895 - accuracy: 0.6494
Epoch 42/100
2364/2364 - 265s - loss: 1.0821 - accuracy: 0.6507
Epoch 43/100
2364/2364 - 265s - loss: 1.0770 - accuracy: 0.6527
Epoch 44/100
2364/2364 - 265s - loss: 1.0712 - accuracy: 0.6547
Epoch 45/100
2364/2364 - 266s - loss: 1.0654 - accuracy: 0.6565
Epoch 46/100&lt;/p&gt;
&lt;p&gt;﻿태조 이성계 선대의 가계. 목조 이안사가 전주에서 삼척·의주를 거쳐 알동에 정착하다 하여 그 직임을 만들어 보낸 자는 이러하였다. 임금이 말하기를,
&amp;ldquo;근일의 일을 이루지 않는다면 어찌 그 직임을 만들어 주는 것이 아니었다. ’고 하였습니다. 이제 상상도 염려되는 것이 아니었다. ’고 하였습니다. 신 등이 상소하기를,
&amp;ldquo;근일의 일을 이루지 않는다면 어ㅉ&lt;/p&gt;
&lt;p&gt;2364/2364 - 268s - loss: 1.0604 - accuracy: 0.6578
Epoch 47/100
2364/2364 - 266s - loss: 1.0563 - accuracy: 0.6588
Epoch 48/100
2364/2364 - 265s - loss: 1.0498 - accuracy: 0.6608
Epoch 49/100
2364/2364 - 266s - loss: 1.0455 - accuracy: 0.6626
Epoch 50/100
2364/2364 - 266s - loss: 1.0404 - accuracy: 0.6640
Epoch 51/100&lt;/p&gt;
&lt;p&gt;﻿태조 이성계 선대의 가계. 목조 이안사가 전주에서 삼척·의주를 거쳐 알동에 정착하다 하였으니 그 장(狀)을 강하게 하고, 상소를 이루게 하고, 이를 죽입니다. 이제 이미 정한 사람은 그 장(狀)을 기다리기를 청하였으니, 이것은 그를 구경하고 서울에 의거하여 아뢰기를,
&amp;ldquo;경연(經筵司) 이상의 아들이 이를 주고, 우의정 신주(春秋)를 사용하고, 한 사람은 그 장차 북쪽에 ㄷ&lt;/p&gt;
&lt;p&gt;2364/2364 - 270s - loss: 1.0354 - accuracy: 0.6654
Epoch 52/100
2364/2364 - 267s - loss: 1.0294 - accuracy: 0.6679
Epoch 53/100
2364/2364 - 265s - loss: 1.0260 - accuracy: 0.6683
Epoch 54/100
2364/2364 - 267s - loss: 1.0213 - accuracy: 0.6698
Epoch 55/100
2364/2364 - 270s - loss: 1.0165 - accuracy: 0.6715
Epoch 56/100&lt;/p&gt;
&lt;p&gt;﻿태조 이성계 선대의 가계. 목조 이안사가 전주에서 삼척·의주를 거쳐 알동에 정착하다 하였다. 이를 주었다. 임금이 말하기를,
&amp;ldquo;이 앞에 있어서 아뢰기를,
&amp;ldquo;이 사람이 일찍이 행하고, 인도하여 아뢰기를,
&amp;ldquo;이 사람이 있다. 그 아들이 아뢰기를,
&amp;ldquo;근일에 이르러 있다. 그 아들이 이를 주었다. 임금이 말하기를,
&amp;ldquo;이 앞에 있어서 아뢰기를,
&amp;ldquo;이 앞에 드리게 하다
임금이 말&lt;/p&gt;
&lt;p&gt;2364/2364 - 272s - loss: 1.0129 - accuracy: 0.6727
Epoch 57/100
2364/2364 - 270s - loss: 1.0085 - accuracy: 0.6744
Epoch 58/100
2364/2364 - 271s - loss: 1.0048 - accuracy: 0.6753
Epoch 59/100
2364/2364 - 270s - loss: 1.0004 - accuracy: 0.6765
Epoch 60/100
2364/2364 - 270s - loss: 0.9973 - accuracy: 0.6772
Epoch 61/100&lt;/p&gt;
&lt;p&gt;﻿태조 이성계 선대의 가계. 목조 이안사가 전주에서 삼척·의주를 거쳐 알동에 정착하다 하여 바라옵건대, 상소(上疏)하여 상고하여 아뢰기를,
&amp;ldquo;이 사람은 반드시 사이에 있었으니, 이것을 아뢰다
의정부에서 아뢰기를,
&amp;ldquo;이 사람은 임금이 말하기를, ‘고을의 각 고을의 성을 일으키고 돌아왔으나, 이는 이름을 인도하여 아뢰기를,
&amp;ldquo;성상의 의논을 인도하여 아뢰기를,
&amp;ldquo;이 ㅇ&lt;/p&gt;
&lt;p&gt;2364/2364 - 272s - loss: 0.9945 - accuracy: 0.6779
Epoch 62/100
2364/2364 - 270s - loss: 0.9906 - accuracy: 0.6801
Epoch 63/100
2364/2364 - 270s - loss: 0.9874 - accuracy: 0.6808
Epoch 64/100
2364/2364 - 270s - loss: 0.9834 - accuracy: 0.6817
Epoch 65/100
2364/2364 - 270s - loss: 0.9804 - accuracy: 0.6828
Epoch 66/100&lt;/p&gt;
&lt;p&gt;﻿태조 이성계 선대의 가계. 목조 이안사가 전주에서 삼척·의주를 거쳐 알동에 정착하다 하고, 이에 이미 다행하게 되면 일이 있었다. 임금이 말하기를,
&amp;ldquo;이 사람이 있었다. 임금이 말하기를,
&amp;ldquo;이 사람의 상소를 인도하여 아뢰기를,
&amp;ldquo;신 등이 이르기를,
&amp;ldquo;만약 대마도는 이러한 일을 맡아서 연훙한 일이 없었으나, 윤허하지 않고 있으니, 청컨대 이제 앞에 의하여 알&lt;/p&gt;
&lt;p&gt;2364/2364 - 272s - loss: 0.9771 - accuracy: 0.6839
Epoch 67/100
2364/2364 - 270s - loss: 0.9758 - accuracy: 0.6841
Epoch 68/100
2364/2364 - 270s - loss: 0.9714 - accuracy: 0.6859
Epoch 69/100
2364/2364 - 270s - loss: 0.9699 - accuracy: 0.6862
Epoch 70/100
2364/2364 - 270s - loss: 0.9658 - accuracy: 0.6871
Epoch 71/100&lt;/p&gt;
&lt;p&gt;﻿태조 이성계 선대의 가계. 목조 이안사가 전주에서 삼척·의주를 거쳐 알동에 정착하다 하여 그대로 따랐다.경상도 등을 아뢰다
상왕이 사신에게 말하기를,
&amp;ldquo;이보다 큰 곳을 정지하기를,
&amp;ldquo;이보다 큰 곳이 없었으므로 나와 같이 없어질 수 없는데, 이를 주다
의정부에서 상소하기를,
&amp;ldquo;이보다 큰 곳을 정지하기를 의심하고 있었는데, 지금 남을 것이다. 이제 상(喪)에 ㅇ&lt;/p&gt;
&lt;p&gt;2364/2364 - 272s - loss: 0.9634 - accuracy: 0.6882
Epoch 72/100
2364/2364 - 270s - loss: 0.9626 - accuracy: 0.6882
Epoch 73/100
2364/2364 - 270s - loss: 0.9582 - accuracy: 0.6893
Epoch 74/100
2364/2364 - 270s - loss: 0.9563 - accuracy: 0.6894
Epoch 75/100
2364/2364 - 270s - loss: 0.9539 - accuracy: 0.6913
Epoch 76/100&lt;/p&gt;
&lt;p&gt;﻿태조 이성계 선대의 가계. 목조 이안사가 전주에서 삼척·의주를 거쳐 알동에 정착하다 하겠습니다. 이를 금하게 하다가 집에 있으면 지금 아니라 이를 만들어 보내어 이를 만들어 보내어 이를 만약 관찰사(咸吉道都節制使) 윤장(印章)·정하여 인명하면서 군사를 가지고 있으니, 청컨대 이제부터는 이직(遞職)하여 아뢰기를,
&amp;ldquo;이제 이미 전하께서 이를 주다
정사(慶事)가 만약 그 인ㅇ&lt;/p&gt;
&lt;p&gt;2364/2364 - 272s - loss: 0.9535 - accuracy: 0.6909
Epoch 77/100
2364/2364 - 270s - loss: 0.9502 - accuracy: 0.6919
Epoch 78/100
2364/2364 - 270s - loss: 0.9518 - accuracy: 0.6904
Epoch 79/100
2364/2364 - 270s - loss: 0.9459 - accuracy: 0.6932
Epoch 80/100
2364/2364 - 269s - loss: 0.9433 - accuracy: 0.6941
Epoch 81/100&lt;/p&gt;
&lt;p&gt;﻿태조 이성계 선대의 가계. 목조 이안사가 전주에서 삼척·의주를 거쳐 알동에 정착하다 하다가 중국 종사를 보고 대장궁에 나아가 그 중에 있었으나, 이미 전에 의거하여 전하께서 아뢰기를,
&amp;ldquo;전일의 의논을 거느리고 돌아간 공신이 불을 드리는 것은 이미 공사를 보고 같은 것을 몹아서 임금을 입었으나, 이미 종사를 보고 대장군의 집에 있었으나 윤허하지 않ㅇ&lt;/p&gt;
&lt;p&gt;2364/2364 - 272s - loss: 0.9425 - accuracy: 0.6940
Epoch 82/100
2364/2364 - 270s - loss: 0.9436 - accuracy: 0.6937
Epoch 83/100
2364/2364 - 272s - loss: 0.9405 - accuracy: 0.6945
Epoch 84/100
2364/2364 - 270s - loss: 0.9372 - accuracy: 0.6959
Epoch 85/100
2364/2364 - 268s - loss: 0.9354 - accuracy: 0.6963
Epoch 86/100&lt;/p&gt;
&lt;p&gt;﻿태조 이성계 선대의 가계. 목조 이안사가 전주에서 삼척·의주를 거쳐 알동에 정착하다 하여, 일이 있으니, 청컨대 그 죄를 주고, 인사(人事)를 올리는 것이 없습니다. 그러나, 오직 관가(三文理)로 하여금 임금이 상소하여 사신에게 전지하기를,
&amp;ldquo;신이 임금이 있었다. 임금이 상성(宮城)을 정하여 일어나 전하께서 상소하여 사신에게 장숙주(申叔舟)에서 일찍이 각기 그 공이&lt;/p&gt;
&lt;p&gt;2364/2364 - 272s - loss: 0.9354 - accuracy: 0.6957
Epoch 87/100
2364/2364 - 271s - loss: 0.9322 - accuracy: 0.6968
Epoch 88/100
2364/2364 - 271s - loss: 0.9318 - accuracy: 0.6971
Epoch 89/100
2364/2364 - 271s - loss: 0.9336 - accuracy: 0.6964
Epoch 90/100
2364/2364 - 271s - loss: 0.9294 - accuracy: 0.6979
Epoch 91/100&lt;/p&gt;
&lt;p&gt;﻿태조 이성계 선대의 가계. 목조 이안사가 전주에서 삼척·의주를 거쳐 알동에 정착하다 하였으니, 일찍이 일을 무엇이 있었다. 임금이 말하기를,
&amp;ldquo;정성을 인송하게 하다
임금이 말하기를,
&amp;ldquo;이제부터 그 죄를 내렸다. 임금이 말하기를,
&amp;ldquo;이제부터 어려운 것이 없습니다. 신이 임금이 이를 비록 일으켜든 중일에 가서 주었다.상왕이 불을 다시 성하게 여기어 성서&lt;/p&gt;
&lt;p&gt;2364/2364 - 273s - loss: 0.9289 - accuracy: 0.6981
Epoch 92/100
2364/2364 - 272s - loss: 0.9283 - accuracy: 0.6979
Epoch 93/100
2364/2364 - 272s - loss: 0.9259 - accuracy: 0.6989
Epoch 94/100
2364/2364 - 272s - loss: 0.9252 - accuracy: 0.6992
Epoch 95/100
2364/2364 - 272s - loss: 0.9245 - accuracy: 0.6994
Epoch 96/100&lt;/p&gt;
&lt;p&gt;﻿태조 이성계 선대의 가계. 목조 이안사가 전주에서 삼척·의주를 거쳐 알동에 정착하다 하였다. 임금이 말하기를,
&amp;ldquo;정상이 들어온 자에게 정지하고, 이어서 상언(上言)하여 정하고, 또 정진수가 처음에 이르렀으니, 다음에는 아래에 있는 자가 있으면 마땅히 윤망한 것이 아니라, 이에 있어서 상언(上言)하여 상언(上言)하여 상언(上言)하여 상언(上言)하여 상언(上言)하여 정부에&lt;/p&gt;
&lt;p&gt;2364/2364 - 273s - loss: 0.9235 - accuracy: 0.6991
Epoch 97/100
2364/2364 - 271s - loss: 0.9234 - accuracy: 0.6989
Epoch 98/100
2364/2364 - 271s - loss: 0.9217 - accuracy: 0.6998
Epoch 99/100
2364/2364 - 272s - loss: 0.9215 - accuracy: 0.7000
Epoch 100/100&lt;/p&gt;
&lt;p&gt;﻿태조 이성계 선대의 가계. 목조 이안사가 전주에서 삼척·의주를 거쳐 알동에 정착하다 하여, 일이 있으면, 중국의 음사운의 청한 것은 아뢰기를,
&amp;ldquo;각도의 아들을 보내어 여러 신하들이 이르기를, ‘군사의 임금이 말하기를,
&amp;ldquo;이박한 그렇게 한 것은 아뢰기를,
&amp;ldquo;각도에 이르렀던 것이 없습니다. 그러나 이에 가서 수령을 감동하다
사헌부에서 계하기를,
&amp;ldquo;원산군(楊根&lt;/p&gt;
&lt;p&gt;2364/2364 - 274s - loss: 0.9206 - accuracy: 0.6999&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;모형 학습 시, 굉장히 많은 시간이 소요됩니다. 따라서 모형을 확인하려면 시간을 충분히 가지시고, 모형 학습을 진행합니다.&lt;/li&gt;
&lt;li&gt;단어 생성 모델처럼 같은 문장을 자소 단위로 넣어서 에포크가 끝날 때마다 생성 결과를 확인합니다.&lt;/li&gt;
&lt;li&gt;단어 생성 모델과 마찬가지로 처음에는 반복되는 패턴이 자주 나타나지만 점점 그럴듯한 결과를 만들어내기 시작합니다.&lt;/li&gt;
&lt;li&gt;특히 한자와 괄호를 그대로 학습에 사용하고 있기 때문에 한글과 한자의 병기도 잘 하는 것을 볼 수 있습니다.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;5-학습모형-테스트&#34;&gt;(5) 학습모형 테스트&lt;/h3&gt;
&lt;p&gt;이제 임의의 문장을 통해서 학습을 진행합니다. 마찬가지로 기존 코드와 크게 달라진 것은 없으나 몇몇 변수만 수정하면 됩니다.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;next_words에서 next_chars로 변경&lt;/li&gt;
&lt;li&gt;word2idx에서 char2idx로 변경&lt;/li&gt;
&lt;li&gt;idx2word에서 idx2char로 변경&lt;/li&gt;
&lt;/ul&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span style=&#34;color:#f92672&#34;&gt;from&lt;/span&gt; tensorflow.keras.preprocessing.sequence &lt;span style=&#34;color:#f92672&#34;&gt;import&lt;/span&gt; pad_sequences
test_sentence &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;동헌에 나가 공무를 본 후 활 십오 순을 쏘았다&amp;#39;&lt;/span&gt;
test_sentence &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; jamotools&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;split_syllables(test_sentence)

next_chars &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;300&lt;/span&gt;
&lt;span style=&#34;color:#66d9ef&#34;&gt;for&lt;/span&gt; _ &lt;span style=&#34;color:#f92672&#34;&gt;in&lt;/span&gt; range(next_chars):
    test_text_X &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; test_sentence[&lt;span style=&#34;color:#f92672&#34;&gt;-&lt;/span&gt;seq_length:]
    test_text_X &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; np&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;array([char2idx[c] &lt;span style=&#34;color:#66d9ef&#34;&gt;if&lt;/span&gt; c &lt;span style=&#34;color:#f92672&#34;&gt;in&lt;/span&gt; char2idx &lt;span style=&#34;color:#66d9ef&#34;&gt;else&lt;/span&gt; char2idx[&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;UNK&amp;#39;&lt;/span&gt;] &lt;span style=&#34;color:#66d9ef&#34;&gt;for&lt;/span&gt; c &lt;span style=&#34;color:#f92672&#34;&gt;in&lt;/span&gt; test_text_X])
    test_text_X &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; pad_sequences([test_text_X], maxlen&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;seq_length, padding&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;pre&amp;#39;&lt;/span&gt;, value&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;char2idx[&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;UNK&amp;#39;&lt;/span&gt;])
    
    output_idx &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; model&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;predict_classes(test_text_X)
    test_sentence &lt;span style=&#34;color:#f92672&#34;&gt;+=&lt;/span&gt; idx2char[output_idx[&lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;]]
    

&lt;span style=&#34;color:#66d9ef&#34;&gt;print&lt;/span&gt;(jamotools&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;join_jamos(test_sentence))
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;동헌에 나가 공무를 본 후 활 십오 순을 쏘았다. 임금이 말하기를,
&amp;ldquo;이보다 큰 공상은 그 집에 돌아오다
정사를 보았다. 임금이 말하기를,
&amp;ldquo;이방성을 아뢰다
함길도 감사가 이미 나라를 행하였다.상왕이 그 사람을 금하다
임금이 말하기를,
&amp;ldquo;이보다 큰 공상은 그 집에 돌아온다. 【모든 것을 보내어 여러 관원은 농산ㄱ&lt;/p&gt;
&lt;p&gt;여기서도 뒤로 가면 비슷한 문장이 다시 나오고 있지만, 한글을 정확하게 조합할 수 있도록 네트워크가 자소를 생성하고 있음을 보여주고 있습니다.&lt;/p&gt;
&lt;h2 id=&#34;iii-정리&#34;&gt;III. 정리&lt;/h2&gt;
&lt;p&gt;여기서 정리해야 하는 것은 순환신경망이 언제 쓰이는 것인지, LSTM, SimpleRNN, GRU레이어에 대해 학습하였습니다.&lt;/p&gt;
&lt;p&gt;기본적인 이론을 중심으로 재 학습을 하는 걸 권해드립니다. 특히, 자연어처리는 아직도 연구중인 분야고, 사실 굉장히 까다롭기 때문에, 학습자가 특별한 Mission이 있지 않으면 감정 분석 정도에서 마무리하는 것이 좋습니다. 자연어처리를 통한 비즈니스 활용 연구는 실제로 대기업에서 본격적인 연구가 가능합니다.&lt;/p&gt;
&lt;p&gt;자연어처리는 학습을 시켜서 아시겠지만, 학습시간이 매우 오래 걸리는 대신, 결과물이 사실 애매모호한 경우가 많습니다. 기본적인 이론을 바탕으로 실무에서는 클라우드를 활용한 서비스 개발(예: 챗봇)에 집중하시는 것을 권유드립니다.&lt;/p&gt;
&lt;h2 id=&#34;iv-연습-파일&#34;&gt;IV. 연습 파일&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://colab.research.google.com/github/chloevan/deeplearningAI/blob/master/tensorflow2.0/ch7_4_naturalLanguageGeneration(2).ipynb&#34;&gt;구글 Colab에서 직접 연습해보자&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;v-reference&#34;&gt;V. Reference&lt;/h2&gt;
&lt;p&gt;김환희. (2020). 시작하세요! 텐서플로 2.0 프로그래밍: 기초 이론부터 실전 예제까지 한번에 끝내는 머신러닝, 딥러닝 핵심 가이드. 서울: 위키북스.&lt;/p&gt;
&lt;p&gt;Karpathy, A. (2015). The Unreasonable Effectiveness of Recurrent Neural Networks. Retrieved April 26, 2020, from &lt;a href=&#34;http://karpathy.github.io/2015/05/21/rnn-effectiveness/&#34;&gt;http://karpathy.github.io/2015/05/21/rnn-effectiveness/&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Tensorflow 2.0 Tutorial ch7.4 - (1) 단어 단위 생성</title>
      <link>https://chloevan.github.io/python/tensorflow2.0/ch7_4_naturallanguagegeneration1/</link>
      <pubDate>Mon, 27 Apr 2020 10:20:30 +0900</pubDate>
      
      <guid>https://chloevan.github.io/python/tensorflow2.0/ch7_4_naturallanguagegeneration1/</guid>
      <description>&lt;h2 id=&#34;공지&#34;&gt;공지&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;본 Tutorial은 교재 &lt;code&gt;시작하세요 텐서플로 2.0 프로그래밍&lt;/code&gt;의 강사에게 국비교육 강의를 듣는 사람들에게 자료 제공을 목적으로 제작하였습니다.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;강사의 주관적인 판단으로 압축해서 자료를 정리하였기 때문에, 자세하게 공부를 하고 싶으신 분은 반드시 교재를 구매하실 것을 권해드립니다.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;img src=&#34;https://chloevan.github.io/img/tensorflow2.0/book.jpg&#34; alt=&#34;&#34;&gt;&lt;!-- raw HTML omitted --&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;본 교재 외에 강사가 추가한 내용에 대한 Reference를 확인하셔서, 추가적으로 학습하시는 것을 권유드립니다.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;tutorial&#34;&gt;Tutorial&lt;/h2&gt;
&lt;p&gt;이전 강의가 궁금하신 분들은 아래에서 선택하여 추가 학습 하시기를 바랍니다.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://chloevan.github.io/python/tensorflow2.0/googlecolab/&#34;&gt;Google Colab Tensorflow 2.0 Installation&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://chloevan.github.io/python/tensorflow2.0/ch3_3_1_random_signoid/&#34;&gt;Tensorflow 2.0 Tutorial ch3.3.1 - 난수 생성 및 시그모이드 함수&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://chloevan.github.io/python/tensorflow2.0/ch3_3_2_random_signoid_bias/&#34;&gt;Tensorflow 2.0 Tutorial ch3.3.2 - 난수 생성 및 시그모이드 함수 편향성&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://chloevan.github.io/python/tensorflow2.0/ch3_3_3_network_and/&#34;&gt;Tensorflow 2.0 Tutorial ch3.3.3 - 첫번째 신경망 네트워크 - AND&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://chloevan.github.io/python/tensorflow2.0/ch3_3_4_network_or/&#34;&gt;Tensorflow 2.0 Tutorial ch3.3.4 - 두번째 신경망 네트워크 - OR&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://chloevan.github.io/python/tensorflow2.0/ch3_3_5_network_xor/&#34;&gt;Tensorflow 2.0 Tutorial ch3.3.5 - 세번째 신경망 네트워크 - XOR&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://chloevan.github.io/python/tensorflow2.0/ch4_1_linear_regression/&#34;&gt;Tensorflow 2.0 Tutorial ch4.1 - 선형회귀&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://chloevan.github.io/python/tensorflow2.0/ch4_2_multiple_linear_regression/&#34;&gt;Tensorflow 2.0 Tutorial ch4.2 - 다항회귀&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://chloevan.github.io/python/tensorflow2.0/ch4_3_regression_with_deeplearning/&#34;&gt;Tensorflow 2.0 Tutorial ch4.3 - 딥러닝 네트워크를 이용한 회귀&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://chloevan.github.io/python/tensorflow2.0/ch4_4_boston_housing_deeplearning/&#34;&gt;Tensorflow 2.0 Tutorial ch4.4 - 보스턴 주택 가격 데이터세트&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://chloevan.github.io/python/tensorflow2.0/ch5_1_binary_classification/&#34;&gt;Tensorflow 2.0 Tutorial ch5.1 - 분류&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://chloevan.github.io/python/tensorflow2.0/ch5_2_multi_classification/&#34;&gt;Tensorflow 2.0 Tutorial ch5.2 - 다항분류&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://chloevan.github.io/python/tensorflow2.0/ch5_3_fashion_mnist/&#34;&gt;Tensorflow 2.0 Tutorial ch5.3 - Fashion MNIST&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://chloevan.github.io/python/tensorflow2.0/ch6_1_2_cnn_theory/&#34;&gt;Tensorflow 2.0 Tutorial ch6.1-2 - CNN 이론&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://chloevan.github.io/python/tensorflow2.0/ch6_3_fashion_mnist_with_cnn/&#34;&gt;Tensorflow 2.0 Tutorial ch6.3 - Fashion MNIST with CNN 실습&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://chloevan.github.io/python/tensorflow2.0/ch6_4_improve_performance/&#34;&gt;Tensorflow 2.0 Tutorial ch6.4 - 모형의 성능 높이기&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://chloevan.github.io/python/tensorflow2.0/ch7_1_2_rnn_theory1/&#34;&gt;Tensorflow 2.0 Tutorial ch7.1 - RNN 이론 (1)&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://chloevan.github.io/python/tensorflow2.0/ch7_1_2_rnn_theory2/&#34;&gt;Tensorflow 2.0 Tutorial ch7.1 - RNN 이론 (2)&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://chloevan.github.io/python/tensorflow2.0/ch7_3_sentimentanalysis/&#34;&gt;Tensorflow 2.0 Tutorial ch7.3 - 긍정, 부정 감성 분석&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;i-개요&#34;&gt;I. 개요&lt;/h2&gt;
&lt;p&gt;테슬라 &lt;code&gt;AI Director&lt;/code&gt;인 안드레아 카르파티(&lt;code&gt;Andrej Karpathy&lt;/code&gt;)는 &lt;code&gt;The Unreasonable Effectiveness of Recurrent Neural Networks&lt;/code&gt;라는 글을 개인 블로그에 작성했는데, 짧게 요약하면 문자 단위의 순환 신경망이 셰익스피어의 희곡, 소스코드, &lt;code&gt;Latex&lt;/code&gt;등을 재생산하는데 순환 신경망이 효과적이라는 것을 보여줍니다.&lt;/p&gt;
&lt;p&gt;이를 바탕으로, 한글 원본 텍스트를 자소 단위와 단어 단위로 나눠서 순환 신경망으로 생성해보도록 합니다.&lt;/p&gt;
&lt;h2 id=&#34;ii-단어-단위-생성&#34;&gt;II. 단어 단위 생성&lt;/h2&gt;
&lt;p&gt;이번에 작업할 원본 텍스트는 국사편찬위원회에서 제공하는 조선왕조실록 국문 번역본입니다. 먼저 &lt;a href=&#34;https://raw.githubusercontent.com/greentec/greentec.github.io/master/public/other/data/chosundynasty/corpus.txt&#34;&gt;데이터&lt;/a&gt;를 다운로드 합니다.&lt;sup id=&#34;fnref:1&#34;&gt;&lt;a href=&#34;#fn:1&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;1&lt;/a&gt;&lt;/sup&gt;&lt;/p&gt;
&lt;p&gt;이제 본격적으로 소스코드를 작성하도록 합니다.&lt;/p&gt;
&lt;h3 id=&#34;1-데이터-로드&#34;&gt;(1) 데이터 로드&lt;/h3&gt;
&lt;p&gt;데이터 파일은 약 62MB 정도입니다.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span style=&#34;color:#75715e&#34;&gt;# 텐서플로 2 버전 선택&lt;/span&gt;
&lt;span style=&#34;color:#66d9ef&#34;&gt;try&lt;/span&gt;:
    &lt;span style=&#34;color:#75715e&#34;&gt;# %tensorflow_version only exists in Colab.&lt;/span&gt;
    &lt;span style=&#34;color:#f92672&#34;&gt;%&lt;/span&gt;tensorflow_version &lt;span style=&#34;color:#ae81ff&#34;&gt;2.&lt;/span&gt;x
&lt;span style=&#34;color:#66d9ef&#34;&gt;except&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;Exception&lt;/span&gt;:
    &lt;span style=&#34;color:#66d9ef&#34;&gt;pass&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;import&lt;/span&gt; tensorflow &lt;span style=&#34;color:#f92672&#34;&gt;as&lt;/span&gt; tf
&lt;span style=&#34;color:#f92672&#34;&gt;import&lt;/span&gt; numpy &lt;span style=&#34;color:#f92672&#34;&gt;as&lt;/span&gt; np
&lt;span style=&#34;color:#f92672&#34;&gt;import&lt;/span&gt; pandas &lt;span style=&#34;color:#f92672&#34;&gt;as&lt;/span&gt; pd
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;path_to_train_file &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; tf&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;keras&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;utils&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;get_file(&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;input.txt&amp;#39;&lt;/span&gt;, &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;https://raw.githubusercontent.com/greentec/greentec.github.io/master/public/other/data/chosundynasty/corpus.txt&amp;#39;&lt;/span&gt;)
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;pre&gt;&lt;code&gt;Downloading data from https://raw.githubusercontent.com/greentec/greentec.github.io/master/public/other/data/chosundynasty/corpus.txt
62013440/62012502 [==============================] - 1s 0us/step
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;데이터를 메모리에 불러옵니다. 인코딩 형식으로 &lt;code&gt;utf-8&lt;/code&gt;을 지정한 뒤 텍스트가 총 몇 자인지 확인해봅니다.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;train_text &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; open(path_to_train_file, &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;rb&amp;#39;&lt;/span&gt;)&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;read()&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;decode(encoding&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;utf-8&amp;#39;&lt;/span&gt;)
&lt;span style=&#34;color:#66d9ef&#34;&gt;print&lt;/span&gt;(&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;Length of text: {} characters&amp;#39;&lt;/span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;format(len(train_text)))
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;pre&gt;&lt;code&gt;Length of text: 26265493 characters
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;처음 100자를 확인한 후, 실제 &lt;a href=&#34;https://raw.githubusercontent.com/greentec/greentec.github.io/master/public/other/data/chosundynasty/corpus.txt&#34;&gt;데이터&lt;/a&gt;와 일치하는지 확인해봅니다.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;print&lt;/span&gt;(train_text[:&lt;span style=&#34;color:#ae81ff&#34;&gt;100&lt;/span&gt;])
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;pre&gt;&lt;code&gt;﻿태조 이성계 선대의 가계. 목조 이안사가 전주에서 삼척·의주를 거쳐 알동에 정착하다 
태조 강헌 지인 계운 성문 신무 대왕(太祖康獻至仁啓運聖文神武大王)의 성은 이씨(李氏)요, 휘
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;일치하는지 확인이 완료되었다면, 다음 소스코드를 진행하면 됩니다. 다운로드 시, 링크가 달라졌다면, 꼭 댓글을 남겨주시기를 바랍니다.&lt;/p&gt;
&lt;h3 id=&#34;2-데이터-문자열-전처리&#34;&gt;(2) 데이터 문자열 전처리&lt;/h3&gt;
&lt;p&gt;한자가 많은 부분을 차지하는데, 여기에서는 단어 기반의 생성을 위해 한자와 한자가 들어간 괄호는 생략합니다. 이전 포스트에 비슷하지만, 조금 다릅니다. 특히 영문 관련 처리는 모두 삭제했습니다. 또한, 한자와 함께 들어가 있는 괄호도 같이 삭제합니다. 또한, 개행 문자(&#39;\n&amp;rsquo;)의 보존을 위해 텍스트를 먼저 개행 문자로 나눈 뒤에 다시 합칠 때 개행 문자를 추가합니다. 꼭 주의해서 사용자 정의 함수 &lt;code&gt;def&lt;/code&gt;를 작성하시기를 바랍니다. 이렇게 정제 과정을 거치면 한자어와 괄호는 보이지 않을 것입니다.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span style=&#34;color:#f92672&#34;&gt;import&lt;/span&gt; re
&lt;span style=&#34;color:#75715e&#34;&gt;# From https://github.com/yoonkim/CNN_sentence/blob/master/process_data.py&lt;/span&gt;
&lt;span style=&#34;color:#66d9ef&#34;&gt;def&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;clean_str&lt;/span&gt;(string):    
    string &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; re&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;sub(&lt;span style=&#34;color:#e6db74&#34;&gt;r&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;[^가-힣A-Za-z0-9(),!?\&amp;#39;\`]&amp;#34;&lt;/span&gt;, &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34; &amp;#34;&lt;/span&gt;, string)
    string &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; re&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;sub(&lt;span style=&#34;color:#e6db74&#34;&gt;r&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;\&amp;#39;ll&amp;#34;&lt;/span&gt;, &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34; &lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;\&amp;#39;&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;ll&amp;#34;&lt;/span&gt;, string)
    string &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; re&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;sub(&lt;span style=&#34;color:#e6db74&#34;&gt;r&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;,&amp;#34;&lt;/span&gt;, &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34; , &amp;#34;&lt;/span&gt;, string)
    string &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; re&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;sub(&lt;span style=&#34;color:#e6db74&#34;&gt;r&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;!&amp;#34;&lt;/span&gt;, &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34; ! &amp;#34;&lt;/span&gt;, string)
    string &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; re&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;sub(&lt;span style=&#34;color:#e6db74&#34;&gt;r&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;\(&amp;#34;&lt;/span&gt;, &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;&amp;#34;&lt;/span&gt;, string)
    string &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; re&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;sub(&lt;span style=&#34;color:#e6db74&#34;&gt;r&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;\)&amp;#34;&lt;/span&gt;, &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;&amp;#34;&lt;/span&gt;, string)
    string &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; re&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;sub(&lt;span style=&#34;color:#e6db74&#34;&gt;r&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;\?&amp;#34;&lt;/span&gt;, &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34; \? &amp;#34;&lt;/span&gt;, string)
    string &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; re&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;sub(&lt;span style=&#34;color:#e6db74&#34;&gt;r&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;\s{2,}&amp;#34;&lt;/span&gt;, &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34; &amp;#34;&lt;/span&gt;, string)
    string &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; re&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;sub(&lt;span style=&#34;color:#e6db74&#34;&gt;r&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;\&amp;#39;{2,}&amp;#34;&lt;/span&gt;, &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;\&amp;#39;&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;&lt;/span&gt;, string)
    string &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; re&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;sub(&lt;span style=&#34;color:#e6db74&#34;&gt;r&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;\&amp;#39;&amp;#34;&lt;/span&gt;, &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;&amp;#34;&lt;/span&gt;, string)

    &lt;span style=&#34;color:#66d9ef&#34;&gt;return&lt;/span&gt; string


train_text &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; train_text&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;split(&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;\n&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;&lt;/span&gt;)
train_text &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; [clean_str(sentence) &lt;span style=&#34;color:#66d9ef&#34;&gt;for&lt;/span&gt; sentence &lt;span style=&#34;color:#f92672&#34;&gt;in&lt;/span&gt; train_text]
train_text_X &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; []
&lt;span style=&#34;color:#66d9ef&#34;&gt;for&lt;/span&gt; sentence &lt;span style=&#34;color:#f92672&#34;&gt;in&lt;/span&gt; train_text:
    train_text_X&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;extend(sentence&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;split(&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39; &amp;#39;&lt;/span&gt;))
    train_text_X&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;append(&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;\n&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;&lt;/span&gt;)
    
train_text_X &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; [word &lt;span style=&#34;color:#66d9ef&#34;&gt;for&lt;/span&gt; word &lt;span style=&#34;color:#f92672&#34;&gt;in&lt;/span&gt; train_text_X &lt;span style=&#34;color:#66d9ef&#34;&gt;if&lt;/span&gt; word &lt;span style=&#34;color:#f92672&#34;&gt;!=&lt;/span&gt; &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;&amp;#39;&lt;/span&gt;]

&lt;span style=&#34;color:#66d9ef&#34;&gt;print&lt;/span&gt;(train_text_X[:&lt;span style=&#34;color:#ae81ff&#34;&gt;20&lt;/span&gt;])
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;pre&gt;&lt;code&gt;[&#39;태조&#39;, &#39;이성계&#39;, &#39;선대의&#39;, &#39;가계&#39;, &#39;목조&#39;, &#39;이안사가&#39;, &#39;전주에서&#39;, &#39;삼척&#39;, &#39;의주를&#39;, &#39;거쳐&#39;, &#39;알동에&#39;, &#39;정착하다&#39;, &#39;\n&#39;, &#39;태조&#39;, &#39;강헌&#39;, &#39;지인&#39;, &#39;계운&#39;, &#39;성문&#39;, &#39;신무&#39;, &#39;대왕&#39;]
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;결과가 잘 나온 것을 확인할 수 있습니다. 이제 단어를 토큰화합니다. 이 때, &lt;code&gt;Tokenizer&lt;/code&gt;를 쓰지 않고, 직접 토큰화합니다. 단어의 수가 너무 많고, 모든 단어를 사용할 것이기 때문에, 단어의 빈도 수로 단어를 정렬하는 &lt;code&gt;Tokenizer&lt;/code&gt;를 쓰면 불필요하게 많은 계산 시간을 쓰게 된다.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;vocab &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; sorted(set(train_text_X))
vocab&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;append(&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;UNK&amp;#39;&lt;/span&gt;)
&lt;span style=&#34;color:#66d9ef&#34;&gt;print&lt;/span&gt; (&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;{} unique words&amp;#39;&lt;/span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;format(len(vocab)))

&lt;span style=&#34;color:#75715e&#34;&gt;# vocab list를 숫자로 맵핑하고, 반대도 실행합니다.&lt;/span&gt;
word2idx &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; {u:i &lt;span style=&#34;color:#66d9ef&#34;&gt;for&lt;/span&gt; i, u &lt;span style=&#34;color:#f92672&#34;&gt;in&lt;/span&gt; enumerate(vocab)}
idx2word &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; np&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;array(vocab)

text_as_int &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; np&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;array([word2idx[c] &lt;span style=&#34;color:#66d9ef&#34;&gt;for&lt;/span&gt; c &lt;span style=&#34;color:#f92672&#34;&gt;in&lt;/span&gt; train_text_X])

&lt;span style=&#34;color:#75715e&#34;&gt;# word2idx 의 일부를 알아보기 쉽게 print 해봅니다.&lt;/span&gt;
&lt;span style=&#34;color:#66d9ef&#34;&gt;print&lt;/span&gt;(&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;{&amp;#39;&lt;/span&gt;)
&lt;span style=&#34;color:#66d9ef&#34;&gt;for&lt;/span&gt; word,_ &lt;span style=&#34;color:#f92672&#34;&gt;in&lt;/span&gt; zip(word2idx, range(&lt;span style=&#34;color:#ae81ff&#34;&gt;10&lt;/span&gt;)):
    &lt;span style=&#34;color:#66d9ef&#34;&gt;print&lt;/span&gt;(&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;  {:4s}: {:3d},&amp;#39;&lt;/span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;format(repr(word), word2idx[word]))
&lt;span style=&#34;color:#66d9ef&#34;&gt;print&lt;/span&gt;(&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;  ...&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;\n&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;}&amp;#39;&lt;/span&gt;)

&lt;span style=&#34;color:#66d9ef&#34;&gt;print&lt;/span&gt;(&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;index of UNK: {}&amp;#39;&lt;/span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;format(word2idx[&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;UNK&amp;#39;&lt;/span&gt;]))
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;pre&gt;&lt;code&gt;332640 unique words
{
  &#39;\n&#39;:   0,
  &#39;!&#39; :   1,
  &#39;,&#39; :   2,
  &#39;000명으로&#39;:   3,
  &#39;001&#39;:   4,
  &#39;002&#39;:   5,
  &#39;003&#39;:   6,
  &#39;004&#39;:   7,
  &#39;005&#39;:   8,
  &#39;006&#39;:   9,
  ...
}
index of UNK: 332639
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;2번째 줄에서 텍스트에 들어간 각 단어가 중복되지 않는 리스트를 만든 다음에, 3번째 줄에서는 텍스트에 존재하지 않는 토큰을 &lt;code&gt;UNK&lt;/code&gt;를 넣습니다. 총 단어 수는 332,640이고, 이 중 &lt;code&gt;UNK&lt;/code&gt;의 인덱스는 332,639입니다. 이 인덱스는 나중에 임의의 문장을 입력할 때 텍스트에 미리 준비돼 있지 않았던 단어를 쓸 수 있기 때문에 그에 대한 토큰으로 쓰일 것입니다.&lt;/p&gt;
&lt;p&gt;토큰화가 잘 되었는지 확인하기 위해 처음 20개의 단어에 대한 토큰을 출력합니다.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;print&lt;/span&gt;(train_text_X[:&lt;span style=&#34;color:#ae81ff&#34;&gt;20&lt;/span&gt;])
&lt;span style=&#34;color:#66d9ef&#34;&gt;print&lt;/span&gt;(text_as_int[:&lt;span style=&#34;color:#ae81ff&#34;&gt;20&lt;/span&gt;])
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;pre&gt;&lt;code&gt;[&#39;태조&#39;, &#39;이성계&#39;, &#39;선대의&#39;, &#39;가계&#39;, &#39;목조&#39;, &#39;이안사가&#39;, &#39;전주에서&#39;, &#39;삼척&#39;, &#39;의주를&#39;, &#39;거쳐&#39;, &#39;알동에&#39;, &#39;정착하다&#39;, &#39;\n&#39;, &#39;태조&#39;, &#39;강헌&#39;, &#39;지인&#39;, &#39;계운&#39;, &#39;성문&#39;, &#39;신무&#39;, &#39;대왕&#39;]
[299305 229634 161443  17430 111029 230292 251081 155087 225462  29027
 190295 256129      0 299305  25624 273553  36147 163996 180466  84413]
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;code&gt;태조&lt;/code&gt;는 299,305로 &lt;code&gt;이성계&lt;/code&gt;는 229,634로, 개행 문자는 0으로 토큰 변환이 된 것을 확인할 수 있습니다. 이제 학습을 위한 데이터세트를 만듭니다.&lt;/p&gt;
&lt;h3 id=&#34;3-데이터셋-생성&#34;&gt;(3) 데이터셋 생성&lt;/h3&gt;
&lt;p&gt;이 때, 기존과는 다르게, &lt;code&gt;train_X&lt;/code&gt;, &lt;code&gt;train_Y&lt;/code&gt;를 넘파이 &lt;code&gt;array&lt;/code&gt;로 만드는 방식이 아닌 &lt;code&gt;tf.data.Dataset&lt;/code&gt;를 이용해봅니다. &lt;code&gt;Dataset&lt;/code&gt;의 장점은 간단한 코드로 데이터 섞기, 배치(&lt;code&gt;batch&lt;/code&gt;)수만큼 자르기, 다른 &lt;code&gt;Dataset&lt;/code&gt;에 매핑하기 등을 빠르게 수행할 수 있습니다.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;seq_length &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;25&lt;/span&gt;
examples_per_epoch &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; len(text_as_int) &lt;span style=&#34;color:#f92672&#34;&gt;//&lt;/span&gt; seq_length
sentence_dataset &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; tf&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;data&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;Dataset&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;from_tensor_slices(text_as_int)

sentence_dataset &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; sentence_dataset&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;batch(seq_length&lt;span style=&#34;color:#f92672&#34;&gt;+&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;, drop_remainder&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;True)
&lt;span style=&#34;color:#66d9ef&#34;&gt;for&lt;/span&gt; item &lt;span style=&#34;color:#f92672&#34;&gt;in&lt;/span&gt; sentence_dataset&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;take(&lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;):
    &lt;span style=&#34;color:#66d9ef&#34;&gt;print&lt;/span&gt;(idx2word[item&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;numpy()])
    &lt;span style=&#34;color:#66d9ef&#34;&gt;print&lt;/span&gt;(item&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;numpy())
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;pre&gt;&lt;code&gt;[&#39;태조&#39; &#39;이성계&#39; &#39;선대의&#39; &#39;가계&#39; &#39;목조&#39; &#39;이안사가&#39; &#39;전주에서&#39; &#39;삼척&#39; &#39;의주를&#39; &#39;거쳐&#39; &#39;알동에&#39; &#39;정착하다&#39;
 &#39;\n&#39; &#39;태조&#39; &#39;강헌&#39; &#39;지인&#39; &#39;계운&#39; &#39;성문&#39; &#39;신무&#39; &#39;대왕&#39; &#39;의&#39; &#39;성은&#39; &#39;이씨&#39; &#39;요&#39; &#39;,&#39; &#39;휘&#39;]
[299305 229634 161443  17430 111029 230292 251081 155087 225462  29027
 190295 256129      0 299305  25624 273553  36147 163996 180466  84413
 224182 164549 230248 210912      2 330313]
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;여기서는 &lt;code&gt;seq_length&lt;/code&gt;를 25로 설정해서 25개의 단어가 주어졌을 때, 다음 단어를 예측하도록 데이터를 만듭니다.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;sentence_dataset &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; tf&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;data&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;Dataset&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;from_tensor_slices(text_as_int)
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;&lt;code&gt;Dataset&lt;/code&gt;를 생성하는 코드는 위와 같이 한 줄로 간단합니다.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;sentence_dataset &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; sentence_dataset&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;batch(seq_length&lt;span style=&#34;color:#f92672&#34;&gt;+&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;, drop_remainder&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;True)
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;ul&gt;
&lt;li&gt;&lt;code&gt;Dataset&lt;/code&gt;에 쓰이는 &lt;code&gt;batch()&lt;/code&gt;함수는 &lt;code&gt;Dataset&lt;/code&gt;에서 한번에 반환하는 데이터의 숫자를 지정합니다.&lt;/li&gt;
&lt;li&gt;여기에서는 &lt;code&gt;seq_length_1&lt;/code&gt;을 지정했는데, 처음의 25개 단어와 그 뒤에 오는 정답이 될 1단어를 합쳐서 함께 반환하기 위해서입니다.&lt;/li&gt;
&lt;li&gt;&lt;code&gt;drop_remainder=True&lt;/code&gt;옵션으로 남는 부분은 버리도록 합니다. 그러면 출력해서 의도한대로 26단어가 반환되는 것을 확인할 수 있습니다.&lt;/li&gt;
&lt;/ul&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;def&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;split_input_target&lt;/span&gt;(chunk):
    &lt;span style=&#34;color:#66d9ef&#34;&gt;return&lt;/span&gt; [chunk[:&lt;span style=&#34;color:#f92672&#34;&gt;-&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;], chunk[&lt;span style=&#34;color:#f92672&#34;&gt;-&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;]]

train_dataset &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; sentence_dataset&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;map(split_input_target)
&lt;span style=&#34;color:#66d9ef&#34;&gt;for&lt;/span&gt; x,y &lt;span style=&#34;color:#f92672&#34;&gt;in&lt;/span&gt; train_dataset&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;take(&lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;):
    &lt;span style=&#34;color:#66d9ef&#34;&gt;print&lt;/span&gt;(idx2word[x&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;numpy()])
    &lt;span style=&#34;color:#66d9ef&#34;&gt;print&lt;/span&gt;(x&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;numpy())
    &lt;span style=&#34;color:#66d9ef&#34;&gt;print&lt;/span&gt;(idx2word[y&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;numpy()])
    &lt;span style=&#34;color:#66d9ef&#34;&gt;print&lt;/span&gt;(y&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;numpy())
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;pre&gt;&lt;code&gt;[&#39;태조&#39; &#39;이성계&#39; &#39;선대의&#39; &#39;가계&#39; &#39;목조&#39; &#39;이안사가&#39; &#39;전주에서&#39; &#39;삼척&#39; &#39;의주를&#39; &#39;거쳐&#39; &#39;알동에&#39; &#39;정착하다&#39;
 &#39;\n&#39; &#39;태조&#39; &#39;강헌&#39; &#39;지인&#39; &#39;계운&#39; &#39;성문&#39; &#39;신무&#39; &#39;대왕&#39; &#39;의&#39; &#39;성은&#39; &#39;이씨&#39; &#39;요&#39; &#39;,&#39;]
[299305 229634 161443  17430 111029 230292 251081 155087 225462  29027
 190295 256129      0 299305  25624 273553  36147 163996 180466  84413
 224182 164549 230248 210912      2]
휘
330313
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;위 코드의 목적은 기존 &lt;code&gt;Dataset&lt;/code&gt;으로 새로운 &lt;code&gt;Dataset&lt;/code&gt;으로 만드는 과정입니다.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;26개의 단어가 각각 입력과 정답으로 묶어서([25단어], 1단어) 형태의 데이터를 반환하게 만드는 &lt;code&gt;Dataset&lt;/code&gt;을 만드는 것입니다.&lt;/li&gt;
&lt;li&gt;먼저, 26개의 단어를 25단어, 1단어로 잘라주는 함수를 &lt;code&gt;split_input_target(chunk)&lt;/code&gt;로 정의한 다음 이 함수를 기존 &lt;code&gt;sentence_dataset&lt;/code&gt;에 &lt;code&gt;map()&lt;/code&gt;함수(참조: 텐서플로 &lt;a href=&#34;http://bit.ly/2GZTyW1&#34;&gt;map 함수&lt;/a&gt;)를 이용해 새로운 데이터세트인 &lt;code&gt;train_dataset&lt;/code&gt;를 만듭니다.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;4-shuffle-batch-설정&#34;&gt;(4) Shuffle, Batch 설정&lt;/h3&gt;
&lt;p&gt;&lt;code&gt;Dataset&lt;/code&gt;의 데이터를 섞고, &lt;code&gt;batch_size&lt;/code&gt;를 다시 설정합니다.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;BATCH_SIZE &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;512&lt;/span&gt;
steps_per_epoch &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; examples_per_epoch &lt;span style=&#34;color:#f92672&#34;&gt;//&lt;/span&gt; BATCH_SIZE
BUFFER_SIZE &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;10000&lt;/span&gt;

train_dataset &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; train_dataset&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;shuffle(BUFFER_SIZE)&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;batch(BATCH_SIZE, drop_remainder&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;True)
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;주의: 이 부분은 교재와 조금 상이합니다! 따라서, 위 코드로 구현 바랍니다.&lt;/p&gt;
&lt;p&gt;빠른 학습을 위해 한번에 512개의 데이터를 학습하게 하고, 데이터를 섞을 때의 &lt;code&gt;BUFFER_SIZE&lt;/code&gt;는 10,000으로 설정합니다. &lt;code&gt;tf.data&lt;/code&gt;는 이론적으로 무한한 데이터에 대해 대응 가능하기 때문에 한번에 모든 데이터를 섞지 않습니다. 따라서, 버퍼에 일정한 양의 데이터를 올려놓고 섞는데, 그 사이즈를 &lt;code&gt;10,000&lt;/code&gt;으로 설정한 것입니다.&lt;/p&gt;
&lt;h3 id=&#34;5-생성-모델-정의&#34;&gt;(5) 생성 모델 정의&lt;/h3&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;total_words &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; len(vocab)
model &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; tf&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;keras&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;Sequential([
    tf&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;keras&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;layers&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;Embedding(total_words, &lt;span style=&#34;color:#ae81ff&#34;&gt;100&lt;/span&gt;, input_length&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;seq_length),
    tf&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;keras&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;layers&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;LSTM(units&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;100&lt;/span&gt;, return_sequences&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;True),
    tf&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;keras&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;layers&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;Dropout(&lt;span style=&#34;color:#ae81ff&#34;&gt;0.2&lt;/span&gt;),
    tf&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;keras&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;layers&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;LSTM(units&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;100&lt;/span&gt;),
    tf&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;keras&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;layers&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;Dense(total_words, activation&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;softmax&amp;#39;&lt;/span&gt;)
])

model&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;compile(optimizer&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;adam&amp;#39;&lt;/span&gt;, loss&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;sparse_categorical_crossentropy&amp;#39;&lt;/span&gt;, metrics&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;[&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;accuracy&amp;#39;&lt;/span&gt;])
model&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;summary()
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;pre&gt;&lt;code&gt;Model: &amp;quot;sequential&amp;quot;
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
embedding (Embedding)        (None, 25, 100)           33264000  
_________________________________________________________________
lstm (LSTM)                  (None, 25, 100)           80400     
_________________________________________________________________
dropout (Dropout)            (None, 25, 100)           0         
_________________________________________________________________
lstm_1 (LSTM)                (None, 100)               80400     
_________________________________________________________________
dense (Dense)                (None, 332640)            33596640  
=================================================================
Total params: 67,021,440
Trainable params: 67,021,440
Non-trainable params: 0
_________________________________________________________________
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;위 소스코드를 통해서 얻고자 하는 결과물은 주어진 입력에 대해 332,640개의 단어 중 어떤 단어를 선택해야 하는지 고르게 하는 기능을 하게 만듭니다.&lt;/p&gt;
&lt;p&gt;이제 학습을 시킵니다. 그런데, 이번에 학습시키는 방법은 기존과 조금 다르니 주의깊게 소스코드를 확인하시기를 바랍니다.&lt;/p&gt;
&lt;p&gt;또한, 학습시간이 GPU로 할당해도 매우 길기 때문에, 모형 학습 시간 계획을 잘 세우기를 바랍니다. (참고: 안 에포크당 10분 정도 소요됨)&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span style=&#34;color:#f92672&#34;&gt;from&lt;/span&gt; tensorflow.keras.preprocessing.sequence &lt;span style=&#34;color:#f92672&#34;&gt;import&lt;/span&gt; pad_sequences

&lt;span style=&#34;color:#66d9ef&#34;&gt;def&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;testmodel&lt;/span&gt;(epoch, logs):
    &lt;span style=&#34;color:#66d9ef&#34;&gt;if&lt;/span&gt; epoch &lt;span style=&#34;color:#f92672&#34;&gt;%&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;5&lt;/span&gt; &lt;span style=&#34;color:#f92672&#34;&gt;!=&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt; &lt;span style=&#34;color:#f92672&#34;&gt;and&lt;/span&gt; epoch &lt;span style=&#34;color:#f92672&#34;&gt;!=&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;49&lt;/span&gt;:
        &lt;span style=&#34;color:#66d9ef&#34;&gt;return&lt;/span&gt;
    test_sentence &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; train_text[&lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;]

    next_words &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;100&lt;/span&gt;
    &lt;span style=&#34;color:#66d9ef&#34;&gt;for&lt;/span&gt; _ &lt;span style=&#34;color:#f92672&#34;&gt;in&lt;/span&gt; range(next_words):
        test_text_X &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; test_sentence&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;split(&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39; &amp;#39;&lt;/span&gt;)[&lt;span style=&#34;color:#f92672&#34;&gt;-&lt;/span&gt;seq_length:]
        test_text_X &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; np&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;array([word2idx[c] &lt;span style=&#34;color:#66d9ef&#34;&gt;if&lt;/span&gt; c &lt;span style=&#34;color:#f92672&#34;&gt;in&lt;/span&gt; word2idx &lt;span style=&#34;color:#66d9ef&#34;&gt;else&lt;/span&gt; word2idx[&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;UNK&amp;#39;&lt;/span&gt;] &lt;span style=&#34;color:#66d9ef&#34;&gt;for&lt;/span&gt; c &lt;span style=&#34;color:#f92672&#34;&gt;in&lt;/span&gt; test_text_X])
        test_text_X &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; pad_sequences([test_text_X], maxlen&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;seq_length, padding&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;pre&amp;#39;&lt;/span&gt;, value&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;word2idx[&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;UNK&amp;#39;&lt;/span&gt;])

        output_idx &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; model&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;predict_classes(test_text_X)
        test_sentence &lt;span style=&#34;color:#f92672&#34;&gt;+=&lt;/span&gt; &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39; &amp;#39;&lt;/span&gt; &lt;span style=&#34;color:#f92672&#34;&gt;+&lt;/span&gt; idx2word[output_idx[&lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;]]
    
    &lt;span style=&#34;color:#66d9ef&#34;&gt;print&lt;/span&gt;()
    &lt;span style=&#34;color:#66d9ef&#34;&gt;print&lt;/span&gt;(test_sentence)
    &lt;span style=&#34;color:#66d9ef&#34;&gt;print&lt;/span&gt;()

testmodelcb &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; tf&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;keras&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;callbacks&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;LambdaCallback(on_epoch_end&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;testmodel)
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;history &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; model&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;fit(train_dataset&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;repeat(), epochs&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;10&lt;/span&gt;, steps_per_epoch&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;steps_per_epoch, callbacks&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;[testmodelcb], verbose&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;2&lt;/span&gt;)
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;Epoch 1/50&lt;/p&gt;
&lt;p&gt;태조 이성계 선대의 가계 목조 이안사가 전주에서 삼척 의주를 거쳐 알동에 정착하다  , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , ,&lt;/p&gt;
&lt;p&gt;533/533 - 255s - loss: 9.3702 - accuracy: 0.0723
Epoch 2/50
533/533 - 249s - loss: 8.3646 - accuracy: 0.0741
Epoch 3/50
533/533 - 249s - loss: 8.0768 - accuracy: 0.0816
Epoch 4/50
533/533 - 249s - loss: 7.8147 - accuracy: 0.0911
Epoch 5/50
533/533 - 249s - loss: 7.5722 - accuracy: 0.1025
Epoch 6/50&lt;/p&gt;
&lt;p&gt;태조 이성계 선대의 가계 목조 이안사가 전주에서 삼척 의주를 거쳐 알동에 정착하다  , , , , , , 그 , , 그 것은 , 그 것은 , 그 것은 , 그 것은 , 그 것은 , 그 것은 , 그 것은 , 그 것은 , 그 것은 , 그 것은 , 그 것은 , 그 것은 , 그 것은 , 그 것은 , 그 것은 , 그 것은 , 그 것은 , 그 것은 , 그 것은 , 그 것은 , 그 것은 , 그 것은 , 그 것은 , 그 것은 , 그 것은 , 그 것은 , 그 것은 , 그 것은 , 그 것은 , 그 것은 , 그&lt;/p&gt;
&lt;p&gt;533/533 - 250s - loss: 7.3301 - accuracy: 0.1165
Epoch 7/50
533/533 - 249s - loss: 7.0574 - accuracy: 0.1305
Epoch 8/50
533/533 - 249s - loss: 6.7848 - accuracy: 0.1433
Epoch 9/50
533/533 - 249s - loss: 6.5153 - accuracy: 0.1555
Epoch 10/50
533/533 - 249s - loss: 6.2694 - accuracy: 0.1679
Epoch 11/50&lt;/p&gt;
&lt;p&gt;태조 이성계 선대의 가계 목조 이안사가 전주에서 삼척 의주를 거쳐 알동에 정착하다  등 것입니다
하니 , 임금이 말하기를 ,
상참을 받고 , 임금이 아뢰기를 ,
이 받고 그 일을 다 다 다 가지고 주고 , 그 거느리고 다 화살을 내어 주고 , 이는 아뢰기를 ,
함길도 의 서쪽에 서향하여 없는 것입니다 또 아뢰기를 ,
이 받고 반드시 보내어 가서 가지고 가서 가서 주고 , 그 거느리고 의하여 서향하여 있으니 , 그 받고 , 아뢰기를 ,
그 일을 가지고 서향하여 서게 하고 , 그 받고 몸을 아뢰기를 ,
예조에서 아뢰기를 ,
함길도 의 서쪽에 인도하여 가지고 주고 , 아뢰기를 ,
평안도 의 서쪽에 서향하여&lt;/p&gt;
&lt;p&gt;533/533 - 250s - loss: 6.0086 - accuracy: 0.1822
Epoch 12/50
533/533 - 249s - loss: 5.7584 - accuracy: 0.1960
Epoch 13/50
533/533 - 248s - loss: 5.5062 - accuracy: 0.2102
Epoch 14/50
533/533 - 249s - loss: 5.2491 - accuracy: 0.2261
Epoch 15/50
533/533 - 249s - loss: 4.9958 - accuracy: 0.2423
Epoch 16/50&lt;/p&gt;
&lt;p&gt;태조 이성계 선대의 가계 목조 이안사가 전주에서 삼척 의주를 거쳐 알동에 정착하다  것이니 , 그 뜻을 다 알지 것이 없습니다 지금 내가 비록 여러 것은 모두 다 일을 다 일을 다 일을 다 말을 말을 다 알고 , 지금 만일 다시 가서 가서 아뢰게 하고 , 드디어 비가 보내어 가서 가서 토물 을 바쳤다 조회를 받았다 을축년 관찰사에게 이르기 살기가 , 의정부에서 아뢰기를 ,
맹규의 관찰사에게 유시하기를 ,
맹규의 벼슬만 아니오라 , 왕비가 관찰사에게 유시하기를 ,
중추원 관찰사에게 유시하기를 ,
중추원 부사 권맹경이
하였다 전 도절제사 부사 로 하여금 상언 하기를 ,
맹규의 관찰사에게 유시하기를 ,
맹규의 죄수 벌할 하게 ,
세자가 아뢰기를 ,&lt;/p&gt;
&lt;p&gt;533/533 - 249s - loss: 4.7473 - accuracy: 0.2599
Epoch 17/50
533/533 - 249s - loss: 4.5038 - accuracy: 0.2782
Epoch 18/50
533/533 - 249s - loss: 4.2670 - accuracy: 0.2984
Epoch 19/50
533/533 - 249s - loss: 4.0450 - accuracy: 0.3195
Epoch 20/50
533/533 - 249s - loss: 3.8258 - accuracy: 0.3440
Epoch 21/50&lt;/p&gt;
&lt;p&gt;태조 이성계 선대의 가계 목조 이안사가 전주에서 삼척 의주를 거쳐 알동에 정착하다  것이다 고 하고 , 그 말을 있고 , 모두 같이 같이 같이 같이 한다 또한 있고 한 것은 더욱 말을 있는 것은 모두 이를 살피게 하라
하였다 일본국 정난 공신 에게 유시 하기를 ,
일본국 병조 에서 상서 하여 각각 더불어 제수 로 하여금 함께 의논하여 가서 아뢰니 , 일본국 명나라 정문 에 의거하여 아뢰니 , 임금이 말하기를 ,
일본국 상송포 새긴들 다만 정난 신숙주 에게 유시 하기를 ,
일본국 병조 에서 전지 하기를 ,
이 사람을 보내어 와서 와서 가서 와서 이르기를 ,
경 은 하직하니 , 그 죄는 와서 와서 의논하게 하였다&lt;/p&gt;
&lt;p&gt;533/533 - 250s - loss: 3.6157 - accuracy: 0.3703
Epoch 22/50
533/533 - 249s - loss: 3.4163 - accuracy: 0.3981
Epoch 23/50
533/533 - 249s - loss: 3.2208 - accuracy: 0.4284
Epoch 24/50
533/533 - 249s - loss: 3.0313 - accuracy: 0.4601
Epoch 25/50
533/533 - 249s - loss: 2.8555 - accuracy: 0.4897
Epoch 26/50&lt;/p&gt;
&lt;p&gt;태조 이성계 선대의 가계 목조 이안사가 전주에서 삼척 의주를 거쳐 알동에 정착하다  것인데 , 만약 혹은 좋은 자는 백성을 주고 , 혹은 폐단이 있어 있어 오래 자가 나누어 주고 , 반드시 말을 일을 가지고 나누어 보내어 가게 하고 , 만일 병이 사실을 염려하여 , 드디어 말하기를 ,
함길도 도절제사 의 죄는 돌아와서 보내지 않은 것을 옮겨 주어 아뢰게 하고 , 만일 자기 베어 징계하소서
예조 에서 공조 판서 겸 형조 참판 제학 이조 등이 불러 불러서 조용히 전하다
경상도 상호군 박경무 으로 종이 대부 로 삼았다 함길도 도절제사 에서 와서 와서 내려 죄를 가서 아뢰다
사헌부에서 아뢰기를 ,
함길도 나라 일로 감사 의 개를 바치었다 햇무리가 쏘았다&lt;/p&gt;
&lt;p&gt;533/533 - 250s - loss: 2.6946 - accuracy: 0.5192
Epoch 27/50
533/533 - 248s - loss: 2.5327 - accuracy: 0.5480
Epoch 28/50
533/533 - 249s - loss: 2.3733 - accuracy: 0.5766
Epoch 29/50
533/533 - 249s - loss: 2.2233 - accuracy: 0.6036
Epoch 30/50
533/533 - 249s - loss: 2.0778 - accuracy: 0.6309
Epoch 31/50&lt;/p&gt;
&lt;p&gt;태조 이성계 선대의 가계 목조 이안사가 전주에서 삼척 의주를 거쳐 알동에 정착하다  되고 , 청컨대 본조 수는 없는데 , 무릇 여러 일은 이미 심히 알지 자는 서로 알 것이다 또한 이미 여러 사람이 심히 말을 사람을 얻게 합니다 이에 갖추어 아뢰니 , 드디어 세자에게 거둥하여 사실을 묻게 하였으나 , 임금이 말하기를 ,
이 사람을 거느리고 가서 와서 이르기를 ,
이 사람을 보내 토산물을 바쳤다 나도 아뢴 않겠는가
하였다 임금이 말하기를 ,
전하께서 전지 에서 함께 사사로이 듣고 아뢰게 하고 , 만일 잘못 묻지는 를 올려 국문 하여 계문 하여 이러하였다
대간 으로 사제 를 하사하다
대간 에 내리니 , 전하께서 다시 와서 가서 잘 알지 것은&lt;/p&gt;
&lt;p&gt;533/533 - 250s - loss: 1.9361 - accuracy: 0.6555
Epoch 32/50
533/533 - 249s - loss: 1.8070 - accuracy: 0.6788
Epoch 33/50
533/533 - 249s - loss: 1.6785 - accuracy: 0.7029
Epoch 34/50
533/533 - 249s - loss: 1.5642 - accuracy: 0.7236
Epoch 35/50
533/533 - 249s - loss: 1.4530 - accuracy: 0.7431
Epoch 36/50&lt;/p&gt;
&lt;p&gt;태조 이성계 선대의 가계 목조 이안사가 전주에서 삼척 의주를 거쳐 알동에 정착하다  된다 말하다 , 사의가 음악은 옳을 수는 없으니 , 본조 가 같이 같이 같이 있고 북쪽을 안 한 뒤에 모두 다 자리로 나아간다
창한다 마치면 , 통찬이 윤대를 행하고 , 신의 하나는 인도하여 내려와 제자리로 돌아간다 집례가 사배하라 찬하여 , 배 이하가 모두 꿇어앉는다 무릇 다음에 정한 것도 돌아간다 찬자가 사배하라 찬하면 , 감찰 는 단 가 자리에 나아간다 찬인이 감찰과 여러 때의 의식과 같이 한다 집례가 그 곡하라 하면 , 다음에 꿇어앉는다 여러 사람이 각기 각기 각기 자리에 나아간다 알자가 술잔을 씻고 올라가 인도하여 자리에 나간다 집례가 사배하라 찬하면 , 흥 , 흥 , 흥 , 흥 ,&lt;/p&gt;
&lt;p&gt;533/533 - 250s - loss: 1.3488 - accuracy: 0.7627
Epoch 37/50
533/533 - 249s - loss: 1.2497 - accuracy: 0.7815
Epoch 38/50
533/533 - 248s - loss: 1.1613 - accuracy: 0.7968
Epoch 39/50
533/533 - 249s - loss: 1.0768 - accuracy: 0.8122
Epoch 40/50
533/533 - 249s - loss: 0.9966 - accuracy: 0.8267
Epoch 41/50&lt;/p&gt;
&lt;p&gt;태조 이성계 선대의 가계 목조 이안사가 전주에서 삼척 의주를 거쳐 알동에 정착하다  되고 , 청컨대 그전 무지한 곳이 없게 되니 , 물러가 가까이 가고 , 혹은 도로 사람이 있을 것이다 진실로 이미 가 서로 폐단이 심히 남겨서 대신의 무리들이 급히 주게 하고 , 그 온 1백 콩 1백 명을 조사하여 급히 오늘에 가서 크게 시종 중에 살펴보게 한다든가 허락하지 아니하였다 신이 그 죄가 와서 와서 가서 청하여 임금의 두게 하소서
하니 , 의금부에서 아뢰기를 ,
평안도 토관 죄수 벌송 애오라지 염법 을 내리어 의논하게 하므로 , 무릇 부득이하여 보면 , 무릇 성상께서 생각해 나아가 바로 전의 를 인도하여 주어 서향하게 하고 , 다음은 나라가 정한 지경에 너무 가지고 반드시 모두&lt;/p&gt;
&lt;p&gt;533/533 - 250s - loss: 0.9232 - accuracy: 0.8399
Epoch 42/50
533/533 - 249s - loss: 0.8571 - accuracy: 0.8523
Epoch 43/50
533/533 - 249s - loss: 0.7950 - accuracy: 0.8637
Epoch 44/50
533/533 - 249s - loss: 0.7335 - accuracy: 0.8753
Epoch 45/50
533/533 - 249s - loss: 0.6764 - accuracy: 0.8857
Epoch 46/50&lt;/p&gt;
&lt;p&gt;태조 이성계 선대의 가계 목조 이안사가 전주에서 삼척 의주를 거쳐 알동에 정착하다  되고 , 청컨대 혹은 감한 많으며 , 겨우 휘 를 올리고 종자 가 서로 알 것이다 또한 감히 뒤에 모두 서로 폐단을 알 것이다 그러나 경 등과 천지 인 약정하기를 사직 과 더불어 글을 모여서 가서 본 말은 서울과 70이 민간에 시행할 있지 않는 것이니 , 이에 어질고 떨어져 구 에 참여하지 않을 것이니 , 마침내 그를 오지 아니하고 , 대개 속전 에 붙여 들지 아니할 것이니 , 그것을 그르다 힘쓰고 수양 3천 떼가 박천 2개는 모순 천하가 의논했다고 되었다 청백 양씨 중후 처소 740 강호덕 615 박사란 김선거 연변에 이등 씩 음식도 어리고 2명씩 뵙고 일본의 주었사오니 하옵고 침체&lt;/p&gt;
&lt;p&gt;533/533 - 250s - loss: 0.6241 - accuracy: 0.8957
Epoch 47/50
533/533 - 249s - loss: 0.5790 - accuracy: 0.9038
Epoch 48/50
533/533 - 249s - loss: 0.5355 - accuracy: 0.9114
Epoch 49/50
533/533 - 249s - loss: 0.4964 - accuracy: 0.9183
Epoch 50/50&lt;/p&gt;
&lt;p&gt;태조 이성계 선대의 가계 목조 이안사가 전주에서 삼척 의주를 거쳐 알동에 정착하다  들을 것과 , 본조 가 없으면 서향하게 것입니다 만약 먼저 단 가 있고 , 한 법이 같이 같이 같이 되어 한 일이 있을 일이 없습니다 그러나 , 이제부터는 진휼사 에 술을 받들어 그 자리에 매우 옳지 를 나아가서 , 내가 그 아비를 가서 온 사람은 한번 가상히 부임 하여 이러한 때 아울러 국문 하여 외방 전의 일수 를 받들어 잡희 를 나아가서 나아가서 나가고 , 이어서 병법 과 악 을 외유 일어나고 , 의장 을 더하여 적당히 판위 에 올라 그치게 하되 , 앙제 에 나아가서 꿇어앉아 나아가서 선다 판통례가 , 다만 술 서문 궤 앞으로 나아가 부복하고 꿇어앉아 부복&lt;/p&gt;
&lt;p&gt;533/533 - 250s - loss: 0.4581 - accuracy: 0.9256&lt;/p&gt;
&lt;p&gt;먼저 모델을 학습시키면서 모델의 생성 결과물을 확인하기 위해 &lt;code&gt;testmodel&lt;/code&gt;이라는 이름으로 콜백 함수를 정의합니다.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;조금 더 콜백 함수에 대해서 정의를 내리면, 먼저 임의의 문장을 입력한 다음, &lt;code&gt;seq_length&lt;/code&gt;만큼의 단어 25개를 선택합니다.&lt;/li&gt;
&lt;/ul&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;test_test_X &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; test_sentence&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;split(&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39; &amp;#39;&lt;/span&gt;)[&lt;span style=&#34;color:#f92672&#34;&gt;-&lt;/span&gt;seg_length:]
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;ul&gt;
&lt;li&gt;그 다음에는 문장의 단어를 인덱스 토큰으로 바꿉니다. 이 때 사전에 등록되어 있지 않은 단어의 경우에는 &lt;code&gt;UNK&lt;/code&gt; 토큰 값으로 바꿉니다.&lt;/li&gt;
&lt;/ul&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;test_text_X &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; pad_sequences([test_text_X], maxlen &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; seq_length, padding&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;pre&amp;#39;&lt;/span&gt;, value&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;word2idx[&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;UNK&amp;#39;&lt;/span&gt;])
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;ul&gt;
&lt;li&gt;&lt;code&gt;pad_sequences()&lt;/code&gt;로 문장의 앞쪽이 빈 자리가 있을 경우 25단어가 채워지도록 패딩을 넣습니다. 이 때 패딩 갑인 &lt;code&gt;value&lt;/code&gt;인수에는 앞에서 지정했던 &lt;code&gt;UNK&lt;/code&gt; 토큰의 값인 &lt;code&gt;word2idx[&#39;UNK&#39;]&lt;/code&gt;를 사용합니다.&lt;/li&gt;
&lt;/ul&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;output_idx &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; model&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;predict_classes(test_text_X)
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;ul&gt;
&lt;li&gt;&lt;code&gt;model.predict()&lt;/code&gt;는 &lt;code&gt;Dense&lt;/code&gt;레이어의 332,640개의 값을 반환하기 때문에 출력을 간결하게 하기 위해 &lt;code&gt;model.predict_classes()&lt;/code&gt;함수를 사용합니다. 이 함수는 출력 중에서 가장 값이 큰 인덱스를 반환합니다.&lt;/li&gt;
&lt;/ul&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;test_sentence &lt;span style=&#34;color:#f92672&#34;&gt;+=&lt;/span&gt; &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39; &amp;#39;&lt;/span&gt; &lt;span style=&#34;color:#f92672&#34;&gt;+&lt;/span&gt; idx2word[output_idx[&lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;]]
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;출력 단어는 test_sentence의 끝 부분에 저장되어 다음 스텝의 입력에 활용됩니다.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;이렇게 정의된 콜백 함수는 &lt;code&gt;testmodelcb&lt;/code&gt;라는 이름으로 저장되어 &lt;code&gt;tf.keras&lt;/code&gt;의 &lt;code&gt;model.fit()&lt;/code&gt;의 &lt;code&gt;callbacks&lt;/code&gt;인수에 포함됩니다.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;마지막으로 모형을 학습시킵니다.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;history &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; model&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;fit(train_dataset&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;repeat(), epochs&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;10&lt;/span&gt;, steps_per_epoch&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;steps_per_epoch, callbacks&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;[testmodelcb], verbose&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;2&lt;/span&gt;)
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;ul&gt;
&lt;li&gt;학습을 시킬 때도 위에서 정의한 &lt;code&gt;Dataset&lt;/code&gt;을 활용합니다. 그런데 주목해야 할 점은 &lt;code&gt;Dataset&lt;/code&gt;에서 데이터를 끊임없이 반환하도록 &lt;code&gt;repeat()&lt;/code&gt;함수를 사용합나다.&lt;/li&gt;
&lt;li&gt;넘파이 &lt;code&gt;array&lt;/code&gt; 형태의 데이터를 사용할 때는 에포크마다 데이터를 한번씩 순회시켰지만, &lt;code&gt;Dataset&lt;/code&gt;을 사용하면 데이터의 시작과 끝을 알 수 없기 때문에 에포크에 데이터를 얼마나 학습시키질를 &lt;code&gt;steps_per_epoch&lt;/code&gt; 인수로 지정합니다.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;학습 결과를 살펴보면 처음에는 의미없는 단어가 반복되지만, 학습을 진행하면서 문맥이 연결되는 것을 확인할 수 있습니다.&lt;/p&gt;
&lt;h3 id=&#34;6-임의의-문장을-사용한-단어-생성-확인&#34;&gt;(6) 임의의 문장을 사용한 단어 생성 확인&lt;/h3&gt;
&lt;p&gt;이제 임의의 문장을 넣어 학습이 잘 되는지 난중일기의 한 구절을 입력해봅니다.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span style=&#34;color:#f92672&#34;&gt;from&lt;/span&gt; tensorflow.keras.preprocessing.sequence &lt;span style=&#34;color:#f92672&#34;&gt;import&lt;/span&gt; pad_sequences
test_sentence &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;동헌에 나가 공무를 본 후 활 십오 순을 쏘았다&amp;#39;&lt;/span&gt;

next_words &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;100&lt;/span&gt;
&lt;span style=&#34;color:#66d9ef&#34;&gt;for&lt;/span&gt; _ &lt;span style=&#34;color:#f92672&#34;&gt;in&lt;/span&gt; range(next_words):
    test_text_X &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; test_sentence&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;split(&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39; &amp;#39;&lt;/span&gt;)[&lt;span style=&#34;color:#f92672&#34;&gt;-&lt;/span&gt;seq_length:]
    test_text_X &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; np&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;array([word2idx[c] &lt;span style=&#34;color:#66d9ef&#34;&gt;if&lt;/span&gt; c &lt;span style=&#34;color:#f92672&#34;&gt;in&lt;/span&gt; word2idx &lt;span style=&#34;color:#66d9ef&#34;&gt;else&lt;/span&gt; word2idx[&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;UNK&amp;#39;&lt;/span&gt;] &lt;span style=&#34;color:#66d9ef&#34;&gt;for&lt;/span&gt; c &lt;span style=&#34;color:#f92672&#34;&gt;in&lt;/span&gt; test_text_X])
    test_text_X &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; pad_sequences([test_text_X], maxlen&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;seq_length, padding&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;pre&amp;#39;&lt;/span&gt;, value&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;word2idx[&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;UNK&amp;#39;&lt;/span&gt;])
    
    output_idx &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; model&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;predict_classes(test_text_X)
    test_sentence &lt;span style=&#34;color:#f92672&#34;&gt;+=&lt;/span&gt; &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39; &amp;#39;&lt;/span&gt; &lt;span style=&#34;color:#f92672&#34;&gt;+&lt;/span&gt; idx2word[output_idx[&lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;]]

&lt;span style=&#34;color:#66d9ef&#34;&gt;print&lt;/span&gt;(test_sentence)
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;동헌에 나가 공무를 본 후 활 십오 순을 쏘았다 그곳의 사로잡힌 대리하는 호군직 2개는 놓기도 토의 으로써 차임 되고 , 저들은 일시에 굳게 것으로서 주고 , 더욱 높은 바가 많아서 정위 라 같다 하는데 하였습니다 빈자 는 부의 에 도와서 알아야 하여 주소서
하니 , 형조 에서 교지 를 하사하고 충청도 도절제사 에서 다시 와서 와서 내려 쓸 하고 , 만일 잘 가는 것이 없었다 상수의 대한 한 한 뒤에 모두 능히 서로 알지 것은 감히 모두 모두 모두 사람을 죄를 죄를 사람을 죄를 온 한 사람을 삼가서 어질고 그믐날에 한다는 것이니 , 청컨대 이러한 정성을 알아서 난 를 정하여 급히 원하는 곳에 두고 특별히 징계하여 인도해 특별히&lt;/p&gt;
&lt;p&gt;전체적인 문장의 의미가 통하는 건 아닙니다만, 부분 부분에서는 자연스럽게 연결되는 단어들이 보이는 것을 확인할 수 있습니다.&lt;/p&gt;
&lt;h2 id=&#34;iii-연습-파일&#34;&gt;III. 연습 파일&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://colab.research.google.com/github/chloevan/deeplearningAI/blob/master/tensorflow2.0/ch7_4_naturalLanguageGeneration(1).ipynb&#34;&gt;구글 Colab에서 직접 연습해보자&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;iv-reference&#34;&gt;IV. Reference&lt;/h2&gt;
&lt;p&gt;김환희. (2020). 시작하세요! 텐서플로 2.0 프로그래밍: 기초 이론부터 실전 예제까지 한번에 끝내는 머신러닝, 딥러닝 핵심 가이드. 서울: 위키북스.&lt;/p&gt;
&lt;p&gt;Karpathy, A. (2015). The Unreasonable Effectiveness of Recurrent Neural Networks. Retrieved April 26, 2020, from &lt;a href=&#34;http://karpathy.github.io/2015/05/21/rnn-effectiveness/&#34;&gt;http://karpathy.github.io/2015/05/21/rnn-effectiveness/&lt;/a&gt;&lt;/p&gt;
&lt;section class=&#34;footnotes&#34; role=&#34;doc-endnotes&#34;&gt;
&lt;hr&gt;
&lt;ol&gt;
&lt;li id=&#34;fn:1&#34; role=&#34;doc-endnote&#34;&gt;
&lt;p&gt;교재에서는 전체 데이터(400MB) 중, 62MB만 잘라낸 파일을 사용했습니다. &lt;a href=&#34;https://www.data.go.kr/&#34;&gt;공공데이터포털&lt;/a&gt;에서 조선왕조실록 데이터를 다운로드 받을 수 있습니다. &lt;a href=&#34;#fnref:1&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;/section&gt;
</description>
    </item>
    
  </channel>
</rss>
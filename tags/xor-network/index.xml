<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>XOR Network on Data Science | ChloEvan</title>
    <link>https://chloevan.github.io/tags/xor-network/</link>
    <description>Recent content in XOR Network on Data Science | ChloEvan</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Mon, 13 Apr 2020 21:40:30 +0900</lastBuildDate>
    
        <atom:link href="https://chloevan.github.io/tags/xor-network/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>Tensorflow 2.0 Tutorial ch3.3.5 - 세번째 신경망 네트워크 - XOR</title>
      <link>https://chloevan.github.io/python/tensorflow2.0/ch3_3_5_network_xor/</link>
      <pubDate>Mon, 13 Apr 2020 21:40:30 +0900</pubDate>
      
      <guid>https://chloevan.github.io/python/tensorflow2.0/ch3_3_5_network_xor/</guid>
      <description>&lt;h2 id=&#34;공지&#34;&gt;공지&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;본 Tutorial은 교재 &lt;code&gt;시작하세요 텐서플로 2.0 프로그래밍&lt;/code&gt;의 강사에게 국비교육 강의를 듣는 사람들에게 자료 제공을 목적으로 제작하였습니다.&lt;/li&gt;
&lt;li&gt;강사의 주관적인 판단으로 압축해서 자료를 정리하였기 때문에, 자세하게 공부를 하고 싶은 반드시 교재를 구매하실 것을 권해드립니다.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;img src=&#34;https://chloevan.github.io/img/tensorflow2.0/book.jpg&#34; alt=&#34;&#34;&gt;&lt;!-- raw HTML omitted --&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;본 교재 외에 강사가 추가한 내용에 대한 Reference를 확인하셔서, 추가적으로 학습하시는 것을 권유드립니다.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;tutorial&#34;&gt;Tutorial&lt;/h2&gt;
&lt;p&gt;이전 강의가 궁금하신 분들은 아래에서 선택하여 추가 학습 하시기를 바랍니다.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://chloevan.github.io/python/tensorflow2.0/googlecolab/&#34;&gt;Google Colab Tensorflow 2.0 Installation&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://chloevan.github.io/python/tensorflow2.0/ch3_3_1_random_signoid/&#34;&gt;Tensorflow 2.0 Tutorial ch3.3.1 - 난수 생성 및 시그모이드 함수&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://chloevan.github.io/python/tensorflow2.0/ch3_3_2_random_signoid_bias/&#34;&gt;Tensorflow 2.0 Tutorial ch3.3.2 - 난수 생성 및 시그모이드 함수 편향성&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://chloevan.github.io/python/tensorflow2.0/ch3_3_3_network_and/&#34;&gt;Tensorflow 2.0 Tutorial ch3.3.3 - 첫번째 신경망 네트워크: AND&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://chloevan.github.io/python/tensorflow2.0/ch3_3_4_network_or/&#34;&gt;Tensorflow 2.0 Tutorial ch3.3.4 - 두번째 신경망 네트워크: OR&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;i-xor-연산의-기본-개념&#34;&gt;I. XOR 연산의 기본 개념&lt;/h2&gt;
&lt;p&gt;&lt;code&gt;XOR&lt;/code&gt; 연산의 기본개념은 아래와 같습니다. 여기서 주의해야 할 점은, 홀수 개의 입력이 참일 때만 결과값이 참이라는 점입니다.&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;&lt;code&gt;입력1&lt;/code&gt;&lt;/th&gt;
&lt;th&gt;&lt;code&gt;입력2&lt;/code&gt;&lt;/th&gt;
&lt;th&gt;&lt;code&gt;AND 연산&lt;/code&gt;&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;참&lt;/td&gt;
&lt;td&gt;참&lt;/td&gt;
&lt;td&gt;거짓&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;참&lt;/td&gt;
&lt;td&gt;거짓&lt;/td&gt;
&lt;td&gt;참&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;거짓&lt;/td&gt;
&lt;td&gt;참&lt;/td&gt;
&lt;td&gt;참&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;거짓&lt;/td&gt;
&lt;td&gt;거짓&lt;/td&gt;
&lt;td&gt;거짓&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;파이썬에서는 참, 거짓을 나타내는 값은 &lt;code&gt;True&lt;/code&gt;, &lt;code&gt;False&lt;/code&gt;입니다. 그런데, 딥러닝의 주요 입력값은 정수(Integer)나 실수(float)입니다. 참과 거짓의 값을 출력하여 확인해봅니다.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;print&lt;/span&gt;(int(True))
&lt;span style=&#34;color:#66d9ef&#34;&gt;print&lt;/span&gt;(int(False))
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;pre&gt;&lt;code&gt;1
0
&lt;/code&gt;&lt;/pre&gt;
&lt;h2 id=&#34;ii-review--edit-for-xor&#34;&gt;II. Review &amp;amp; Edit For XOR&lt;/h2&gt;
&lt;p&gt;&lt;code&gt;XOR&lt;/code&gt; 연산의 네트워크를 구성하는 코드를 작성해봅니다. 이 때에도 동일하게 기존 코드에서 &lt;code&gt;y&lt;/code&gt;값만 수정해야 하니, 잘 참조하셔서 코드 작성하기를 바랍니다.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span style=&#34;color:#75715e&#34;&gt;# 본 예제&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;import&lt;/span&gt; tensorflow &lt;span style=&#34;color:#f92672&#34;&gt;as&lt;/span&gt; tf
&lt;span style=&#34;color:#f92672&#34;&gt;import&lt;/span&gt; numpy &lt;span style=&#34;color:#f92672&#34;&gt;as&lt;/span&gt; np

&lt;span style=&#34;color:#f92672&#34;&gt;import&lt;/span&gt; math
&lt;span style=&#34;color:#75715e&#34;&gt;# 시그모이드 함수 정의&lt;/span&gt;
&lt;span style=&#34;color:#66d9ef&#34;&gt;def&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;sigmoid&lt;/span&gt;(x): 
  &lt;span style=&#34;color:#66d9ef&#34;&gt;return&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt; &lt;span style=&#34;color:#f92672&#34;&gt;/&lt;/span&gt; (&lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt; &lt;span style=&#34;color:#f92672&#34;&gt;+&lt;/span&gt; math&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;exp(&lt;span style=&#34;color:#f92672&#34;&gt;-&lt;/span&gt;x))

x &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; np&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;array([[&lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;], [&lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;], [&lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;], [&lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;]])
y &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; np&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;array([[&lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;], [&lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;], [&lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;], [&lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;]])
w &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; tf&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;random&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;normal([&lt;span style=&#34;color:#ae81ff&#34;&gt;2&lt;/span&gt;], &lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;)
b &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; tf&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;random&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;normal([&lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;], &lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;)
b_x &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;

&lt;span style=&#34;color:#66d9ef&#34;&gt;for&lt;/span&gt; i &lt;span style=&#34;color:#f92672&#34;&gt;in&lt;/span&gt; range(&lt;span style=&#34;color:#ae81ff&#34;&gt;2000&lt;/span&gt;): 
  error_sum &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;

  &lt;span style=&#34;color:#66d9ef&#34;&gt;for&lt;/span&gt; j &lt;span style=&#34;color:#f92672&#34;&gt;in&lt;/span&gt; range(&lt;span style=&#34;color:#ae81ff&#34;&gt;4&lt;/span&gt;):
    output &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; sigmoid(np&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;sum(x[j] &lt;span style=&#34;color:#f92672&#34;&gt;*&lt;/span&gt; w) &lt;span style=&#34;color:#f92672&#34;&gt;+&lt;/span&gt; b_x &lt;span style=&#34;color:#f92672&#34;&gt;*&lt;/span&gt; b)
    error &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; y[j][&lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;] &lt;span style=&#34;color:#f92672&#34;&gt;-&lt;/span&gt; output
    w &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; w &lt;span style=&#34;color:#f92672&#34;&gt;+&lt;/span&gt; x[j] &lt;span style=&#34;color:#f92672&#34;&gt;*&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;0.1&lt;/span&gt; &lt;span style=&#34;color:#f92672&#34;&gt;*&lt;/span&gt; error
    b &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; b &lt;span style=&#34;color:#f92672&#34;&gt;+&lt;/span&gt; b_x &lt;span style=&#34;color:#f92672&#34;&gt;*&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;0.1&lt;/span&gt; &lt;span style=&#34;color:#f92672&#34;&gt;*&lt;/span&gt; error
    error_sum &lt;span style=&#34;color:#f92672&#34;&gt;+=&lt;/span&gt; error

  &lt;span style=&#34;color:#66d9ef&#34;&gt;if&lt;/span&gt; i &lt;span style=&#34;color:#f92672&#34;&gt;%&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;200&lt;/span&gt; &lt;span style=&#34;color:#f92672&#34;&gt;==&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;199&lt;/span&gt;:
    &lt;span style=&#34;color:#66d9ef&#34;&gt;print&lt;/span&gt;(i, error_sum)
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;pre&gt;&lt;code&gt;199 0.00035103711355610745
399 1.4262242213569643e-05
599 5.82624861134029e-07
799 1.8614212393686103e-09
999 1.8614210173240053e-09
1199 1.8614210173240053e-09
1399 1.8614210173240053e-09
1599 1.8614210173240053e-09
1799 1.8614210173240053e-09
1999 1.8614210173240053e-09
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;연산이 &lt;code&gt;999&lt;/code&gt; 이후 시점부터는 변하지 않는 것을 확인 할 수 있습니다.&lt;/p&gt;
&lt;h2 id=&#34;iii-xor-네트워크의-평가&#34;&gt;III. XOR 네트워크의 평가&lt;/h2&gt;
&lt;p&gt;일단, 네트워크 평가를 해봅니다.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;for&lt;/span&gt; i &lt;span style=&#34;color:#f92672&#34;&gt;in&lt;/span&gt; range(&lt;span style=&#34;color:#ae81ff&#34;&gt;4&lt;/span&gt;):
  &lt;span style=&#34;color:#66d9ef&#34;&gt;print&lt;/span&gt;(&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;X:&amp;#39;&lt;/span&gt;, x[i], &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;Y:&amp;#39;&lt;/span&gt;, y[i], &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;Output:&amp;#39;&lt;/span&gt;, sigmoid(np&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;sum(x[i]&lt;span style=&#34;color:#f92672&#34;&gt;*&lt;/span&gt;w)&lt;span style=&#34;color:#f92672&#34;&gt;+&lt;/span&gt;b))
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;pre&gt;&lt;code&gt;X: [1 1] Y: [0] Output: 0.5128176286712095
X: [1 0] Y: [1] Output: 0.5128176305326305
X: [0 1] Y: [1] Output: 0.4999999990686774
X: [0 0] Y: [0] Output: 0.5000000009313226
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;code&gt;Y&lt;/code&gt;와 &lt;code&gt;Output&lt;/code&gt; 사이에는 큰 차이가 있어 보이는데, &lt;code&gt;X&lt;/code&gt;가 변해도 &lt;code&gt;Output&lt;/code&gt;은 0.5 근처에서 머물고 있는데, 왜 그런결과가 나온걸까요?&lt;/p&gt;
&lt;h2 id=&#34;iv-xor-네트워크의-문제점&#34;&gt;IV. XOR 네트워크의 문제점&lt;/h2&gt;
&lt;p&gt;우선 &lt;code&gt;output = sigmoid(np.sum(x[j] * w) + b_x * b)&lt;/code&gt; 공식을 구성하는 &lt;code&gt;w&lt;/code&gt;와 &lt;code&gt;b&lt;/code&gt;를 출력해보면 다음과 같습니다.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;print&lt;/span&gt;(&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;w:&amp;#39;&lt;/span&gt;, w)
&lt;span style=&#34;color:#66d9ef&#34;&gt;print&lt;/span&gt;(&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;b:&amp;#39;&lt;/span&gt;, b)
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;pre&gt;&lt;code&gt;w: tf.Tensor([5.1281769e-02 3.7252903e-09], shape=(2,), dtype=float32)
b: tf.Tensor([-7.450581e-09], shape=(1,), dtype=float32)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;code&gt;w&lt;/code&gt;는 약 &lt;code&gt;0.0512&lt;/code&gt;, &lt;code&gt;-0.000000000745&lt;/code&gt;이고, &lt;code&gt;b&lt;/code&gt;는 &lt;code&gt;0.000000000372&lt;/code&gt; 입니다.&lt;/p&gt;
&lt;p&gt;조금더 구체적으로 XOR 네트워크의 중간값과 출력값이 어떻게 변하는지 확인하는 코드를 짜봅니다.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;for&lt;/span&gt; i &lt;span style=&#34;color:#f92672&#34;&gt;in&lt;/span&gt; range(&lt;span style=&#34;color:#ae81ff&#34;&gt;4&lt;/span&gt;):
  &lt;span style=&#34;color:#66d9ef&#34;&gt;print&lt;/span&gt;(&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;X:&amp;#39;&lt;/span&gt;, x[i], &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;Y:&amp;#39;&lt;/span&gt;, y[i], &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;Cal_mid:&amp;#39;&lt;/span&gt;, np&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;sum(x[i]&lt;span style=&#34;color:#f92672&#34;&gt;*&lt;/span&gt;w)&lt;span style=&#34;color:#f92672&#34;&gt;+&lt;/span&gt;b,&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;Output:&amp;#39;&lt;/span&gt;, sigmoid(np&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;sum(x[i]&lt;span style=&#34;color:#f92672&#34;&gt;*&lt;/span&gt;w)&lt;span style=&#34;color:#f92672&#34;&gt;+&lt;/span&gt;b))
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;pre&gt;&lt;code&gt;X: [1 1] Y: [0] Cal_mid: tf.Tensor([0.05128177], shape=(1,), dtype=float32) Output: 0.5128176323940516
X: [1 0] Y: [1] Cal_mid: tf.Tensor([0.05128176], shape=(1,), dtype=float32) Output: 0.5128176314633411
X: [0 1] Y: [1] Cal_mid: tf.Tensor([-3.7252903e-09], shape=(1,), dtype=float32) Output: 0.4999999990686774
X: [0 0] Y: [0] Cal_mid: tf.Tensor([-7.450581e-09], shape=(1,), dtype=float32) Output: 0.49999999813735485
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;첫번째 입력에 따라, &lt;code&gt;Cal_mid:&lt;/code&gt; 중간 계산값은 크게 달라지지만, &lt;code&gt;Output:&lt;/code&gt;은 큰 변동이 없는 것을 확인할 수 있습니다.&lt;/p&gt;
&lt;p&gt;&lt;code&gt;AND&lt;/code&gt; 네트워크와 비교해보면 그 차이는 더 명확해집니다.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span style=&#34;color:#75715e&#34;&gt;# 본 예제&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;import&lt;/span&gt; tensorflow &lt;span style=&#34;color:#f92672&#34;&gt;as&lt;/span&gt; tf
&lt;span style=&#34;color:#f92672&#34;&gt;import&lt;/span&gt; numpy &lt;span style=&#34;color:#f92672&#34;&gt;as&lt;/span&gt; np

&lt;span style=&#34;color:#f92672&#34;&gt;import&lt;/span&gt; math
&lt;span style=&#34;color:#75715e&#34;&gt;# 시그모이드 함수 정의&lt;/span&gt;
&lt;span style=&#34;color:#66d9ef&#34;&gt;def&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;sigmoid&lt;/span&gt;(x): 
  &lt;span style=&#34;color:#66d9ef&#34;&gt;return&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt; &lt;span style=&#34;color:#f92672&#34;&gt;/&lt;/span&gt; (&lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt; &lt;span style=&#34;color:#f92672&#34;&gt;+&lt;/span&gt; math&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;exp(&lt;span style=&#34;color:#f92672&#34;&gt;-&lt;/span&gt;x))

x &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; np&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;array([[&lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;], [&lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;], [&lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;], [&lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;]])
y &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; np&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;array([[&lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;], [&lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;], [&lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;], [&lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;]])
w &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; tf&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;random&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;normal([&lt;span style=&#34;color:#ae81ff&#34;&gt;2&lt;/span&gt;], &lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;)
b &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; tf&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;random&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;normal([&lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;], &lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;)
b_x &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;

&lt;span style=&#34;color:#66d9ef&#34;&gt;for&lt;/span&gt; i &lt;span style=&#34;color:#f92672&#34;&gt;in&lt;/span&gt; range(&lt;span style=&#34;color:#ae81ff&#34;&gt;2000&lt;/span&gt;): 
  error_sum &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;

  &lt;span style=&#34;color:#66d9ef&#34;&gt;for&lt;/span&gt; j &lt;span style=&#34;color:#f92672&#34;&gt;in&lt;/span&gt; range(&lt;span style=&#34;color:#ae81ff&#34;&gt;4&lt;/span&gt;): 
    output &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; sigmoid(np&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;sum(x[j] &lt;span style=&#34;color:#f92672&#34;&gt;*&lt;/span&gt; w) &lt;span style=&#34;color:#f92672&#34;&gt;+&lt;/span&gt; b_x &lt;span style=&#34;color:#f92672&#34;&gt;*&lt;/span&gt; b)
    error &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; y[j][&lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;] &lt;span style=&#34;color:#f92672&#34;&gt;-&lt;/span&gt; output
    w &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; w &lt;span style=&#34;color:#f92672&#34;&gt;+&lt;/span&gt; x[j] &lt;span style=&#34;color:#f92672&#34;&gt;*&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;0.1&lt;/span&gt; &lt;span style=&#34;color:#f92672&#34;&gt;*&lt;/span&gt; error
    b &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; b &lt;span style=&#34;color:#f92672&#34;&gt;+&lt;/span&gt; b_x &lt;span style=&#34;color:#f92672&#34;&gt;*&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;0.1&lt;/span&gt; &lt;span style=&#34;color:#f92672&#34;&gt;*&lt;/span&gt; error
    error_sum &lt;span style=&#34;color:#f92672&#34;&gt;+=&lt;/span&gt; error

  &lt;span style=&#34;color:#66d9ef&#34;&gt;if&lt;/span&gt; i &lt;span style=&#34;color:#f92672&#34;&gt;%&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;200&lt;/span&gt; &lt;span style=&#34;color:#f92672&#34;&gt;==&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;199&lt;/span&gt;:
    &lt;span style=&#34;color:#66d9ef&#34;&gt;print&lt;/span&gt;(i, error_sum)

&lt;span style=&#34;color:#66d9ef&#34;&gt;for&lt;/span&gt; i &lt;span style=&#34;color:#f92672&#34;&gt;in&lt;/span&gt; range(&lt;span style=&#34;color:#ae81ff&#34;&gt;4&lt;/span&gt;):
  &lt;span style=&#34;color:#66d9ef&#34;&gt;print&lt;/span&gt;(&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;X:&amp;#39;&lt;/span&gt;, x[i], &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;Y:&amp;#39;&lt;/span&gt;, y[i], &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;Cal_mid:&amp;#39;&lt;/span&gt;, np&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;sum(x[i]&lt;span style=&#34;color:#f92672&#34;&gt;*&lt;/span&gt;w)&lt;span style=&#34;color:#f92672&#34;&gt;+&lt;/span&gt;b,&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;Output:&amp;#39;&lt;/span&gt;, sigmoid(np&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;sum(x[i]&lt;span style=&#34;color:#f92672&#34;&gt;*&lt;/span&gt;w)&lt;span style=&#34;color:#f92672&#34;&gt;+&lt;/span&gt;b))
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;pre&gt;&lt;code&gt;199 -0.11459902898577794
399 -0.06719349972993488
599 -0.04743256822513228
799 -0.03654512498706137
999 -0.029666378761408263
1199 -0.024938480158672418
1399 -0.02149223338941213
1599 -0.01887404965969957
1799 -0.016816783475954222
1999 -0.0151601229142651
X: [1 1] Y: [1] Cal_mid: tf.Tensor([3.3052025], shape=(1,), dtype=float32) Output: 0.9646068559309787
X: [1 0] Y: [0] Cal_mid: tf.Tensor([-3.6603136], shape=(1,), dtype=float32) Output: 0.025079293237166324
X: [0 1] Y: [0] Cal_mid: tf.Tensor([-3.657158], shape=(1,), dtype=float32) Output: 0.02515656706949759
X: [0 0] Y: [0] Cal_mid: tf.Tensor([-10.622674], shape=(1,), dtype=float32) Output: 2.4356827795380005e-05
&lt;/code&gt;&lt;/pre&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;print&lt;/span&gt;(&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;w:&amp;#39;&lt;/span&gt;, w)
&lt;span style=&#34;color:#66d9ef&#34;&gt;print&lt;/span&gt;(&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;b:&amp;#39;&lt;/span&gt;, b)
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;pre&gt;&lt;code&gt;w: tf.Tensor([6.9623604 6.965516 ], shape=(2,), dtype=float32)
b: tf.Tensor([-10.622674], shape=(1,), dtype=float32)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;code&gt;AND&lt;/code&gt; 네트워크의 가중치가 하려는 작업은 XOR 네트워크에 비해 분명합니다. 편향값이 &lt;code&gt;-10.622&lt;/code&gt; 큰 음수인데, 두 가중치 &lt;code&gt;w1=6.9623604&lt;/code&gt;, &lt;code&gt;w2=6.965516&lt;/code&gt;를 모두 합쳐야 음수 편향을 이겨낼 수 있습니다.&lt;/p&gt;
&lt;p&gt;반면에 &lt;code&gt;XOR&lt;/code&gt; 네트워크는 어떤 일을 하려는지 명확하지 않습니다. 가중치 &lt;code&gt;w1=5.1281769e-02&lt;/code&gt;이 &lt;code&gt;w2=3.7252903e-09&lt;/code&gt;에 비해 조금 더 큰 값을 가지고 있기는 하지만, 중간값이 이미 0에 가까워지고 따라서 시그모이드 함수를 취한 값은 &lt;code&gt;0.5&lt;/code&gt;에 가까워질 뿐입니다.&lt;/p&gt;
&lt;p&gt;이부분이 &lt;code&gt;XOR&lt;/code&gt; 문제입니다. 하나의 퍼셉트론으로는 간단한 XOR 연산자도 만들어낼 수 없다는 것을 &lt;code&gt;퍼셉트론(Perceptron)&lt;/code&gt;에서 마빈 민스키(Marvin Minsky)와 시모어 페퍼트(Seymour Papert)가 증명해냈습니다.&lt;/p&gt;
&lt;p&gt;이러한 해결책으로 등장한 것이 여러 개의 퍼셉트론을 사용합니다. 딥러닝 자체가 사실 이러한 여러개의 퍼셉트론을 만들어 가는 과정이고, 단순 반복적인 코드를 함수화해서 구현하는 것이 딥러닝 프레임워크의 일반적인 전개 과정입니다.&lt;/p&gt;
&lt;h2 id=&#34;v-keras-model-모델-활용&#34;&gt;V. Keras Model 모델 활용&lt;/h2&gt;
&lt;p&gt;이제 &lt;code&gt;tf.keras&lt;/code&gt;를 사용해서 네트워크를 만드는 과정을 담습니다.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span style=&#34;color:#f92672&#34;&gt;import&lt;/span&gt; numpy &lt;span style=&#34;color:#f92672&#34;&gt;as&lt;/span&gt; np
x &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; np&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;array([[&lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;,&lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;], [&lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;,&lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;], [&lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;,&lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;], [&lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;,&lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;]])
y &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; np&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;array([[&lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;], [&lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;], [&lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;], [&lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;]])

model &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; tf&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;keras&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;Sequential([
    tf&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;keras&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;layers&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;Dense(units&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;2&lt;/span&gt;, activation&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;sigmoid&amp;#39;&lt;/span&gt;, input_shape&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;(&lt;span style=&#34;color:#ae81ff&#34;&gt;2&lt;/span&gt;,)), 
    tf&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;keras&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;layers&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;Dense(units&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;, activation&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;sigmoid&amp;#39;&lt;/span&gt;)     
])

model&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;compile(optimizer&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;tf&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;keras&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;optimizers&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;SGD(lr&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;0.1&lt;/span&gt;), loss&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;mse&amp;#39;&lt;/span&gt;)

model&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;summary()
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;pre&gt;&lt;code&gt;Model: &amp;quot;sequential_3&amp;quot;
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
dense_6 (Dense)              (None, 2)                 6         
_________________________________________________________________
dense_7 (Dense)              (None, 1)                 3         
=================================================================
Total params: 9
Trainable params: 9
Non-trainable params: 0
_________________________________________________________________
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;위 코드에 대한 설명은 다음과 같습니다.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;model: 딥러닝 계산을 위한 여러 함수와 변수의 묶음입니다.&lt;/li&gt;
&lt;li&gt;tf.keras.Sequential: 순차적(sequential)으로 뉴런과 뉴런이 합쳐진 단위인 레이어를 일직선으로 배치한 것입니다. 외국어 표기법으로 시퀀셜 네트워크, 시퀀셜 모델로 부릅니다.&lt;/li&gt;
&lt;li&gt;tf.keras.layers.Dense: 모델에서 사용하는 레이어를 정의하는 명령입니다. &lt;code&gt;Dense&lt;/code&gt;는 가장 기본적인 레이어로써, 레이어의 입력과 출력 사이에 있는 모든 뉴런이 서로 연결되는 레이어입니다.&lt;/li&gt;
&lt;li&gt;units: 레이어를 구성하는 뉴런의 수를 정의합니다.&lt;/li&gt;
&lt;li&gt;input_shape: 입력의 차원수를 정의합니다. x의 array가 각 데이터가 &lt;code&gt;[1, 1]&lt;/code&gt;, &lt;code&gt;[1, 0]&lt;/code&gt; 1차원 array이기 때문에 원소의 개수인 2를 명시해서 (2, )라고 정의했습니다.&lt;/li&gt;
&lt;li&gt;보통 Dense 레이어의 파라미터 수는 &lt;code&gt;(입력측 뉴런의 수 + 1) X (출력측 뉴런의 수)&lt;/code&gt;의 식으로 구할 수 있습니다. 여기서 입력측, 출력측이란 &lt;code&gt;Dense&lt;/code&gt; 레이어에 들어오는 입력을 입력측, &lt;code&gt;Dense&lt;/code&gt;레이어의 뉴런을 출력측이라고 합니다. 이 식에 따르면 첫 번째 레이어의 파라미터 수는 &lt;code&gt;(2+1) X 2 = 6&lt;/code&gt;이고, 두 번째 레이어의 파라미터 수는 &lt;code&gt;(2+1) X 1 = 3&lt;/code&gt;으로 [&lt;code&gt;OUT&lt;/code&gt;]에서 출력되는 결과와 동일합니다.&lt;/li&gt;
&lt;li&gt;optimizer: 최적화 함수라고 하며, 딥러닝의 학습식을 정의하는 부분입니다. 미리 정의된 최적화 함수를 불러올 수 있습니다. &lt;code&gt;SGD&lt;/code&gt;는 확률적 경사 하강법(Stochastic Gradient Descent)의 약자입니다. &lt;code&gt;확률적&lt;/code&gt;이라는 말의 뜻은 전체를 한번에 계산하지 않고 확률적으로 일부 샘플을 구해서 조금씩 나눠서 계산한다는 뜻입니다.&lt;/li&gt;
&lt;li&gt;loss(손실): 앞에서 살펴본 error와 비슷한 개념입니다. 평균 제곱 오차(Mean Squared Error)의 약자로 기대출력에서 실제출력을 뺀 뒤에 제곱한 값을 평균하는 것입니다. 수식으로 나타내면 다음과 같습니다.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;$$Mean Squared Error = \frac{1}{n}\sum_{k=1}^{n}\left (y_{k} - output_{k}\right )^2$$&lt;/p&gt;
&lt;p&gt;앞의 예제들에서 사용했던 에러 식 $error = y - output$과 비슷한 기능을 합니다.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;model.summary()는 현재 네트워크의 구조를 알아보기 위해 쉽게 출력하는 기능입니다. 이 때 에러가 난다면 앞의 소스코드를 재확인 해야 합니다.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;이제 실제로 모형을 학습시킵니다.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;history &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; model&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;fit(x, y, epochs&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;2000&lt;/span&gt;, batch_size&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;)
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;pre&gt;&lt;code&gt;Epoch 1/2000
4/4 [==============================] - 0s 2ms/step - loss: 0.2670
Epoch 2/2000
4/4 [==============================] - 0s 2ms/step - loss: 0.2653
Epoch 3/2000
4/4 [==============================] - 0s 1ms/step - loss: 0.2638
Epoch 4/2000
4/4 [==============================] - 0s 1ms/step - loss: 0.2625
Epoch 5/2000
4/4 [==============================] - 0s 1ms/step - loss: 0.2615
Epoch 6/2000
.
.
.
Epoch 1997/2000
4/4 [==============================] - 0s 2ms/step - loss: 0.2516
Epoch 1998/2000
4/4 [==============================] - 0s 2ms/step - loss: 0.2516
Epoch 1999/2000
4/4 [==============================] - 0s 2ms/step - loss: 0.2516
Epoch 2000/2000
4/4 [==============================] - 0s 2ms/step - loss: 0.2516
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;code&gt;for&lt;/code&gt;문을 실행한 것처럼 에포크(epochs)에 지정된 횟수만큼 학습시킵니다. &lt;code&gt;batch_size&lt;/code&gt;는 한번에 학습시키는 데이터의 수인데, 여기서는 1로 지정해서 입력을 넣었을 때 정확한 값을 출력하는지 알아보려고 합니다. 첫 부분의 x, y는 각각 입력과 기대출력을 나타냅니다.&lt;/p&gt;
&lt;p&gt;학습이 끝나면 네트워크를 평가해볼 수 있습니다.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;model&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;predict(x)
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;pre&gt;&lt;code&gt;array([[0.05740971 ],
       [0.9505177 ],
       [0.95034307],
       [0.03410044]], dtype=float32)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;첫번째와 네번째 값이 0에 가깝고, 두번째와 세번째 값은 1에 가깝게 나온 것을 확인할 수 있습니다. &lt;code&gt;XOR&lt;/code&gt;과 비교했을 때보다 네트워크를 좀 더 잘 계산하고 있는 것을 확인할 수 있습니다.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;for&lt;/span&gt; weight &lt;span style=&#34;color:#f92672&#34;&gt;in&lt;/span&gt; model&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;weights:
  &lt;span style=&#34;color:#66d9ef&#34;&gt;print&lt;/span&gt;(weight)
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;pre&gt;&lt;code&gt;&amp;lt;tf.Variable &#39;dense_2/kernel:0&#39; shape=(2, 2) dtype=float32, numpy=
array([[-0.90128756,  0.5459947 ],
       [ 1.5134072 , -1.2097402 ]], dtype=float32)&amp;gt;
&amp;lt;tf.Variable &#39;dense_2/bias:0&#39; shape=(2,) dtype=float32, numpy=array([ 0.6614232, -0.3731091], dtype=float32)&amp;gt;
&amp;lt;tf.Variable &#39;dense_3/kernel:0&#39; shape=(2, 1) dtype=float32, numpy=
array([[-0.7053246 ],
       [ 0.23433231]], dtype=float32)&amp;gt;
&amp;lt;tf.Variable &#39;dense_3/bias:0&#39; shape=(1,) dtype=float32, numpy=array([0.3860617], dtype=float32)&amp;gt;
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;가중치 정보는 &lt;code&gt;model.weights&lt;/code&gt;에 저장되어 있습니다. 입력과 레이어 또는 레이어 사이의 뉴런을 연결할 때 사용되는 가중치는 &lt;code&gt;kernel&lt;/code&gt;이고, 편향과 연결된 가중치는 &lt;code&gt;bias&lt;/code&gt;로 표시됩니다.&lt;/p&gt;
&lt;p&gt;보통 네트워크의 가중치 숫자가 많기 때문에 구분을 위해 편의상 가중치에 첨자를 붙여서 표시합니다. 레이어의 순서대로 위첨자를 붙이고, 아래첨자는 각 뉴런의 순서에 맞게 차례로 붙입니다.&lt;/p&gt;
&lt;p&gt;뉴런의 개수가 3개, 레이어 개수가 2개로 늘자 이 가중치들이 무슨 일을 하는지 한눈에 잘 들어오지 않습니다. 뉴런과 레이어가 많아지면 이 문제는 더욱 커집니다.&lt;/p&gt;
&lt;p&gt;가중치 시각화보다 네트워크의 학습 상황을 더 잘 파악할 수 있는 방법이 필요한데, &lt;code&gt;matplotlib.pyplot&lt;/code&gt;을 활용하여 시각화를 진행합니다.&lt;/p&gt;
&lt;h2 id=&#34;vi-시각화-기초&#34;&gt;VI. 시각화 기초&lt;/h2&gt;
&lt;h2 id=&#34;1-간단한-꺾은선-그래프-그리기&#34;&gt;(1) 간단한 꺾은선 그래프 그리기&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;x = range(20)&lt;/code&gt;으로 &lt;code&gt;[0, 1, 2, ..., 19]&lt;/code&gt;의 20개의 정수로 구성된 리스트를 넣었습니다.&lt;/li&gt;
&lt;li&gt;&lt;code&gt;y&lt;/code&gt; 난수 생성을 활용해 랜덤 데이터를 변수에 저장했습니다.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;code&gt;plt.plot(x, y)&lt;/code&gt;는 &lt;code&gt;x&lt;/code&gt;축, &lt;code&gt;y&lt;/code&gt;축에 각각 x, y를 넣어서 그래프를 그린 후, &lt;code&gt;plt.show()&lt;/code&gt;함수를 호출합니다.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span style=&#34;color:#f92672&#34;&gt;import&lt;/span&gt; matplotlib.pyplot &lt;span style=&#34;color:#f92672&#34;&gt;as&lt;/span&gt; plt
x &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; range(&lt;span style=&#34;color:#ae81ff&#34;&gt;20&lt;/span&gt;)
y &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; tf&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;random&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;normal([&lt;span style=&#34;color:#ae81ff&#34;&gt;20&lt;/span&gt;], &lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;)
plt&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;plot(x, y)
plt&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;show()
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;&lt;img src=&#34;https://chloevan.github.io/img/tensorflow2.0/tutorial_03_05/tutorial_01.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;h2 id=&#34;2-정규분포-히스토그램-그리기&#34;&gt;(2) 정규분포 히스토그램 그리기&lt;/h2&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span style=&#34;color:#f92672&#34;&gt;import&lt;/span&gt; matplotlib.pyplot &lt;span style=&#34;color:#f92672&#34;&gt;as&lt;/span&gt; plt
random_normal &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; tf&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;random&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;normal([&lt;span style=&#34;color:#ae81ff&#34;&gt;100000&lt;/span&gt;], &lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;)
plt&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;hist(random_normal, bins &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;100&lt;/span&gt;)
plt&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;show()
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;&lt;img src=&#34;https://chloevan.github.io/img/tensorflow2.0/tutorial_03_05/tutorial_02.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;h2 id=&#34;v-2-레이어-xor-네트워크의-정보-시각화&#34;&gt;V. 2-레이어 XOR 네트워크의 정보 시각화&lt;/h2&gt;
&lt;p&gt;딥러닝을 학습시킬 때 가장 많이 보게 되는 그래프는 바로 학습이 잘 되고 있는지 확인하기 위한 측정치(&lt;code&gt;metric&lt;/code&gt;)변화량을 나타내는 선 그래프입니다. 여기서는 선 그래프를 이용해서 손실이 어떻게 변했는지를 알아보겠습니다.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span style=&#34;color:#f92672&#34;&gt;import&lt;/span&gt; matplotlib.pyplot &lt;span style=&#34;color:#f92672&#34;&gt;as&lt;/span&gt; plt
plt&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;plot(history&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;history[&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;loss&amp;#39;&lt;/span&gt;])
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;pre&gt;&lt;code&gt;[&amp;lt;matplotlib.lines.Line2D at 0x7f21635d7208&amp;gt;]
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://chloevan.github.io/img/tensorflow2.0/tutorial_03_05/tutorial_03.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;p&gt;그래프를 통해 처음에는 손실이 서서히 감소하다가 어느 시점부터 급격히 감소하고, 나중에는 거의 감소하지 않는 뒤집힌 S자 곡선을 그리는 모습을 확인할 수 있습니다. 이렇게 손실을 시각화하면 네트워크의 학습 현황을 한눈에 파악할 수 있습니다.&lt;/p&gt;
&lt;h2 id=&#34;vii-연습-파일&#34;&gt;VII. 연습 파일&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://colab.research.google.com/github/chloevan/deeplearningAI/blob/master/tensorflow2.0/ch3_3_5_Network_XOR.ipynb&#34;&gt;구글 Colab에서 직접 연습해보자&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;viii-reference&#34;&gt;VIII. Reference&lt;/h2&gt;
&lt;p&gt;김환희. (2020). 시작하세요! 텐서플로 2.0 프로그래밍: 기초 이론부터 실전 예제까지 한번에 끝내는 머신러닝, 딥러닝 핵심 가이드. 서울: 위키북스.&lt;/p&gt;
</description>
    </item>
    
  </channel>
</rss>